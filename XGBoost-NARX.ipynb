{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4747bf86",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chardet\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64884434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt \n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import plotly.colors as pc\n",
    "%matplotlib inline  \n",
    "import psutil\n",
    "from pathlib import Path\n",
    "#from Functions import *\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "from scipy.signal import savgol_filter\n",
    "from tqdm import tqdm\n",
    "from sysidentpy.model_structure_selection import FROLS\n",
    "from sysidentpy.basis_function import Polynomial\n",
    "from sysidentpy.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "\n",
    "from optuna.pruners import MedianPruner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84768a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for full reproducibility\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604ddaf",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_recursive_series(model, X_df, output_cols, X_feature_names, na):\n",
    "    \"\"\"\n",
    "    Predict recursively one step at a time using lag updates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn pipeline\n",
    "        Trained model\n",
    "    X_df : pd.DataFrame\n",
    "        Input features (with lag columns, same format as training)\n",
    "    output_cols : list of str\n",
    "        List of output columns (e.g., ['heave', 'pitch', ...])\n",
    "    X_feature_names : list\n",
    "        Ordered list of input feature names expected by the model\n",
    "    na : int\n",
    "        Number of output lags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred_df : pd.DataFrame\n",
    "        Recursive prediction results (same shape as y_df)\n",
    "    x_used_df : pd.DataFrame\n",
    "        Input rows actually used at each timestep (after lag updates)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    if na == 0:\n",
    "        y_pred = model.predict(X_df)\n",
    "        return pd.DataFrame(y_pred, columns=output_cols), X_df.copy()\n",
    "\n",
    "    n_steps = len(X_df)\n",
    "    n_outputs = len(output_cols)\n",
    "\n",
    "    y_pred = np.zeros((n_steps, n_outputs))\n",
    "    x_used_rows = []\n",
    "\n",
    "    # Start with the first input row\n",
    "    x_row = X_df.iloc[0].copy()\n",
    "    \n",
    "    value_threshold = 5\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        \n",
    "        # Check if this row contains any absurd value\n",
    "        if np.any(np.abs(x_row.values) > value_threshold):\n",
    "            print(f\"[ABORT] Found large value at step t={t}. Aborting prediction.\")\n",
    "            return pd.DataFrame(np.zeros((n_steps, n_outputs)), columns=output_cols), pd.DataFrame(x_used_rows)\n",
    "        \n",
    "        # Build input feature vector for model\n",
    "        x_input = pd.DataFrame([[x_row[feat] for feat in X_feature_names]], columns=X_feature_names)\n",
    "\n",
    "        # Save the input row used at this step\n",
    "        x_used_rows.append(x_input.iloc[0])\n",
    "\n",
    "        # Predict\n",
    "        pred = model.predict(x_input)\n",
    "        # Ensure it's always a 2D shape: (1, n_outputs)\n",
    "        if pred.ndim == 1:\n",
    "            pred = pred.reshape(1, -1)\n",
    "\n",
    "        pred = pred[0]  # Now this is always indexable\n",
    "\n",
    "        y_pred[t] = pred\n",
    "\n",
    "        if t < n_steps - 1:\n",
    "            # Prepare next input row\n",
    "            x_row_next = X_df.iloc[t + 1].copy()\n",
    "\n",
    "            for col in output_cols:\n",
    "                # Shift lags: lag_n = lag_{n-1}, ..., lag_2 = lag_1\n",
    "                for lag in reversed(range(2, na + 1)):\n",
    "                    x_row_next[f\"{col}_lag_{lag}\"] = x_row[f\"{col}_lag_{lag - 1}\"]\n",
    "                # Set lag_1 to current prediction\n",
    "                x_row_next[f\"{col}_lag_1\"] = pred[output_cols.index(col)]\n",
    "\n",
    "            x_row = x_row_next.copy()\n",
    "\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=output_cols)\n",
    "    x_used_df = pd.DataFrame(x_used_rows)\n",
    "\n",
    "    return y_pred_df, x_used_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9feaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_arx_lagged_with_scalers(\n",
    "    df,\n",
    "    input_cols,\n",
    "    output_cols,\n",
    "    scaler_X_func,\n",
    "    scaler_y_func,\n",
    "    na=0,\n",
    "    nb_past=0,\n",
    "    nf_future=0,\n",
    "    test_name_col='test_name',\n",
    "    y_initial_mode='original'  # 'original' ➔ normal; 'zero' ➔ prepend zero rows after scaling\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ARX lagged data from a single DataFrame using custom scaler functions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: original DataFrame (raw, unscaled) with test_name column\n",
    "    - input_cols: list of input column names to scale\n",
    "    - output_cols: list of output column names to scale\n",
    "    - scaler_X_func: function or fitted scaler to scale inputs (df ➔ df)\n",
    "    - scaler_y_func: function or fitted scaler to scale outputs (df ➔ df)\n",
    "    - na: number of output lags (autoregressive)\n",
    "    - nb_past: number of past input lags (exogenous input past)\n",
    "    - nf_future: number of future input lags (exogenous input preview/future)\n",
    "    - test_name_col: column name that identifies test cases\n",
    "    - y_initial_mode: 'original' ➔ normal; 'zero' ➔ prepend zero rows after scaling\n",
    "\n",
    "    Returns:\n",
    "    - X_lagged_df: lagged input DataFrame\n",
    "    - y_target_df: target output DataFrame\n",
    "    - y_initial_df: DataFrame of initial output values used as initial conditions\n",
    "    \"\"\"\n",
    "\n",
    "    X_df_list = []\n",
    "    Y_df_list = []\n",
    "    Y_initial_list = []\n",
    "\n",
    "    # Unique test cases\n",
    "    test_names = df[test_name_col].unique()\n",
    "\n",
    "    for test in test_names:\n",
    "        df_test = df[df[test_name_col] == test].copy()\n",
    "\n",
    "        # Apply scaling functions first\n",
    "        X_scaled = scaler_X_func(df_test[input_cols])\n",
    "        y_scaled = scaler_y_func(df_test[output_cols])\n",
    "\n",
    "        # Convert to DataFrames and keep test_name column for tracking\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=input_cols)\n",
    "        y_scaled_df = pd.DataFrame(y_scaled, columns=output_cols)\n",
    "\n",
    "        if y_initial_mode == 'zero':\n",
    "            # Create zero rows in scaled space\n",
    "            lag_required = max(na, nb_past)\n",
    "\n",
    "            zero_inputs = pd.DataFrame(\n",
    "                np.zeros((lag_required, len(input_cols))),\n",
    "                columns=input_cols\n",
    "            )\n",
    "\n",
    "            zero_outputs = pd.DataFrame(\n",
    "                np.zeros((lag_required, len(output_cols))),\n",
    "                columns=output_cols\n",
    "            )\n",
    "\n",
    "            # Concatenate zero inputs/outputs\n",
    "            zero_rows_inputs = zero_inputs\n",
    "            zero_rows_outputs = zero_outputs\n",
    "\n",
    "            # Concatenate zero rows on top of X_scaled_df and y_scaled_df\n",
    "            X_scaled_df = pd.concat([zero_rows_inputs, X_scaled_df], ignore_index=True)\n",
    "            y_scaled_df = pd.concat([zero_rows_outputs, y_scaled_df], ignore_index=True)\n",
    "\n",
    "        # Now proceed with lag creation\n",
    "        inputs = X_scaled_df.values\n",
    "        outputs = y_scaled_df.values\n",
    "\n",
    "        n_samples = len(X_scaled_df)\n",
    "\n",
    "        lag_required = max(na, nb_past)\n",
    "        min_future_offset = nf_future\n",
    "\n",
    "       \n",
    "        # ➤ Save the first `na` rows as y_initial\n",
    "        if y_initial_mode == 'zero':\n",
    "            start_idx = lag_required\n",
    "        else:  # 'original'\n",
    "            start_idx = 0\n",
    "\n",
    "        end_idx = n_samples - nf_future\n",
    "        \n",
    "        if na > 0:\n",
    "            if y_initial_mode == 'original':\n",
    "               \n",
    "                y_initial = y_scaled_df.iloc[:lag_required].copy()\n",
    "            else:  # 'zero'\n",
    "                y_initial= pd.DataFrame(np.zeros((na, len(output_cols))), columns=output_cols)\n",
    "               \n",
    "                \n",
    "            y_initial[test_name_col] = test\n",
    "            Y_initial_list.append(y_initial)\n",
    "\n",
    "        X_rows = []\n",
    "        Y_rows = []\n",
    "\n",
    "        for t in range(start_idx, end_idx):\n",
    "            row = {}\n",
    "            \n",
    "            #Add current unscaled input values\n",
    "            for i, in_col in enumerate(input_cols):\n",
    "                row[in_col] = inputs[t, i]  # Current time step value\n",
    "\n",
    "            # Add output past lags (autoregressive)\n",
    "            for lag in range(1, na + 1):\n",
    "                for i, out_col in enumerate(output_cols):\n",
    "                    idx = t - lag\n",
    "                    if y_initial_mode == 'zero' and idx < 0:\n",
    "                        row[f'{out_col}_lag_{lag}'] = 0.0\n",
    "                    else:\n",
    "                        row[f'{out_col}_lag_{lag}'] = outputs[idx, i]\n",
    "                            \n",
    "            # Add input past lags (exogenous)\n",
    "            for lag in range(1, nb_past + 1):\n",
    "                for i, in_col in enumerate(input_cols):\n",
    "                    idx = t - lag\n",
    "                    if y_initial_mode == 'zero' and idx < 0:\n",
    "                        row[f'{in_col}_past_{lag}'] = 0.0\n",
    "                    else:\n",
    "                        row[f'{in_col}_past_{lag}'] = inputs[idx, i]\n",
    "\n",
    "            # Add input future lags (preview control)\n",
    "            for lag in range(1, nf_future + 1):\n",
    "                if t + lag < n_samples:\n",
    "                    for i, in_col in enumerate(input_cols):\n",
    "                        row[f'{in_col}_future_{lag}'] = inputs[t + lag, i]\n",
    "                else:\n",
    "                    for in_col in input_cols:\n",
    "                        row[f'{in_col}_future_{lag}'] = 0.0  # optional padding if beyond end\n",
    "\n",
    "            X_rows.append(row)\n",
    "            Y_rows.append(outputs[t])\n",
    "\n",
    "        # Create DataFrames for this test\n",
    "        X_df = pd.DataFrame(X_rows)\n",
    "        Y_df = pd.DataFrame(Y_rows, columns=output_cols)\n",
    "\n",
    "        # Add test_name to track\n",
    "        X_df[test_name_col] = test\n",
    "        Y_df[test_name_col] = test\n",
    "\n",
    "        X_df_list.append(X_df)\n",
    "        Y_df_list.append(Y_df)\n",
    "\n",
    "    # Combine all test cases into final DataFrames\n",
    "    X_lagged_df = pd.concat(X_df_list, ignore_index=True)\n",
    "    y_target_df = pd.concat(Y_df_list, ignore_index=True)\n",
    "\n",
    "    if na > 0:\n",
    "        y_initial_df = pd.concat(Y_initial_list, ignore_index=True)\n",
    "    else:\n",
    "        y_initial_df = pd.DataFrame()\n",
    "\n",
    "    return X_lagged_df, y_target_df, y_initial_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdccb0-fee1-4899-a5c4-e8d0ab55c0f6",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train_full = pd.read_csv('prepared_data/train_data.csv')\n",
    "df_val_full = pd.read_csv('prepared_data/val_data.csv')\n",
    "df_test_full = pd.read_csv('prepared_data/test_data.csv')\n",
    "\n",
    "print(df_train_full.head())\n",
    "print(df_val_full.head())\n",
    "print(df_test_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test case\n",
    "case='Tp6p8s_Hs2m'\n",
    "df_case_train = df_train_full[df_train_full['test_name'] == case].copy()\n",
    "df_case_val = df_val_full[df_val_full['test_name'] == case].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb9903",
   "metadata": {},
   "source": [
    "# scalling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f71a1-5744-45e1-90a2-2debdaea899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize scalers\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Feature and target columns\n",
    "input_cols = ['eta']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "# Extract training data as DataFrames\n",
    "X_train = df_train_full[input_cols]\n",
    "y_train = df_train_full[output_cols]\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X.fit(X_train)\n",
    "scaler_y.fit(y_train)\n",
    "\n",
    "# Transform all sets (keeps DataFrame structure)\n",
    "X_train_scaled = pd.DataFrame(scaler_X.transform(X_train), columns=input_cols)\n",
    "y_train_scaled = pd.DataFrame(scaler_y.transform(y_train), columns=output_cols)\n",
    "\n",
    "X_val_scaled = pd.DataFrame(scaler_X.transform(df_val_full[input_cols]), columns=input_cols)\n",
    "y_val_scaled = pd.DataFrame(scaler_y.transform(df_val_full[output_cols]), columns=output_cols)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(scaler_X.transform(df_test_full[input_cols]), columns=input_cols)\n",
    "y_test_scaled = pd.DataFrame(scaler_y.transform(df_test_full[output_cols]), columns=output_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation functions using DataFrame input/output\n",
    "scaler_X_func = lambda df: pd.DataFrame(scaler_X.transform(df[input_cols]), columns=input_cols)\n",
    "scaler_y_func = lambda df: pd.DataFrame(scaler_y.transform(df[output_cols]), columns=output_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_X_vel = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "# Feature and target columns\n",
    "input_cols = ['eta','eta_velocity']\n",
    "\n",
    "\n",
    "# Extract training data as DataFrames\n",
    "X_train = df_train_full[input_cols]\n",
    "\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X_vel.fit(X_train)\n",
    "\n",
    "# Define the transformation functions using DataFrame input/output\n",
    "scaler_X_func_vel = lambda df: pd.DataFrame(scaler_X_vel.transform(df[input_cols]), columns=input_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_X_all = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "# Feature and target columns\n",
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "\n",
    "\n",
    "# Extract training data as DataFrames\n",
    "X_train = df_train_full[input_cols]\n",
    "\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X_all.fit(X_train)\n",
    "\n",
    "# Define the transformation functions using DataFrame input/output\n",
    "scaler_X_func_all = lambda df: pd.DataFrame(scaler_X_all.transform(df[input_cols]), columns=input_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f10991",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['heave']\n",
    "# Initialize scalers\n",
    "scaler_y_heave = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "scaler_y_heave.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "\n",
    "scaler_y_func_heave = lambda df: pd.DataFrame(scaler_y_heave.transform(df[output_cols]),columns=output_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pitch']\n",
    "# Initialize scalers\n",
    "\n",
    "scaler_y_pitch = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_pitch.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "\n",
    "scaler_y_func_pitch = lambda df: pd.DataFrame(scaler_y_pitch.transform(df[output_cols]),columns=output_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pendulum']\n",
    "# Initialize scalers\n",
    "\n",
    "scaler_y_pend = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_pend.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "\n",
    "scaler_y_func_pend = lambda df: pd.DataFrame(scaler_y_pend.transform(df[output_cols]),columns=output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e70c8",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Base save folder\n",
    "save_folder = 'Xgboost/XGBregressor'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f611f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective function for Optuna\n",
    "# This function will be called by Optuna to evaluate the model performance\n",
    "def objective_0(trial):\n",
    "    params = {\n",
    "        'n_estimators':trial.suggest_int('n_estimators', 10, 300),  \n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'auto'\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "    \n",
    "    # Keep track of the actual number of trees used after early stopping\n",
    "    trees_used = 0\n",
    "\n",
    "\n",
    "\n",
    "    # Model with early stopping\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train_target,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "            \n",
    "    preds = model.predict(X_train)\n",
    "    score = r2_score(y_train_target, preds)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "\n",
    "    # Add complexity penalties for both tree depth and number of trees\n",
    "    depth_penalty = 0.0005 * params['max_depth']  # Penalty factor for tree depth\n",
    "    tree_penalty = 0.000005 * params['n_estimators']  # Penalty factor for number of trees\n",
    "    \n",
    "    # Combine penalties\n",
    "    complexity_penalty = depth_penalty + tree_penalty\n",
    "    \n",
    "    return score - complexity_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c537f",
   "metadata": {},
   "source": [
    "We will Find the Optimal Hyperparametrs per DoF across diffrent input features scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e7732",
   "metadata": {},
   "source": [
    "## 1- Heave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input, output columns, and lags\n",
    "output_cols=['heave']\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "\n",
    "na=2\n",
    "nb=0\n",
    "nf=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ad988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lagged data with scalers for training and validation sets\n",
    "# This will create lagged features and apply the scalers to the input and output columns\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "X_train=dfx_train.drop(columns='test_name')\n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "        df = df_case_val,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call optuna to optimize the hyperparameters\n",
    "# Create a study object with a pruner\n",
    "\n",
    "study_heave_0 = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=100)\n",
    ") ## Don't prune the first 5 trials (give Optuna some data first) ,  Don't prune trials until at least 5 steps (e.g., boosting rounds) are completed\n",
    "\n",
    "study_heave_0.optimize(objective_0, n_trials=250, timeout=600)  # 250 trials or 10 min\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value (R²):\", study_heave_0.best_value)\n",
    "print(\"  Params:\", study_heave_0.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2194f2",
   "metadata": {},
   "source": [
    "## 2- Pitch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input, output columns, and lags\n",
    "output_cols=['pitch']\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']                             \n",
    "\n",
    "na=2\n",
    "nb=0\n",
    "nf=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba49519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lagged data with scalers for training and validation sets\n",
    "# This will create lagged features and apply the scalers to the input and output columns\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "        df = df_case_val,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db204171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call optuna to optimize the hyperparameters\n",
    "# Create a study object with a pruner\n",
    "study_pitch = optuna.create_study( \n",
    "    direction='maximize',\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=100)\n",
    ") ## Don't prune the first 5 trials (give Optuna some data first) ,  Don't prune trials until at least 5 steps (e.g., boosting rounds) are completed\n",
    "study_pitch.optimize(objective_0, n_trials=250, timeout=600)  # 250 trials or 10 min\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value (R²):\", study_pitch.best_value)\n",
    "print(\"  Params:\", study_pitch.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae2666",
   "metadata": {},
   "source": [
    "## 3- Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d84934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input, output columns, and lags\n",
    "output_cols=['pendulum']\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "\n",
    "na=2\n",
    "nb=0\n",
    "nf=0\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lagged data with scalers for training and validation sets\n",
    "# This will create lagged features and apply the scalers to the input and output columns\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)                                                                         \n",
    "\n",
    "dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "        df = df_case_val,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d23d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call optuna to optimize the hyperparameters\n",
    "# Create a study object with a pruner\n",
    "study_pendulum = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=100)\n",
    ") ## Don't prune the first 5 trials (give Optuna some data first) ,  Don't prune trials until at least 5 steps (e.g., boosting rounds) are completed\n",
    "\n",
    "study_pendulum.optimize(objective_0, n_trials=250, timeout=600)  # 250 trials or 10 min\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value (R²):\", study_pendulum.best_value)\n",
    "print(\"  Params:\", study_pendulum.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b3441",
   "metadata": {},
   "source": [
    "-Saving Studies, and calculating the Avrage of each hyperprameter values for Multivariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best parameters for each study\n",
    "\n",
    "best_params_dict = {\n",
    "    'heave': study_heave_0.best_params,\n",
    "    'pitch': study_pitch.best_params,\n",
    "    'pendulum': study_pendulum.best_params\n",
    "}\n",
    "\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"w\") as f:\n",
    "    json.dump(best_params_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "averaged_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", averaged_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456f6be",
   "metadata": {},
   "source": [
    "# Choosing Best Input Feature case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7c5f3",
   "metadata": {},
   "source": [
    "-Hyperparameters are Loaded according to the desired case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2decc",
   "metadata": {},
   "source": [
    "## Multivariate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "xgb_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", xgb_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model\n",
    "base_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Wrap it for multi-output regression\n",
    "multi_output_model = MultiOutputRegressor(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21605311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave', 'pitch',  'pendulum'] \n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5]\n",
    "nf_max= [0,1,2,3,4,5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebddd7",
   "metadata": {},
   "source": [
    "No version = parameters from $/eta$ only study,\n",
    "ver2 = parameters from corrosponding study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models/3dof/check_eta_Vel_acc'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "print(input_cols)\n",
    "\n",
    "# loop over lags \n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_3dof_eta_vel_acc_ver2'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the model\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "       \n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        \n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        print(f'-----Training model----')\n",
    "\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "\n",
    "\n",
    "        print(f'-----predicting----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "       \n",
    "        y_pred_train_scaled=model.predict(X_train)\n",
    "        \n",
    "        y_pred_val_scaled=model.predict(X_val)       \n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2 = pd.concat([metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6015eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/3dof\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2.csv\")\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The saved metrics DataFrame\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_only= pd.read_csv('Xgboost/metrics_outpus/3dof/metrics_df_Xgboost_3dof_ckeck_eta_only.csv')\n",
    "\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_vel_ver2= pd.read_csv('Xgboost/metrics_outpus/3dof/metrics_df_Xgboost_3dof_ckeck_eta_vel_ver2.csv')\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_vel= pd.read_csv('Xgboost/metrics_outpus/3dof/metrics_df_Xgboost_3dof_ckeck_eta_vel.csv')\n",
    "\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2= pd.read_csv('Xgboost/metrics_outpus/3dof/metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2.csv')\n",
    "metrics_df_Xgboost_3dof_ckeck_eta_vel_acc= pd.read_csv('Xgboost/metrics_outpus/3dof/metrics_df_Xgboost_3dof_ckeck_eta_vel_acc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a173da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from corrosponding study\n",
    "\n",
    "target_values= ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_3dof_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_3dof_ckeck_eta_vel_ver2.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_3dof_ckeck_eta_vel_acc_ver2.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Multivariate Model Using η Only Tuned Hyperparameters (na = {na_val})\",\n",
    "\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "        \n",
    "            \n",
    "\n",
    "         # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/3dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from eta only study\n",
    "\n",
    "target_values= ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_3dof_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_3dof_ckeck_eta_vel.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_3dof_ckeck_eta_vel_acc.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Multivariate Model Using Corresponding Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "        \n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/3dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}_ver2.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2db1e",
   "metadata": {},
   "source": [
    "## Uni-variate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d399e36",
   "metadata": {},
   "source": [
    "### 1-Heave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params['heave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc833c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave'] \n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5,6,7]\n",
    "nf_max= [0,1,2,3,4,5,6,7]\n",
    "\n",
    "best_params = loaded_params['heave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84cf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models/heave/check_eta_vel__acc_ver2'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "print(input_cols)\n",
    "\n",
    "# loop over lags \n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_heave_only_eta_vel_acc_ver2'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the model\n",
    "        model = XGBRegressor(**best_params)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "       \n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        \n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        print(f'-----Training model----')\n",
    "\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "\n",
    "\n",
    "        print(f'-----predicting----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "       \n",
    "        y_pred_train_scaled=model.predict(X_train).reshape(-1, 1)\n",
    "        \n",
    "        y_pred_val_scaled=model.predict(X_val).reshape(-1, 1)       \n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_heave.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_heave.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2 = pd.concat([metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/heave\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2.csv\")\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The saved metrics DataFrame\n",
    "\n",
    "metrics_df_Xgboost_heave_ckeck_eta_only= pd.read_csv('Xgboost/metrics_outpus/heave/metrics_df_Xgboost_heave_ckeck_eta_only.csv')\n",
    "\n",
    "metrics_df_Xgboost_heave_ckeck_eta_vel= pd.read_csv('Xgboost/metrics_outpus/heave/metrics_df_Xgboost_heave_ckeck_eta_vel.csv')\n",
    "metrics_df_Xgboost_heave_ckeck_eta_vel_ver2= pd.read_csv('Xgboost/metrics_outpus/heave/metrics_df_Xgboost_heave_ckeck_eta_vel_ver2.csv')\n",
    "\n",
    "metrics_df_Xgboost_heave_ckeck_eta_vel_acc= pd.read_csv('Xgboost/metrics_outpus/heave/metrics_df_Xgboost_heave_ckeck_eta_vel_acc.csv')\n",
    "metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2= pd.read_csv('Xgboost/metrics_outpus/heave/metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from eta only study\n",
    "target_values= ['heave']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_heave_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_heave_ckeck_eta_vel.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_heave_ckeck_eta_vel_acc.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Uni-variate Model Using η Only Tuned Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "        \n",
    "\n",
    "\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "        # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/1dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from corresponding study\n",
    "\n",
    "target_values= ['heave']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_heave_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_heave_ckeck_eta_vel_ver2.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_heave_ckeck_eta_vel_acc_ver2.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Uni-variate Model Using Corresponding Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "        \n",
    "    \n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "        # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/1dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}_ver2.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30da26",
   "metadata": {},
   "source": [
    "### 2-Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11283486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The hyperparameters from the JSON file\n",
    "\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params['pitch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['pitch'] \n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5,6,7]\n",
    "nf_max= [0,1,2,3,4,5,6,7]\n",
    "\n",
    "best_params = loaded_params['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models/pich/check_eta_vel_acc_ver2'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "print(input_cols)\n",
    "\n",
    "# loop over lags \n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_pitch_only_eta_vel_acc_ver2'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the model\n",
    "        model = XGBRegressor(**best_params)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_pitch,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "       \n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all,\n",
    "                scaler_y_func   = scaler_y_func_pitch,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        \n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        print(f'-----Training model----')\n",
    "\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "\n",
    "\n",
    "        print(f'-----predicting----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "       \n",
    "        y_pred_train_scaled=model.predict(X_train).reshape(-1, 1)\n",
    "        \n",
    "        y_pred_val_scaled=model.predict(X_val).reshape(-1, 1)       \n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_pitch.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_pitch.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2 = pd.concat([metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/pitch\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2.csv\")\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The saved metrics DataFrame\n",
    "\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_only= pd.read_csv(\"Xgboost/metrics_outpus/pitch/metrics_df_Xgboost_pitch_ckeck_eta_only.csv\")\n",
    "\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_vel= pd.read_csv(\"Xgboost/metrics_outpus/pitch/metrics_df_Xgboost_pitch_ckeck_eta_vel.csv\")\n",
    "\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_vel_acc= pd.read_csv(\"Xgboost/metrics_outpus/pitch/metrics_df_Xgboost_pitch_ckeck_eta_vel_acc.csv\")\n",
    "\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_Vel_ver2= pd.read_csv(\"Xgboost/metrics_outpus/pitch/metrics_df_Xgboost_pitch_ckeck_eta_vel_ver2.csv\")\n",
    "\n",
    "metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2= pd.read_csv(\"Xgboost/metrics_outpus/pitch/metrics_df_Xgboost_pitch_ckeck_eta_vel_acc_ver2.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb483ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from eta only study\n",
    "target_values= ['pitch']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_pitch_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_pitch_ckeck_eta_vel.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_pitch_ckeck_eta_vel_acc.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Uni-variate Model Using η Only Tuned Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "        \n",
    "        fig.update_yaxes(range=[0.994, 1], row=1, col=1)\n",
    "        fig.update_yaxes(range=[0.994, 1], row=1, col=2)\n",
    "        fig.update_yaxes(range=[0.994, 1], row=1, col=3)\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/1dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from corresponding study\n",
    "\n",
    "target_values= ['pitch']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_pitch_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_pitch_ckeck_eta_Vel_ver2.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_pitch_ckeck_eta_Vel_acc_ver2.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Uni-variate Model Using Corresponding Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "        \n",
    "            # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/1dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}_ver2.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5177e54",
   "metadata": {},
   "source": [
    "### 3-Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel/best_params_eta_Vel.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params['pendulum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity']\n",
    "output_cols=['pendulum'] \n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5,6,7]\n",
    "nf_max= [0,1,2,3,4,5,6,7]\n",
    "\n",
    "best_params = loaded_params['pendulum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models/pendulum/check_eta_vel_ver2'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "print(input_cols)\n",
    "\n",
    "# loop over lags \n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_pendulum_only_eta_vel_ver2'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the model\n",
    "        model = XGBRegressor(**best_params)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_vel ,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "       \n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_vel ,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        \n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        print(f'-----Training model----')\n",
    "\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "\n",
    "\n",
    "        print(f'-----predicting----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "       \n",
    "        y_pred_train_scaled=model.predict(X_train).reshape(-1, 1)\n",
    "        \n",
    "        y_pred_val_scaled=model.predict(X_val).reshape(-1, 1)       \n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_pend.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_pend.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2 = pd.concat([metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/pendulum\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2.csv\")\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The saved metrics DataFrame\n",
    "\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_only= pd.read_csv(\"Xgboost/metrics_outpus/pendulum/metrics_df_Xgboost_pendulum_ckeck_eta_only.csv\")\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_vel= pd.read_csv(\"Xgboost/metrics_outpus/pendulum/metrics_df_Xgboost_pendulum_ckeck_eta_vel.csv\")\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_vel_acc= pd.read_csv(\"Xgboost/metrics_outpus/pendulum/metrics_df_Xgboost_pendulum_ckeck_eta_vel_acc.csv\")\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2= pd.read_csv(\"Xgboost/metrics_outpus/pendulum/metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2.csv\")\n",
    "metrics_df_Xgboost_pendulum_ckeck_eta_vel_acc_ver2= pd.read_csv(\"Xgboost/metrics_outpus/pendulum/metrics_df_Xgboost_pendulum_ckeck_eta_vel_acc_ver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from eta only study\n",
    "\n",
    "target_values= ['pendulum']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_pendulum_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_pendulum_ckeck_eta_vel.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_pendulum_ckeck_eta_vel_acc.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Uni-variate Model Using η Only Tuned Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "        \n",
    "         # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/1dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case of hyperparameters from corresponding study\n",
    "target_values= ['pendulum']\n",
    "\n",
    "# Set target output variable\n",
    "for target_var in  target_values : # Change this to 'heave', 'pitch', or 'pendulum' as needed\n",
    "    \n",
    "    df_plot_1 = metrics_df_Xgboost_pendulum_ckeck_eta_only.copy()\n",
    "    df_plot_2 = metrics_df_Xgboost_pendulum_ckeck_eta_vel_ver2.copy()\n",
    "    df_plot_3 = metrics_df_Xgboost_pendulum_ckeck_eta_vel_acc_ver2.copy()\n",
    "    \n",
    "    # Unique values\n",
    "    na_values = [2]\n",
    "    nb_values = sorted(df_plot_1['nb'].unique())\n",
    "\n",
    "    # Color map to keep consistent colors for each nb\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(nb_values)}\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na_1 = df_plot_1[df_plot_1['na'] == na_val].copy()\n",
    "        df_na_2 = df_plot_2[df_plot_2['na'] == na_val].copy()\n",
    "        df_na_3 = df_plot_3[df_plot_3['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\n",
    "        f\"R² Val: η only (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ (na={na_val})\", \n",
    "        f\"R² Val: η + ẋ + ẍ (na={na_val})\"\n",
    "        ],\n",
    "        shared_yaxes=True\n",
    "        )\n",
    "\n",
    "\n",
    "        for nb_val in nb_values:\n",
    "            df_nb_1 = df_na_1[df_na_1['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_2 = df_na_2[df_na_2['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            df_nb_3 = df_na_3[df_na_3['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "            \n",
    "            color = color_map[nb_val]\n",
    "\n",
    "            # Add val R² for eta only\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_1['nf'],\n",
    "                    y=df_nb_1[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R² for eta + vel\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_2['nf'],\n",
    "                    y=df_nb_2[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Add val R² for eta + vel + acc\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nb_3['nf'],\n",
    "                    y=df_nb_3[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nb={nb_val}',\n",
    "                    legendgroup=f'nb={nb_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "           \n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"{target_var} Validation R² for the Uni-variate Model Using Corresponding Hyperparameters (na = {na_val})\",\n",
    "            height=500,\n",
    "            width=1100,\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"nb (Input Lag)\"\n",
    "            )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"nf (Disturbance Lags)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "         # Create the directory if it doesn't exist\n",
    "        save_dir = \"Results/XGBoostHyper/1dof\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define filename\n",
    "        filename = f\"{target_var}_ver2.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure with same width & height\n",
    "        pio.write_image(fig, save_path, format='png', width=1100, height=500)\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ace0e",
   "metadata": {},
   "source": [
    "# Multivariate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f7ba3",
   "metadata": {},
   "source": [
    "## Finding Optimal Lags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd92bd",
   "metadata": {},
   "source": [
    "According to Desired input feature case, Input_cols and the x scale function are adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_3dof = pd.DataFrame()\n",
    "perf_df_Xgboost_3dof = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0cfc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave', 'pitch',  'pendulum'] \n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5,6,7,8]\n",
    "nf_max= [0,1,2,3,4,5,6,7,8,9,10,11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ac4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)\n",
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "xgb_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", xgb_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a537a",
   "metadata": {},
   "source": [
    "Predict only 1st 100000 data points in train data to make it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models/3dof/ver2_extra'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# loop over lags \n",
    "\n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_3dof_ver2_extra'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the model\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all, \n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "       \n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_val,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_3dof = pd.concat([metrics_df_Xgboost_3dof, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        perf_df_Xgboost_3dof = pd.concat([perf_df_Xgboost_3dof, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/3dof\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_3dof.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_Xgboost_3dof.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_Xgboost_3dof to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_df_Xgboost_3dof.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_df_Xgboost_3dof.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_df_Xgboost_3dof to: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f13d8",
   "metadata": {},
   "source": [
    "### Plotting Data From Remote Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718c527",
   "metadata": {},
   "source": [
    "ver 1= eta only input\n",
    "ver 2 = eta + eta vel + eta acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c673948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The saved metrics DataFrame\n",
    "metrics_df_Xgboost_3dof_ver_1=pd.read_csv('Xgboost/R_M/metrics_outpus/3dof/ver1/metrics_df_Xgboost_3dof_ver1.csv')\n",
    "metrics_df_Xgboost_3dof_ver_2=pd.read_csv('Xgboost/R_M/metrics_outpus/3dof/ver2/metrics_df_Xgboost_3dof_ver2.csv')\n",
    "metrics_df_Xgboost_3dof_ver_2_extra=pd.read_csv('Xgboost/R_M/metrics_outpus/3dof/ver2/metrics_df_Xgboost_3dof_ver2_extra.csv')\n",
    "metrics_df_Xgboost_3dof_ver_2_extra_2=pd.read_csv('Xgboost/R_M/metrics_outpus/3dof/ver2/metrics_df_Xgboost_3dof_ver2_extra_2.csv')\n",
    "metrics_df_Xgboost_3dof_ver_2_final=pd.concat([metrics_df_Xgboost_3dof_ver_2, metrics_df_Xgboost_3dof_ver_2_extra, metrics_df_Xgboost_3dof_ver_2_extra_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case on eta + eta vel + eta acc as input features\n",
    "\n",
    "target_values = ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "for target_var in target_values:\n",
    "    df_plot = metrics_df_Xgboost_3dof_ver_2_final[metrics_df_Xgboost_3dof_ver_2_final['na'] == 2].copy()\n",
    "\n",
    "    na_values = sorted(df_plot['na'].unique())\n",
    "    nf_values = sorted(df_plot['nf'].unique())  # Now nf is the legend\n",
    "    nb_values = sorted(df_plot['nb'].unique())  # Now nb is the x-axis\n",
    "\n",
    "    # Color map for each nf (now the legend)\n",
    "    color_map = {nf: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nf in enumerate(nf_values)}\n",
    "    color_map[10] = px.colors.qualitative.D3[8]  # just another predefined color\n",
    "    color_map[11] = px.colors.qualitative.D3[3]  # just another predefined color\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na = df_plot[df_plot['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=[f\"R² Train\", f\"R² Val \"],\n",
    "            shared_yaxes=False\n",
    "        )\n",
    "\n",
    "        for nf_val in nf_values:\n",
    "            df_nf = df_na[df_na['nf'] == nf_val].copy().sort_values(by='nb')\n",
    "            color = color_map[nf_val]\n",
    "\n",
    "            # Add train R²\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nf['nb'],\n",
    "                    y=df_nf[f'r2_train_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nd={-nf_val}',\n",
    "                    legendgroup=f'nd={-nf_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R²\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nf['nb'],\n",
    "                    y=df_nf[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nd={-nf_val}',\n",
    "                    legendgroup=f'nd={-nf_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"R² Performance of XGBoost Shared Model for All DOFs <br> η , η̇ and η̈ as Input Features, na = {na_val} ({target_var}) <br>\",\n",
    "            xaxis_title=\"nb \",\n",
    "            xaxis2_title=\"nb\",\n",
    "            yaxis_title=\"R² Score\",\n",
    "            template=\"plotly_white\",\n",
    "            height=600,\n",
    "            width=1350,\n",
    "            legend_title=\"nd\",\n",
    "            #yaxis=dict(range=[0, 1.05])\n",
    "        )\n",
    "\n",
    "        import os\n",
    "\n",
    "        # Define output directory and filename\n",
    "        output_dir = \"Xgboost/final_plots/3dof_ver2\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        filename = f\"Validation_R2_eta_only_na{na_val}_{target_var}_ver2.png\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        fig.write_image(save_path, scale=2)  # scale=2 for higher resolution\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting in case of eta only as an input feature\n",
    "\n",
    "target_values = ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "for target_var in target_values:\n",
    "    df_plot = metrics_df_Xgboost_3dof_ver_1[metrics_df_Xgboost_3dof_ver_1['na'] == 2].copy()\n",
    "\n",
    "    na_values = sorted(df_plot['na'].unique())\n",
    "    nf_values = sorted(df_plot['nf'].unique())  # Now nf is the legend\n",
    "    nb_values = sorted(df_plot['nb'].unique())  # Now nb is the x-axis\n",
    "\n",
    "    # Color map for each nf (now the legend)\n",
    "    color_map = {nf: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nf in enumerate(nf_values)}\n",
    "    color_map[10] = px.colors.qualitative.D3[8]  # just another predefined color\n",
    "    color_map[11] = px.colors.qualitative.D3[3]  # just another predefined color\n",
    "\n",
    "    for na_val in na_values:\n",
    "        df_na = df_plot[df_plot['na'] == na_val].copy()\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=[f\"R² Train\", f\"R² Val \"],\n",
    "            shared_yaxes=False\n",
    "        )\n",
    "\n",
    "        for nf_val in nf_values:\n",
    "            df_nf = df_na[df_na['nf'] == nf_val].copy().sort_values(by='nb')\n",
    "            color = color_map[nf_val]\n",
    "\n",
    "            # Add train R²\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nf['nb'],\n",
    "                    y=df_nf[f'r2_train_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nd={-nf_val}',\n",
    "                    legendgroup=f'nd={-nf_val}',\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Add val R²\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nf['nb'],\n",
    "                    y=df_nf[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'nd={-nf_val}',\n",
    "                    legendgroup=f'nd={-nf_val}',\n",
    "                    showlegend=False,\n",
    "                    line=dict(color=color)\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = f\"R² Performance of XGBoost Shared Model for All DOFs <br> η only as an nput Feature, na = {na_val} ({target_var}) <br>\",\n",
    "            xaxis_title=\"nb \",\n",
    "            xaxis2_title=\"nb\",\n",
    "            yaxis_title=\"R² Score\",\n",
    "            template=\"plotly_white\",\n",
    "            height=600,\n",
    "            width=1350,\n",
    "            legend_title=\"nd\",\n",
    "            #yaxis=dict(range=[0, 1.05])\n",
    "        )\n",
    "\n",
    "        import os\n",
    "\n",
    "        # Define output directory and filename\n",
    "        output_dir = \"Xgboost/final_plots/3dof_ver2\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        filename = f\"Validation_R2_eta_only_na{na_val}_{target_var}_ver2.png\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        #fig.write_image(save_path, scale=2)  # scale=2 for higher resolution\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e29b2",
   "metadata": {},
   "source": [
    "## loading models to calculate full R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f181ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/R_M/saved_models/3dof/ver2/Best_combination'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb10ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame for training metrics\n",
    "train_metrics_df_Xgboost_3dof= pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave', 'pitch',  'pendulum']\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "   # extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "    \n",
    "    # prepare training data\n",
    "    dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "    \n",
    "    y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "    X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "    X_feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    # predict on train data\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_train_df= pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nd': -1*nf,\n",
    "            }\n",
    "    \n",
    "    for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "           \n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            \n",
    "    train_metrics_df_Xgboost_3dof = pd.concat([train_metrics_df_Xgboost_3dof, pd.DataFrame([metrics_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training metrics DataFrame\n",
    "train_metrics_df_Xgboost_3dof.to_csv('Xgboost/R_M/metrics_outpus/3dof/ver2/train_metrics_df_Xgboost_3dof_ver2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_df_Xgboost_3dof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea33d67",
   "metadata": {},
   "source": [
    "## Testing The selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da69ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_case='Tp6p8s_Hs2m'\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave', 'pitch',  'pendulum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base folder where results will be saved\n",
    "save_folder = 'XGboost/results/3dof_model/test_best_models'  # you can change this to any directory you want\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_metrics = {}  # Store separate metrics DataFrames per model\n",
    "y_true_pred_dict = {}    # Store true and predicted values per model and case\n",
    "\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "    \n",
    "    # Extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "\n",
    "    metrics_df_test_all = []  # Per-model metrics list\n",
    "    model_y_true_pred = {}          # Per-model true/predicted dict\n",
    "\n",
    "    cases = df_test_full['test_name'].unique()\n",
    "\n",
    "    for case in cases:\n",
    "        df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "\n",
    "        # Prepare the test data\n",
    "        dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "            df=df_case_test,\n",
    "            input_cols=input_cols,\n",
    "            output_cols=output_cols,\n",
    "            scaler_X_func=scaler_X_func_all,\n",
    "            scaler_y_func=scaler_y_func,\n",
    "            na=na,\n",
    "            nb_past=nb,\n",
    "            nf_future=nf,\n",
    "            test_name_col='test_name',\n",
    "            y_initial_mode='original'\n",
    "        )\n",
    "\n",
    "        X_test_selected_df = dfx_test.drop(columns='test_name')\n",
    "        X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "        # Predict\n",
    "        y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_test_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "        )\n",
    "\n",
    "        y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "        y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "        y_true_test = scaler_y.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "        y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "        # Save predictions and ground truth\n",
    "        model_y_true_pred[case] = {\n",
    "            'y_true': y_true_test_df,\n",
    "            'y_pred': y_pred_test_df\n",
    "        }\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics_test = {\n",
    "            'model': fname,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'test case': case\n",
    "        }\n",
    "\n",
    "        if case == training_case:\n",
    "            metrics_test['Comments'] = 'case used for training'\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "            r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "            metrics_test[f'r2_test_{col}'] = r2_test\n",
    "            metrics_test[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "        metrics_df_test_all.append(metrics_test)\n",
    "\n",
    "    # Store per-model data\n",
    "    metrics_df_test_all = pd.DataFrame(metrics_df_test_all)\n",
    "    all_models_metrics[fname] = metrics_df_test_all\n",
    "    y_true_pred_dict[fname] = model_y_true_pred\n",
    "\n",
    "    # Optionally display or save\n",
    "    print(f\"\\nMetrics for model {fname}:\")\n",
    "    display(metrics_df_test_all)\n",
    "\n",
    "   \n",
    "    # Save predictions and true values if needed\n",
    "    for case, data in model_y_true_pred.items():\n",
    "        y_true_path = os.path.join(save_folder, f'{fname}_case_{case}_y_true.csv')\n",
    "        y_pred_path = os.path.join(save_folder, f'{fname}_case_{case}_y_pred.csv')\n",
    "        \n",
    "        data['y_true'].to_csv(y_true_path, index=False)\n",
    "        data['y_pred'].to_csv(y_pred_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50408514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics DataFrames for all models\n",
    "for fname, metrics_df in all_models_metrics.items():\n",
    "    metrics_path = os.path.join(save_folder, f\"{fname}_metrics.csv\")\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edeabe5",
   "metadata": {},
   "source": [
    "## dt Sensitivety "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1628c6",
   "metadata": {},
   "source": [
    "Series configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame\n",
    "metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608eb821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)\n",
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "xgb_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", xgb_params)\n",
    "# Create the base model\n",
    "base_model = XGBRegressor(**xgb_params)\n",
    "multi_output_model = MultiOutputRegressor(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6105f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "dfx_train_full,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_train=dfx_train_full.drop(columns='test_name')\n",
    "\n",
    "\n",
    "\n",
    "# prepare Testing data\n",
    "\n",
    "dfx_test_full,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "y_test_target = dfy_test_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_test=dfx_test_full.drop(columns='test_name')\n",
    "\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "    \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the model\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "        \n",
    "   \n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': 3,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new = pd.concat([metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new, row_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0778536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = \"Xgboost/metrics_outpus/3dof/dt_test-new\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new to: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8489d8",
   "metadata": {},
   "source": [
    "parallel configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame for parallel processing results\n",
    "metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93df86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave','pitch','pendulum']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=0\n",
    "nf=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a89828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)\n",
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "xgb_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", xgb_params)\n",
    "# Create the base model\n",
    "base_model = XGBRegressor(**xgb_params)\n",
    "multi_output_model = MultiOutputRegressor(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f011b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "dfx_train_full,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_train=dfx_train_full.drop(columns='test_name')\n",
    "\n",
    "\n",
    "\n",
    "# prepare Testing data\n",
    "\n",
    "dfx_test_full,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "y_test_target = dfy_test_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_test=dfx_test_full.drop(columns='test_name')\n",
    "\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "    \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the model\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "        \n",
    "  \n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled=model.predict(X_train_selected_df)\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': 3,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel = pd.concat([metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel, row_df], ignore_index=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Base save folder\n",
    "save_folder = \"Xgboost/metrics_outpus/3dof/dt_test-new\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b957c",
   "metadata": {},
   "source": [
    "## Sensitivety to data size \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e0de8",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the length of the series for each step\n",
    "lenght=np.array([0.002,0.004,0.008,0.01 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9,1])*len(df_case_train)\n",
    "lenght=lenght.astype(int)\n",
    "# best model parameters\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave','pitch','pendulum']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=0\n",
    "nf=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)\n",
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "xgb_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", xgb_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model\n",
    "base_model = XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_train_selected_df=dfx_train.drop(columns='test_name')\n",
    "\n",
    "# prepare Testing data\n",
    "\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "y_test_target = dfy_test[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_test_selected_df=dfx_test.drop(columns='test_name')\n",
    "\n",
    "# get feature names\n",
    "X_feature_names = X_train_selected_df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame \n",
    "metrics_df_Xgboost_3dof_data_test_new_parralel=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for l in lenght:\n",
    "    \n",
    "    # Create the model\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "\n",
    "\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "        \n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled=model.predict(X_train_selected_df[0:l])\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled = model.predict(X_test_selected_df)\n",
    "   \n",
    "\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "    y_true_test_cut = scaler_y.inverse_transform(dfy_test[output_cols][0:l_test].reset_index(drop=True))\n",
    "    y_true_test_df_cut = pd.DataFrame(y_true_test_cut, columns=output_cols)\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': 3,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        \n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "        \n",
    "       \n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_Xgboost_3dof_data_test_new_parralel= pd.concat([metrics_df_Xgboost_3dof_data_test_new_parralel ,row_df], ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = \"Xgboost/metrics_outpus/3dof/final\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_3dof_data_test_new_parralel_ver2.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_Xgboost_3dof_data_test_new_parralel.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_Xgboost_3dof_data_test_new_parralel to: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32d7cb",
   "metadata": {},
   "source": [
    "Series Configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b695c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame\n",
    "metrics_df_Xgboost_3dof_data_test_series=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b714c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for l in lenght:\n",
    "    \n",
    "    # Create the model\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "        \n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df[0:l],\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "    \n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': 3,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        \n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "       \n",
    "       \n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_Xgboost_3dof_data_test_series= pd.concat([metrics_df_Xgboost_3dof_data_test_series ,row_df], ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = \"Xgboost/metrics_outpus/3dof/final\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_3dof_data_test_series_ver2.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_Xgboost_3dof_data_test_series.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_Xgboost_3dof_data_test_series to: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626e7e9",
   "metadata": {},
   "source": [
    "## Testing The best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e57cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "\n",
    "model = joblib.load('Xgboost_3dof_ver2_extra_na2_nb0_nf10.joblib')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 0\n",
    "best_nf = 10\n",
    "\n",
    "\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['heave','pitch','pendulum']\n",
    "# load the best model\n",
    "\n",
    "# print all test cases\n",
    "test_cases = df_test_full['test_name'].unique()\n",
    "print(\"All test cases:\")\n",
    "print(test_cases)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_test = []  # Will collect all test metrics\n",
    "training_case='Tp6p8s_Hs2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test case\n",
    "cases=df_test_full['test_name'].unique()\n",
    "for case in cases:\n",
    "    # Filter the test data\n",
    "    df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Prepare the test data\n",
    "    dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "        df=df_case_test,\n",
    "        input_cols=best_input_cols,\n",
    "        output_cols=best_output_cols,\n",
    "        scaler_X_func   = scaler_X_func_all,\n",
    "        scaler_y_func   = scaler_y_func,\n",
    "        na=best_na,\n",
    "        nb_past=best_nb,\n",
    "        nf_future=best_nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "    )\n",
    "    \n",
    "    X_test_selected_df=dfx_test.drop(columns='test_name')\n",
    "\n",
    "    \n",
    "\n",
    "    # get feature names from model\n",
    "    X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_test_selected_df,\n",
    "        output_cols=best_output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=best_na\n",
    "    )\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "    # Convert to DataFrame\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "    # Get true values aligned with dfy_test indexes\n",
    "    y_true_test = scaler_y.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_test = {}\n",
    "    metrics_test[f'test case'] = case\n",
    "    if case == training_case:\n",
    "        metrics_test[f'Comments'] = 'case used for training'\n",
    "   \n",
    "    for col in best_output_cols:\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        \n",
    "        metrics_test[f'r2_test_{col}'] = r2_test\n",
    "        metrics_test[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "\n",
    "    # save predictions and true values\n",
    "    y_true_test_df.to_csv(f'testing/XGB_true_{case}.csv', index=False)\n",
    "    y_pred_test_df.to_csv(f'testing/XGB_pred_{case}.csv', index=False)\n",
    "    print(\"metrics:\", metrics_test)\n",
    "    metrics_df_test.append(metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics DataFrame\n",
    "metrics_df_test.to_csv(f'testing/XGB_metrics_df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7010dde",
   "metadata": {},
   "source": [
    "# Uni-variate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbc058",
   "metadata": {},
   "source": [
    "According to Desired input feature case, Input_cols and the x scale function are adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a74d6",
   "metadata": {},
   "source": [
    "## 1-Heave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f5b64",
   "metadata": {},
   "source": [
    "### Finding Optimal Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_heave_only_ALL_ver2 = pd.DataFrame()\n",
    "perf_df_Xgboost_heave_only_ALL_ver2 = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f73334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params['heave'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity', 'eta_acceleration']\n",
    "output_cols=['heave'] \n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5,6,7]\n",
    "nf_max= [0,1,2,3,4,5,6,7,8,9,10]\n",
    "best_params = loaded_params['heave']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7962415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models/heave_only/extra'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    " # get  df at true scale witAh only this test case\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_heave_eta_only'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the pipeline model\n",
    "        model = XGBRegressor(**best_params)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "        df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train[0:10000],\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_val,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_heave.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols][0:10000].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_heave.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_heave_only_ALL_ver2 = pd.concat([metrics_df_Xgboost_heave_only_ALL_ver2, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        perf_df_Xgboost_heave_only_ALL_ver2 = pd.concat([perf_df_Xgboost_heave_only_ALL_ver2, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/heave_only\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_heave_only_ALL_ver2.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_Xgboost_heave_only_ALL_ver2.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_Xgboost_heave_only_ALL_ver2_extra_2 to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_df_Xgboost_heave_only_ALL_ver2.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "\n",
    "perf_df_Xgboost_heave_only_ALL_ver2.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_df_Xgboost_heave_only_ALL_ver2 to: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e1ae7",
   "metadata": {},
   "source": [
    "### Plotting Data From Remote Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7675d",
   "metadata": {},
   "source": [
    "ver 1= eta only input\n",
    "ver 2 = eta + eta vel + eta acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe70c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics DataFrame\n",
    "metrics_df_Xgboost_heave_only_ALL_ver1=pd.read_csv('Xgboost/R_M/metrics_outpus\\heave_only/metrics_df_Xgboost_heave_only_ALL_ver1.csv')\n",
    "metrics_df_Xgboost_heave_only_ALL_ver2=pd.read_csv('Xgboost/R_M/metrics_outpus\\heave_only/metrics_df_Xgboost_heave_only_ALL_ver2.csv')\n",
    "metrics_df_Xgboost_heave_only_ALL_ver2_extra=pd.read_csv('Xgboost/R_M/metrics_outpus\\heave_only/metrics_df_Xgboost_heave_only_ALL_ver2_extra.csv')\n",
    "metrics_df_Xgboost_heave_only_ALL_ver2_extra_2=pd.read_csv('Xgboost/R_M/metrics_outpus\\heave_only/metrics_df_Xgboost_heave_only_ALL_ver2_extra_2.csv')\n",
    "metrics_df_Xgboost_heave_only_ALL_ver2_final=pd.concat([metrics_df_Xgboost_heave_only_ALL_ver2, metrics_df_Xgboost_heave_only_ALL_ver2_extra, metrics_df_Xgboost_heave_only_ALL_ver2_extra_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case on eta + eta vel + eta acc as input features\n",
    "# Set target output variable\n",
    "target_var = 'heave'\n",
    "df_plot = metrics_df_Xgboost_heave_only_ALL_ver2_final.copy()\n",
    "\n",
    "# Filter for only na = 2\n",
    "df_plot = df_plot[df_plot['na'] == 2]\n",
    "\n",
    "# Unique nb values (will now be legend entries)\n",
    "unique_nb = sorted(df_plot['nb'].unique())\n",
    "color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(unique_nb)}\n",
    "\n",
    "# Create subplots (Train and Validation)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"R² Train\", \"R² Val\"],\n",
    "    shared_yaxes=False\n",
    ")\n",
    "\n",
    "# Loop through nb values to plot curves vs nd\n",
    "for nb_val in unique_nb:\n",
    "    df_nb = df_plot[df_plot['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "    color = color_map[nb_val]\n",
    "    \n",
    "    # Check if nb is the one to highlight\n",
    "    if nb_val == 0:\n",
    "        line_width = 4    # Thicker line for highlighting\n",
    "        marker_size = 12  # Larger markers for highlighting\n",
    "        marker_symbol = 'star'\n",
    "    else:\n",
    "        line_width = 2\n",
    "        marker_size = 8\n",
    "        marker_symbol = 'circle'\n",
    "\n",
    "    # Compute nd = -nf\n",
    "    df_nb['nd'] = -df_nb['nf']\n",
    "\n",
    "    # Add train R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_train_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            line=dict(color=color, width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add val R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_val_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color , width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    title_text=(\"R² Performance of One XGBoost Model for Each DOF<br>η,  η̇ , and η̈  as Input Features, na = 2 (Heave)\"),\n",
    "    xaxis_title=\"nd \",\n",
    "    xaxis2_title=\"nd\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=460,\n",
    "    width=1000,\n",
    "    legend_title=\"nb\"\n",
    ")\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=1)\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting in case of eta only as an input feature\n",
    "\n",
    "# Set target output variable\n",
    "target_var = 'heave'\n",
    "df_plot = metrics_df_Xgboost_heave_only_ALL_ver2_final.copy()\n",
    "\n",
    "# Filter for only na = 2\n",
    "df_plot = df_plot[df_plot['na'] == 2]\n",
    "\n",
    "# Unique nb values (will now be legend entries)\n",
    "unique_nb = sorted(df_plot['nb'].unique())\n",
    "color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(unique_nb)}\n",
    "\n",
    "# Create subplots (Train and Validation)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"R² Train\", \"R² Val\"],\n",
    "    shared_yaxes=False\n",
    ")\n",
    "\n",
    "# Loop through nb values to plot curves vs nd\n",
    "for nb_val in unique_nb:\n",
    "    df_nb = df_plot[df_plot['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "    color = color_map[nb_val]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compute nd = -nf\n",
    "    df_nb['nd'] = -df_nb['nf']\n",
    "\n",
    "    # Add train R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_train_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            line=dict(color=color)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add val R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_val_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    title_text=(\"R² Performance of One XGBoost Model for Each DOF<br>η Only as an Input Feature, na = 2 (Heave)\"),\n",
    "    xaxis_title=\"nd \",\n",
    "    xaxis2_title=\"nd\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=460,\n",
    "    width=1000,\n",
    "    legend_title=\"nb\"\n",
    ")\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=1)\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39913be",
   "metadata": {},
   "source": [
    "### loading models to calculate full R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c482fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/saved_models/heave_only/best_combination'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame \n",
    "\n",
    "train_metrics_df_Xgboost_heave= pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave']\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "   # extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "    \n",
    "    # prepare training data\n",
    "    dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "    \n",
    "    y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "    X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "    X_feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    # predict on train data\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_train_df= pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nd': -1*nf,\n",
    "            }\n",
    "    \n",
    "    for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "           \n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            \n",
    "    train_metrics_df_Xgboost_heave = pd.concat([train_metrics_df_Xgboost_heave, pd.DataFrame([metrics_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c052d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics DataFrame\n",
    "train_metrics_df_Xgboost_heave.to_csv('Xgboost/metrics_outpus/heave_only/train_metrics_df_Xgboost_heave_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34cabcb",
   "metadata": {},
   "source": [
    "### Testing The selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d38d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_case='Tp6p8s_Hs2m'\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base folder where results will be saved\n",
    "save_folder = 'XGboost/results/heave_model/test_best_models'  # you can change this to any directory you want\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc77d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/saved_models/heave_only/best_combination'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57671238",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_metrics = {}  # Store separate metrics DataFrames per model\n",
    "y_true_pred_dict = {}    # Store true and predicted values per model and case\n",
    "\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "    \n",
    "    # Extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "\n",
    "    metrics_df_test_all = []  # Per-model metrics list\n",
    "    model_y_true_pred = {}          # Per-model true/predicted dict\n",
    "\n",
    "    cases = df_test_full['test_name'].unique()\n",
    "\n",
    "    for case in cases:\n",
    "        df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "\n",
    "        # Prepare the test data\n",
    "        dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "            df=df_case_test,\n",
    "            input_cols=input_cols,\n",
    "            output_cols=output_cols,\n",
    "            scaler_X_func=scaler_X_func_all,\n",
    "            scaler_y_func=scaler_y_func_heave,\n",
    "            na=na,\n",
    "            nb_past=nb,\n",
    "            nf_future=nf,\n",
    "            test_name_col='test_name',\n",
    "            y_initial_mode='original'\n",
    "        )\n",
    "\n",
    "        X_test_selected_df = dfx_test.drop(columns='test_name')\n",
    "        X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "        # Predict\n",
    "        y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_test_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "        )\n",
    "\n",
    "        y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "        y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "        y_true_test = scaler_y_heave.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "        y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "        # Save predictions and ground truth\n",
    "        model_y_true_pred[case] = {\n",
    "            'y_true': y_true_test_df,\n",
    "            'y_pred': y_pred_test_df\n",
    "        }\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics_test = {\n",
    "            'model': fname,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'test case': case\n",
    "        }\n",
    "\n",
    "        if case == training_case:\n",
    "            metrics_test['Comments'] = 'case used for training'\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "            r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "            metrics_test[f'r2_test_{col}'] = r2_test\n",
    "            metrics_test[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "        metrics_df_test_all.append(metrics_test)\n",
    "\n",
    "    # Store per-model data\n",
    "    metrics_df_test_all = pd.DataFrame(metrics_df_test_all)\n",
    "    all_models_metrics[fname] = metrics_df_test_all\n",
    "    y_true_pred_dict[fname] = model_y_true_pred\n",
    "\n",
    "    # Optionally display or save\n",
    "    print(f\"\\nMetrics for model {fname}:\")\n",
    "    display(metrics_df_test_all)\n",
    "\n",
    "    # Save the metrics DataFrame for this model\n",
    "    model_save_path = os.path.join(save_folder, f'metrics_{fname}.csv')\n",
    "    metrics_df_test_all.to_csv(model_save_path, index=False)\n",
    "   \n",
    "    # Save predictions and true values if needed\n",
    "    for case, data in model_y_true_pred.items():\n",
    "        y_true_path = os.path.join(save_folder, f'{fname}_case_{case}_y_true.csv')\n",
    "        y_pred_path = os.path.join(save_folder, f'{fname}_case_{case}_y_pred.csv')\n",
    "        \n",
    "        data['y_true'].to_csv(y_true_path, index=False)\n",
    "        data['y_pred'].to_csv(y_pred_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb611a",
   "metadata": {},
   "source": [
    "## 2-Pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c862e",
   "metadata": {},
   "source": [
    "### Finding Optimal Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_pitch_only = pd.DataFrame()\n",
    "perf_df_Xgboost_pitch_only = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The hyperparameters from the JSON file\n",
    "\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params['pitch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity', 'eta_acceleration']\n",
    "output_cols=['pitch'] \n",
    "na_max  = [2]\n",
    "nb_max= [9,10,11]\n",
    "nf_max= [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "best_params = loaded_params['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_pitch_ver1_'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the pipeline model\n",
    "        model = XGBRegressor(**best_params)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_pitch,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "        df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all,\n",
    "                scaler_y_func   = scaler_y_func_pitch,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train[0:10000],\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_val,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_pitch.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols][0:10000].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_pitch.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_pitch_only = pd.concat([metrics_df_Xgboost_pitch_only, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        perf_df_Xgboost_pitch_only = pd.concat([perf_df_Xgboost_pitch_only, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfda603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/pitch_only\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_pitch_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_Xgboost_pitch_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_Xgboost_pitch_only to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_df_Xgboost_pitch_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_df_Xgboost_pitch_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_df_Xgboost_pitch_only to: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4bd58",
   "metadata": {},
   "source": [
    "### Plotting Data From Remote Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddfbee",
   "metadata": {},
   "source": [
    "ver 1= eta only input\n",
    "ver 2 = eta + eta vel + eta acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e43b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics DataFrame\n",
    "metrics_df_Xgboost_pitch_only_ver1= pd.read_csv('Xgboost/R_M/metrics_outpus/pitch_only/metrics_df_Xgboost_pitch_only_ver1.csv')\n",
    "metrics_df_Xgboost_pitch_only_ver2= pd.read_csv('Xgboost/R_M/metrics_outpus/pitch_only/metrics_df_Xgboost_pitch_only_ver2.csv')\n",
    "metrics_df_Xgboost_pitch_only_ver2_extra= pd.read_csv('Xgboost/R_M/metrics_outpus/pitch_only/metrics_df_Xgboost_pitch_only_ver2_extra.csv')\n",
    "metrics_df_Xgboost_pitch_only_ver2_extra_2= pd.read_csv('Xgboost/R_M/metrics_outpus/pitch_only/metrics_df_Xgboost_pitch_only_ver2_extra_2.csv')\n",
    "metrics_df_Xgboost_pitch_only_ver2_extra_3= pd.read_csv('Xgboost/R_M/metrics_outpus/pitch_only/metrics_df_Xgboost_pitch_only_ver2_extra_3.csv')\n",
    "metrics_df_Xgboost_pitch_only_ver2_extra_4  = pd.read_csv('Xgboost/R_M/metrics_outpus/pitch_only/metrics_df_Xgboost_pitch_only_ver2_extra_4.csv')\n",
    "metrics_df_Xgboost_pitch_only_ver2_final= pd.concat([metrics_df_Xgboost_pitch_only_ver2,metrics_df_Xgboost_pitch_only_ver2_extra, metrics_df_Xgboost_pitch_only_ver2_extra_2,metrics_df_Xgboost_pitch_only_ver2_extra_3,metrics_df_Xgboost_pitch_only_ver2_extra_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case on eta only as and input feature\n",
    "\n",
    "# Set target output variable\n",
    "target_var = 'pitch'\n",
    "df_plot = metrics_df_Xgboost_pitch_only_ver1.copy()\n",
    "\n",
    "# Filter for only na = 2\n",
    "df_plot = df_plot[df_plot['na'] == 2]\n",
    "\n",
    "# Unique nb values (will now be legend entries)\n",
    "unique_nb = sorted(df_plot['nb'].unique())\n",
    "color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(unique_nb)}\n",
    "\n",
    "# Create subplots (Train and Validation)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"R² Train\", \"R² Val\"],\n",
    "    shared_yaxes=False\n",
    ")\n",
    "\n",
    "# Loop through nb values to plot curves vs nd\n",
    "for nb_val in unique_nb:\n",
    "    df_nb = df_plot[df_plot['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "    color = color_map[nb_val]\n",
    "    \n",
    "    # Check if nb is the one to highlight\n",
    "    if nb_val == -1:\n",
    "        line_width = 4    # Thicker line for highlighting\n",
    "        marker_size = 12  # Larger markers for highlighting\n",
    "        marker_symbol = 'star'\n",
    "    else:\n",
    "        line_width = 2\n",
    "        marker_size = 8\n",
    "        marker_symbol = 'circle'\n",
    "\n",
    "    # Compute nd = -nf\n",
    "    df_nb['nd'] = -df_nb['nf']\n",
    "\n",
    "    # Add train R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_train_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            line=dict(color=color, width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add val R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_val_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color , width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    title_text=(\"R² Performance of One XGBoost Model for Each DOF<br>η Only as an Input Feature, na = 2 (Pitch)\"),\n",
    "    xaxis_title=\"nd \",\n",
    "    xaxis2_title=\"nd\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=460,\n",
    "    width=1000,\n",
    "    legend_title=\"nb\",\n",
    "    yaxis1=dict(range=[0, 1]),  # Set y-axis limits to show R² clearly\n",
    "    yaxis2=dict(range=[0, 1]),  # Set y-axis limits to show R² clearly\n",
    ")\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=1)\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d52d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case on eta + eta vel + eta acc as input features\n",
    "\n",
    "# Set target output variable\n",
    "target_var = 'pitch'\n",
    "df_plot = metrics_df_Xgboost_pitch_only_ver2_final.copy()\n",
    "\n",
    "# Filter for only na = 2\n",
    "df_plot = df_plot[df_plot['na'] == 2]\n",
    "\n",
    "# Unique nb values (will now be legend entries)\n",
    "unique_nb = sorted(df_plot['nb'].unique())\n",
    "color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(unique_nb)}\n",
    "color_map[10] = px.colors.qualitative.D3[8]  # just another predefined color\n",
    "color_map[11] = px.colors.qualitative.D3[3]  # just another predefined color\n",
    "# Create subplots (Train and Validation)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"R² Train\", \"R² Val\"],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Loop through nb values to plot curves vs nd\n",
    "for nb_val in unique_nb:\n",
    "    df_nb = df_plot[df_plot['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "    color = color_map[nb_val]\n",
    "    \n",
    "    # Check if nb is the one to highlight\n",
    "    if nb_val == 0:\n",
    "        line_width = 4    # Thicker line for highlighting\n",
    "        marker_size = 12  # Larger markers for highlighting\n",
    "        marker_symbol = 'star'\n",
    "    else:\n",
    "        line_width = 2\n",
    "        marker_size = 8\n",
    "        marker_symbol = 'circle'\n",
    "\n",
    "    # Compute nd = -nf\n",
    "    df_nb['nd'] = -df_nb['nf']\n",
    "\n",
    "    # Add train R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_train_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            line=dict(color=color, width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add val R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_val_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color , width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    title_text=(\"R² Performance of One XGBoost Model for Each DOF<br>η, η̇ , and η̈  as Input Features, na = 2 (Pitch)\"),  \n",
    "    xaxis_title=\"nd \",\n",
    "    xaxis2_title=\"nd\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=460,\n",
    "    width=1000,\n",
    "    legend_title=\"nb\",\n",
    "    \n",
    " \n",
    ")\n",
    " \n",
    "fig.update_yaxes(showticklabels=True, row=1, col=1)  # Left\n",
    "fig.update_yaxes(showticklabels=True, row=1, col=2)  # Right\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=1)\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c23057",
   "metadata": {},
   "source": [
    "### loading models to calculate full R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/saved_models/pitch/check'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame\n",
    "train_metrics_df_Xgboost_pitch_new= pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['pitch']\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "   # extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "    \n",
    "    # prepare training data\n",
    "    dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_pitch,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "    \n",
    "    y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "    X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "    X_feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    # predict on train data\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_train_df= pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nd': -1*nf,\n",
    "            }\n",
    "    \n",
    "    for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "           \n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            \n",
    "    train_metrics_df_Xgboost_pitch = pd.concat([train_metrics_df_Xgboost_pitch, pd.DataFrame([metrics_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics DataFrame\n",
    "train_metrics_df_Xgboost_pitch_new.to_csv('Xgboost/metrics_outpus/pitch_only/train_metrics_df_Xgboost_pitch_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d13f1",
   "metadata": {},
   "source": [
    "### Testing The selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ed774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/saved_models/pitch/best_old'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_case='Tp6p8s_Hs2m'\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base folder where results will be saved\n",
    "save_folder = 'XGboost/results/1dof_model/pitch/test_best_models'  # you can change this to any directory you want\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_metrics = {}  # Store separate metrics DataFrames per model\n",
    "y_true_pred_dict = {}    # Store true and predicted values per model and case\n",
    "\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "    \n",
    "    # Extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "\n",
    "    metrics_df_test_all = []  # Per-model metrics list\n",
    "    model_y_true_pred = {}          # Per-model true/predicted dict\n",
    "\n",
    "    cases = df_test_full['test_name'].unique()\n",
    "\n",
    "    for case in cases:\n",
    "        df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "\n",
    "        # Prepare the test data\n",
    "        dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "            df=df_case_test,\n",
    "            input_cols=input_cols,\n",
    "            output_cols=output_cols,\n",
    "            scaler_X_func=scaler_X_func_all,\n",
    "            scaler_y_func=scaler_y_func_pitch,\n",
    "            na=na,\n",
    "            nb_past=nb,\n",
    "            nf_future=nf,\n",
    "            test_name_col='test_name',\n",
    "            y_initial_mode='original'\n",
    "        )\n",
    "\n",
    "        X_test_selected_df = dfx_test.drop(columns='test_name')\n",
    "        X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "        # Predict\n",
    "        y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_test_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "        )\n",
    "\n",
    "        y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "        y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "        y_true_test = scaler_y_pitch.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "        y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "        # Save predictions and ground truth\n",
    "        model_y_true_pred[case] = {\n",
    "            'y_true': y_true_test_df,\n",
    "            'y_pred': y_pred_test_df\n",
    "        }\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics_test = {\n",
    "            'model': fname,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'test case': case\n",
    "        }\n",
    "\n",
    "        if case == training_case:\n",
    "            metrics_test['Comments'] = 'case used for training'\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "            r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "            metrics_test[f'r2_test_{col}'] = r2_test\n",
    "            metrics_test[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "        metrics_df_test_all.append(metrics_test)\n",
    "\n",
    "    # Store per-model data\n",
    "    metrics_df_test_all = pd.DataFrame(metrics_df_test_all)\n",
    "    all_models_metrics[fname] = metrics_df_test_all\n",
    "    y_true_pred_dict[fname] = model_y_true_pred\n",
    "\n",
    "    # Optionally display or save\n",
    "    print(f\"\\nMetrics for model {fname}:\")\n",
    "    display(metrics_df_test_all)\n",
    "\n",
    "   # Save the metrics DataFrame for this model\n",
    "    metric_save_path = os.path.join(save_folder, f'metrics_pitch_{fname}.csv')\n",
    "    metrics_df_test_all.to_csv(metric_save_path, index=False)\n",
    "   \n",
    "    # Save predictions and true values if needed\n",
    "    for case, data in model_y_true_pred.items():\n",
    "        y_true_path = os.path.join(save_folder, f'{fname}_case_pitch_{case}_y_true.csv')\n",
    "        y_pred_path = os.path.join(save_folder, f'{fname}_case_pitch_{case}_y_pred.csv')\n",
    "        \n",
    "        data['y_true'].to_csv(y_true_path, index=False)\n",
    "        data['y_pred'].to_csv(y_pred_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b97f1",
   "metadata": {},
   "source": [
    "## 3- Pendulum "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2da0c0",
   "metadata": {},
   "source": [
    "### Finding Optimal Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e415ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Xgboost/saved_models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497cdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_Xgboost_pendulum_only = pd.DataFrame()\n",
    "perf_df_Xgboost_pendulum_only = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity', 'eta_acceleration']\n",
    "output_cols=['pendulum'] \n",
    "case='Tp6p8s_Hs2m'\n",
    "na_max  = [2]\n",
    "nb_max= [0,1,2,3,4,5,6,7,8,9,10]\n",
    "nf_max=[0,1,2,3,4,5,6,7,8,9,10]\n",
    "best_params_pendulum = loaded_params['pendulum']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3000e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params['pendulum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for na in na_max :\n",
    "    for nb in nb_max :\n",
    "      for nf in nf_max:\n",
    "      \n",
    "\n",
    "        # Model parameters\n",
    "        model_name = F'Xgboost_pendulum_ver1_'  + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the pipeline model\n",
    "        model = XGBRegressor(**best_params)\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} ----')\n",
    "    \n",
    "       \n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func ,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "        # prepare validation data\n",
    "        df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "        \n",
    "        y_val_target = dfy_val[output_cols].reset_index(drop=True)\n",
    "\n",
    "        X_val=dfx_val.drop(columns='test_name')\n",
    "      \n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train  ,y_train_target )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train[0:10000],\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_val,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_pend.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols][0:10000].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_pend.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_Xgboost_pendulum_only= pd.concat([metrics_df_Xgboost_pendulum_only, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        perf_df_Xgboost_pendulum_only = pd.concat([perf_df_Xgboost_pendulum_only, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"Xgboost/metrics_outpus/pendulum_only\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_Xgboost_pendulum_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_Xgboost_pendulum_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_Xgboost_pendulum_only to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_df_Xgboost_pendulum_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_df_Xgboost_pendulum_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_df_Xgboost_pendulum_only to: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a218c",
   "metadata": {},
   "source": [
    "### Plotting Data From Remote Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e739564",
   "metadata": {},
   "source": [
    "ver 1= eta only input\n",
    "ver 2 = eta + eta vel + eta acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics DataFrame\n",
    "metrics_df_Xgboost_pendulum_only_ver1= pd.read_csv('Xgboost/R_M/metrics_outpus/pendulum_only/metrics_df_Xgboost_pendulum_only_ver1.csv')\n",
    "metrics_df_Xgboost_pendulum_only_ver2= pd.read_csv('Xgboost/R_M/metrics_outpus/pendulum_only/metrics_df_Xgboost_pendulum_only_ver2.csv')\n",
    "metrics_df_Xgboost_pendulum_only_ver2_extra= pd.read_csv('Xgboost/R_M/metrics_outpus/pendulum_only/metrics_df_Xgboost_pendulum_only_ver2_extra.csv')\n",
    "metrics_df_Xgboost_pendulum_only_ver2_extra_2=pd.read_csv('Xgboost/R_M/metrics_outpus/pendulum_only/metrics_df_Xgboost_pendulum_only_ver2_extra_2.csv')\n",
    "metrics_df_Xgboost_pendulum_only_ver2_final=pd.concat([metrics_df_Xgboost_pendulum_only_ver2, metrics_df_Xgboost_pendulum_only_ver2_extra, metrics_df_Xgboost_pendulum_only_ver2_extra_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735449eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting in case of eta only as an input feature\n",
    "\n",
    "# Set target output variable\n",
    "target_var = 'pendulum'\n",
    "df_plot = metrics_df_Xgboost_pendulum_only_ver1.copy()\n",
    "\n",
    "# Filter for only na = 2\n",
    "df_plot = df_plot[df_plot['na'] == 2]\n",
    "\n",
    "# Unique nb values (will now be legend entries)\n",
    "unique_nb = sorted(df_plot['nb'].unique())\n",
    "color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(unique_nb)}\n",
    "\n",
    "# Create subplots (Train and Validation)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"R² Train\", \"R² Val\"],\n",
    "    shared_yaxes=False\n",
    ")\n",
    "\n",
    "# Loop through nb values to plot curves vs nd\n",
    "for nb_val in unique_nb:\n",
    "    df_nb = df_plot[df_plot['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "    color = color_map[nb_val]\n",
    "    \n",
    "    # Check if nb is the one to highlight\n",
    "    if nb_val == -1:\n",
    "        line_width = 4    # Thicker line for highlighting\n",
    "        marker_size = 12  # Larger markers for highlighting\n",
    "        marker_symbol = 'star'\n",
    "    else:\n",
    "        line_width = 2\n",
    "        marker_size = 8\n",
    "        marker_symbol = 'circle'\n",
    "\n",
    "    # Compute nd = -nf\n",
    "    df_nb['nd'] = -df_nb['nf']\n",
    "\n",
    "    # Add train R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_train_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            line=dict(color=color, width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add val R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_val_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color , width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    title_text=(\"R² Performance of One XGBoost Model for Each DOF<br>η Only as an Input Feature, na = 2 (Pendulum)\"),\n",
    "    xaxis_title=\"nd \",\n",
    "    xaxis2_title=\"nd\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=460,\n",
    "    width=1000,\n",
    "    legend_title=\"nb\",\n",
    "    yaxis1=dict(range=[0, 0.6]),  # Set y-axis limits to show R² clearly\n",
    "    yaxis2=dict(range=[0, 0.6]),  # Set y-axis limits to show R² clearly\n",
    ")\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=1)\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for case on eta + eta vel + eta acc as input features\n",
    "\n",
    "# Set target output variable\n",
    "target_var = 'pendulum'\n",
    "df_plot = metrics_df_Xgboost_pendulum_only_ver2_final.copy()\n",
    "\n",
    "# Filter for only na = 2\n",
    "df_plot = df_plot[df_plot['na'] == 2]\n",
    "\n",
    "# Unique nb values (will now be legend entries)\n",
    "unique_nb = sorted(df_plot['nb'].unique())\n",
    "color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, nb in enumerate(unique_nb)}\n",
    "color_map[10] = px.colors.qualitative.D3[8]  # just another predefined color\n",
    "color_map[11] = px.colors.qualitative.D3[3]  # just another predefined color\n",
    "# Create subplots (Train and Validation)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"R² Train\", \"R² Val\"],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Loop through nb values to plot curves vs nd\n",
    "for nb_val in unique_nb:\n",
    "    df_nb = df_plot[df_plot['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "    color = color_map[nb_val]\n",
    "    \n",
    "    # Check if nb is the one to highlight\n",
    "    if nb_val == 22:\n",
    "        line_width = 4    # Thicker line for highlighting\n",
    "        marker_size = 12  # Larger markers for highlighting\n",
    "        marker_symbol = 'star'\n",
    "    else:\n",
    "        line_width = 2\n",
    "        marker_size = 8\n",
    "        marker_symbol = 'circle'\n",
    "\n",
    "    # Compute nd = -nf\n",
    "    df_nb['nd'] = -df_nb['nf']\n",
    "\n",
    "    # Add train R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_train_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            line=dict(color=color, width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add val R² trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_nb['nd'],\n",
    "            y=df_nb[f'r2_val_{target_var}'],\n",
    "            mode='lines+markers',\n",
    "            name=f'nb={nb_val}',\n",
    "            legendgroup=f'nb={nb_val}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color , width=line_width),\n",
    "            marker=dict(symbol=marker_symbol, size=marker_size, color=color)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    title_text=\"R² Performance of One XGBoost Model for Each DOF<br>η, η̇ , and η̈  as Input Features, na = 2 (Pendulum)\",\n",
    "    xaxis_title=\"nd \",\n",
    "    xaxis2_title=\"nd\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=550,\n",
    "    width=1000,\n",
    "    legend_title=\"nb\",\n",
    "    \n",
    "    \n",
    ")\n",
    "fig.update_yaxes(showticklabels=True, row=1, col=1)  # Left\n",
    "fig.update_yaxes(showticklabels=True, row=1, col=2)  # Right\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=1)\n",
    "fig.update_xaxes(autorange=\"reversed\", row=1, col=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcad90",
   "metadata": {},
   "source": [
    "### loading models to calculate full R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ec028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/saved_models/pendulum/best_combination'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame\n",
    "train_metrics_df_Xgboost_pendulum= pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8204a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['pendulum']\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "   # extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "    \n",
    "    # prepare training data\n",
    "    dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "    \n",
    "    y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "    X_train=dfx_train.drop(columns='test_name')\n",
    "\n",
    "    X_feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    # predict on train data\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_train_df= pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nd': -1*nf,\n",
    "            }\n",
    "    \n",
    "    for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "           \n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            \n",
    "    train_metrics_df_Xgboost_pendulum = pd.concat([train_metrics_df_Xgboost_pendulum, pd.DataFrame([metrics_row])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfe9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics DataFrame\n",
    "train_metrics_df_Xgboost_pendulum.to_csv('Xgboost/metrics_outpus/pendulum_only/train_metrics_df_Xgboost_pendulum.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7cdde",
   "metadata": {},
   "source": [
    "\n",
    "### Testing The selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70faec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models from the specified folder\n",
    "folder = 'Xgboost/R_M/saved_models/pendulum/best_combination'\n",
    "\n",
    "# List all .joblib model files\n",
    "model_files = [f for f in os.listdir(folder) if f.endswith('.joblib')]\n",
    "\n",
    "# Load all models into a dictionary\n",
    "models = {}\n",
    "for fname in model_files:\n",
    "    model_path = os.path.join(folder, fname)\n",
    "    model = joblib.load(model_path)\n",
    "    models[fname] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_case='Tp6p8s_Hs2m'\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['pendulum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base folder where results will be saved\n",
    "save_folder = 'XGboost/results/1dof_model/pendulum/test_best_models'  # you can change this to any directory you want\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_metrics = {}  # Store separate metrics DataFrames per model\n",
    "y_true_pred_dict = {}    # Store true and predicted values per model and case\n",
    "\n",
    "for fname, model in models.items():\n",
    "    print(f\"Filename: {fname}\")\n",
    "    \n",
    "    # Extract na, nb, nf from filenames\n",
    "    pattern = r'na(\\d+)_nb(\\d+)_nf(\\d+)'\n",
    "    match = re.search(pattern, fname)\n",
    "    na, nb, nf = map(int, match.groups())\n",
    "    print(f\"{fname} ➤ na: {na}, nb: {nb}, nf: {nf}\")\n",
    "\n",
    "    metrics_df_test_all = []  # Per-model metrics list\n",
    "    model_y_true_pred = {}          # Per-model true/predicted dict\n",
    "\n",
    "    cases = df_test_full['test_name'].unique()\n",
    "\n",
    "    for case in cases:\n",
    "        df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "\n",
    "        # Prepare the test data\n",
    "        dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "            df=df_case_test,\n",
    "            input_cols=input_cols,\n",
    "            output_cols=output_cols,\n",
    "            scaler_X_func=scaler_X_func_all,\n",
    "            scaler_y_func=scaler_y_func_pend,\n",
    "            na=na,\n",
    "            nb_past=nb,\n",
    "            nf_future=nf,\n",
    "            test_name_col='test_name',\n",
    "            y_initial_mode='original'\n",
    "        )\n",
    "\n",
    "        X_test_selected_df = dfx_test.drop(columns='test_name')\n",
    "        X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "        # Predict\n",
    "        y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_test_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "        )\n",
    "\n",
    "        y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "        y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "        y_true_test = scaler_y_pend.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "        y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "        # Save predictions and ground truth\n",
    "        model_y_true_pred[case] = {\n",
    "            'y_true': y_true_test_df,\n",
    "            'y_pred': y_pred_test_df\n",
    "        }\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics_test = {\n",
    "            'model': fname,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'test case': case\n",
    "        }\n",
    "\n",
    "        if case == training_case:\n",
    "            metrics_test['Comments'] = 'case used for training'\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "            r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "            metrics_test[f'r2_test_{col}'] = r2_test\n",
    "            metrics_test[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "        metrics_df_test_all.append(metrics_test)\n",
    "\n",
    "    # Store per-model data\n",
    "    metrics_df_test_all = pd.DataFrame(metrics_df_test_all)\n",
    "    all_models_metrics[fname] = metrics_df_test_all\n",
    "    y_true_pred_dict[fname] = model_y_true_pred\n",
    "\n",
    "    # Optionally display or save\n",
    "    print(f\"\\nMetrics for model {fname}:\")\n",
    "    display(metrics_df_test_all)\n",
    "\n",
    "   # Save the metrics DataFrame for this model\n",
    "    metric_save_path = os.path.join(save_folder, f'metrics_pendulum_{fname}.csv')\n",
    "    metrics_df_test_all.to_csv(metric_save_path, index=False)\n",
    "   \n",
    "    # Save predictions and true values if needed\n",
    "    for case, data in model_y_true_pred.items():\n",
    "        y_true_path = os.path.join(save_folder, f'{fname}_case_pendulum_{case}_y_true.csv')\n",
    "        y_pred_path = os.path.join(save_folder, f'{fname}_case_pendulum_{case}_y_pred.csv')\n",
    "        \n",
    "        data['y_true'].to_csv(y_true_path, index=False)\n",
    "        data['y_pred'].to_csv(y_pred_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ca583",
   "metadata": {},
   "source": [
    "# Computing time and emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecarbon\n",
    "print(codecarbon.__version__)\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04ac65",
   "metadata": {},
   "source": [
    "We adjust the data sizes to be devisible by 64 (like in lstm case) for a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "def adjust_for_batch_df(df, y):\n",
    "    # Calculate the number of rows that are divisible by the batch size\n",
    "    n = (df.shape[0] // batch_size) * batch_size\n",
    "    \n",
    "    # Adjust DataFrame and target variable\n",
    "    df_adjusted = df.iloc[:n]  # Select the rows to match the batch size\n",
    "    y_adjusted = y[:n]         # Adjust the target variable in the same way\n",
    "    \n",
    "    return df_adjusted, y_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open(\"Xgboost/hyperparameters/eta_Vel_acc/best_params_eta_Vel_acc.json\", \"r\") as f:\n",
    "    loaded_params = json.load(f)\n",
    "\n",
    "print(\"Loaded params:\", loaded_params)\n",
    "# Accumulate values\n",
    "accumulator = defaultdict(list)\n",
    "for params in loaded_params.values():\n",
    "    for key, val in params.items():\n",
    "        accumulator[key].append(val)\n",
    "\n",
    "# Define keys you want to round to integers\n",
    "int_keys = {\"max_depth\", \"n_estimators\"}\n",
    "\n",
    "# Compute averages\n",
    "xgb_params = {\n",
    "    k: int(np.round(np.mean(v))) if k in int_keys else float(np.mean(v))\n",
    "    for k, v in accumulator.items()\n",
    "}\n",
    "\n",
    "print(\"Averaged parameters:\", xgb_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "case='Tp6p8s_Hs2m'\n",
    "case_test='Tp6p8s_Hs1m'\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "df_case_test=df_train_full[df_train_full['test_name']==case_test].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "input_cols=['eta','eta_velocity','eta_acceleration']\n",
    "output_cols=['heave', 'pitch',  'pendulum'] \n",
    "na  = 2\n",
    "nb= 0\n",
    "nf = 10\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "X_train=dfx_train.drop(columns='test_name')\n",
    "# get feature names\n",
    "X_feature_names = X_train.columns.tolist()\n",
    "\n",
    "X_train, y_train_target = adjust_for_batch_df(X_train, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "# Create the base model\n",
    "base_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Wrap it for multi-output regression\n",
    "model = MultiOutputRegressor(base_model)\n",
    "\n",
    "model.fit(X_train  ,y_train_target )\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"xgb_train.csv\"    # Custom filename\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "# Create the base model\n",
    "base_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Wrap it for multi-output regression\n",
    "model = MultiOutputRegressor(base_model)\n",
    "\n",
    "model.fit(X_train  ,y_train_target )\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "                df = df_case_test,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_all ,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "y_test_target = dfy_test[output_cols].reset_index(drop=True)\n",
    "\n",
    "x_test=dfx_test.drop(columns='test_name')\n",
    "\n",
    "x_test, y_test_target = adjust_for_batch_df(x_test, y_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eacd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# parralel prediction \n",
    "\n",
    "y_pred_parralel_test=model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40dc720",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"xgb_parralel_pred.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "\n",
    "# parralel prediction \n",
    "\n",
    "y_pred_parralel_test=model.predict(x_test)\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc93fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Series prediction \n",
    "\n",
    "y_pred_test_scaled,x_used_test = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=x_test,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"xgb_series.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "# Series prediction \n",
    "\n",
    "y_pred_test_scaled,x_used_test = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=x_test,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32be667",
   "metadata": {},
   "source": [
    "# Noisy data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load noisy data\n",
    "model = joblib.load('Xgboost/Xgboost_3dof_ver2_extra_na2_nb0_nf10.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the best model and predict on the test data\n",
    "import os\n",
    "\n",
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 0\n",
    "best_nf = 10\n",
    "\n",
    "\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['heave','pitch','pendulum']\n",
    "# load the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfcee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_case_test_noisy=pd.read_csv('Results/df_case_test_noisy.csv')\n",
    "scaler_X_func_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data\n",
    "dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "    df=df_case_test_noisy.reset_index(drop=True),\n",
    "    input_cols=best_input_cols,\n",
    "    output_cols=best_output_cols,\n",
    "    scaler_X_func   = scaler_X_func_all,\n",
    "    scaler_y_func   = scaler_y_func,\n",
    "    na=best_na,\n",
    "    nb_past=best_nb,\n",
    "    nf_future=best_nf,\n",
    "    test_name_col='test_name',\n",
    "    y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1eea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_selected_df=dfx_test.drop(columns='test_name')\n",
    "\n",
    "\n",
    "\n",
    "# get feature names from model\n",
    "X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=best_output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=best_na\n",
    ")\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)\n",
    "# Convert to DataFrame\n",
    "y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "# Get true values aligned with dfy_test indexes\n",
    "y_true_test = scaler_y.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "# Compute metrics\n",
    "metrics_test = {}\n",
    "\n",
    "\n",
    "for col in best_output_cols:\n",
    "    mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "    r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "    \n",
    "    metrics_test[f'r2_test_{col}'] = r2_test\n",
    "    metrics_test[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "\n",
    "\n",
    "print(\"metrics:\", metrics_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "y_true_test_df.to_csv(f'Results/noisy test/XGB_true.csv', index=False)\n",
    "y_pred_test_df.to_csv(f'Results/noisy test/XGB_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
