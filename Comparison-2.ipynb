{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da377b4",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chardet\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt \n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import plotly.colors as pc\n",
    "%matplotlib inline  \n",
    "import psutil\n",
    "from pathlib import Path\n",
    "#from Functions import *\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3174a68",
   "metadata": {},
   "source": [
    "# Test error acumlation over Data Lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "# ARX\n",
    "ARX_testing_heave_true=pd.read_csv('Results/testing for comparison/ARX_true_heave.csv')[:-5]\n",
    "ARX_testing_heave_pred=pd.read_csv('Results/testing for comparison/ARX_pred_heave.csv')[:-5]\n",
    "\n",
    "ARX_testing_pitch_true=pd.read_csv('Results/testing for comparison/ARX_true_pitch.csv')[:-5]\n",
    "ARX_testing_pitch_pred=pd.read_csv('Results/testing for comparison/ARX_pred_pitch.csv')[:-5]\n",
    "\n",
    "ARX_testing_pendulum_true=pd.read_csv('Results/testing for comparison/ARX_true_pendulum.csv')[:-10]\n",
    "ARX_testing_pendulum_pred=pd.read_csv('Results/testing for comparison/ARX_pred_pendulum.csv')[:-10]\n",
    "\n",
    "#XGB\n",
    "XGB_testing_true=pd.read_csv('Results/testing for comparison/xgb_true.csv')[:-2]\n",
    "XGB_testing_pred=pd.read_csv('Results/testing for comparison/xgb_pred.csv')[:-2]\n",
    "\n",
    "#LSTM\n",
    "LSTM_testing_true=pd.read_csv('Results/testing for comparison/lstm_true.csv')\n",
    "LSTM_testing_pred=pd.read_csv('Results/testing for comparison/lstm_pred.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ARX Heave True:\", len(ARX_testing_heave_true))\n",
    "print(\"ARX Heave Pred:\", len(ARX_testing_heave_pred))\n",
    "print(\"ARX Pitch True:\", len(ARX_testing_pitch_true))\n",
    "print(\"ARX Pitch Pred:\", len(ARX_testing_pitch_pred))\n",
    "print(\"ARX Pendulum True:\", len(ARX_testing_pendulum_true))\n",
    "print(\"ARX Pendulum Pred:\", len(ARX_testing_pendulum_pred))\n",
    "\n",
    "print(\"XGB True:\", len(XGB_testing_true))\n",
    "print(\"XGB Pred:\", len(XGB_testing_pred))\n",
    "\n",
    "print(\"LSTM True:\", len(LSTM_testing_true))\n",
    "print(\"LSTM Pred:\", len(LSTM_testing_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that all True data sets are identical\n",
    "\n",
    "a = ARX_testing_pendulum_true['pendulum'].to_numpy()\n",
    "b = XGB_testing_true['pendulum'].to_numpy()\n",
    "c = LSTM_testing_true['pendulum'].to_numpy()  # this is just 'a' again\n",
    "\n",
    "are_all_equal = np.all((a == b) & (b == c))  # or (a == b) & (a == c)\n",
    "\n",
    "print(\"All equal:\", are_all_equal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARX Heave Absolute Error\n",
    "errors_ARX_heave = np.abs(ARX_testing_heave_true['heave'] - ARX_testing_heave_pred['heave'])\n",
    "\n",
    "# ARX Pitch Absolute Error\n",
    "errors_ARX_pitch = np.abs(ARX_testing_pitch_true['pitch'] - ARX_testing_pitch_pred['pitch'])\n",
    "\n",
    "# ARX Pendulum Absolute Error\n",
    "errors_ARX_pendulum = np.abs(ARX_testing_pendulum_true['pendulum'] - ARX_testing_pendulum_pred['pendulum'])\n",
    "\n",
    "# XGB Absolute Error\n",
    "errors_XGB_heave = np.abs(XGB_testing_true['heave'] - XGB_testing_pred['heave'])\n",
    "\n",
    "errors_XGB_pitch = np.abs(XGB_testing_true['pitch'] - XGB_testing_pred['pitch'])\n",
    "\n",
    "errors_XGB_pendulum = np.abs(XGB_testing_true['pendulum'] - XGB_testing_pred['pendulum'])\n",
    "\n",
    "# LSTM Absolute Error\n",
    "errors_LSTM_heave = np.abs(LSTM_testing_true['heave'] - LSTM_testing_pred['heave_pred'])\n",
    "\n",
    "errors_LSTM_pitch = np.abs(LSTM_testing_true['pitch'] - LSTM_testing_pred['pitch_pred'])\n",
    "\n",
    "errors_LSTM_pendulum = np.abs(LSTM_testing_true['pendulum'] - LSTM_testing_pred['pendulum_pred'])\n",
    "\n",
    "# Time steps (or indices for x-axis)\n",
    "time_steps = np.arange(len(errors_ARX_heave))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1909d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Degrees of freedom (DoF) for which plots will be generated\n",
    "dofs = ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "# Loop over each DoF\n",
    "for dof in dofs:\n",
    "    # Create the figure for absolute errors over time for the current DoF\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "    # Select the appropriate error variables based on the DoF\n",
    "    errors_ARX = globals()[f\"errors_ARX_{dof}\"]\n",
    "    errors_XGB = globals()[f\"errors_XGB_{dof}\"]\n",
    "    errors_LSTM = globals()[f\"errors_LSTM_{dof}\"]\n",
    "\n",
    "    # Add the traces for each model and current DoF (without markers)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=errors_ARX,\n",
    "        mode=\"lines\",  # Only plot lines, no markers\n",
    "        name=\"ARX\",\n",
    "        line=dict(color=\"blue\", dash=\"dash\")  # No marker, just line\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=errors_XGB,\n",
    "        mode=\"lines\",  # Only plot lines, no markers\n",
    "        name=\"XGBoost-NARX\",\n",
    "        line=dict(color=\"red\", dash=\"dot\")  # No marker, just line\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=errors_LSTM,\n",
    "        mode=\"lines\",  # Only plot lines, no markers\n",
    "        name=\"LSTM\",\n",
    "        line=dict(color=\"green\", dash=\"dashdot\")  # No marker, just line\n",
    "    ))\n",
    "\n",
    "    # Update layout and display\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Absolute Error for {dof.capitalize()} Over Time - Tp = 6.8 s, Hs = 1 m\",\n",
    "        template=\"plotly_white\",\n",
    "        height=420,  # Reduced height to make the plot lighter\n",
    "        showlegend=True,\n",
    "        plot_bgcolor=\"white\",  # Simplified background for a cleaner look\n",
    "    )\n",
    "\n",
    "    # Set the y-axis label with the appropriate unit based on DoF\n",
    "    if dof == 'heave':\n",
    "        fig.update_yaxes(title_text=\"Absolute Error (m)\", row=1, col=1)  # Units in meters for Heave\n",
    "    else:\n",
    "        fig.update_yaxes(title_text=\"Absolute Error (°)\", row=1, col=1)  # Units in degrees for Pitch and Pendulum\n",
    "\n",
    "    # Set the x-axis label\n",
    "    fig.update_xaxes(title_text=\"Time Step\")\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"Results/error vs time/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fig.write_image(f\"{output_dir}{dof}_absolute_error_plot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2605df",
   "metadata": {},
   "source": [
    "# senstivety to intial conditoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a158441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data for R² scores\n",
    "original_r2 = {\n",
    "    \"Model\": [\"ARX\", \"ARX\", \"ARX\", \"XGBoost-NARX\", \"XGBoost-NARX\", \"XGBoost-NARX\", \"LSTM\", \"LSTM\", \"LSTM\"],\n",
    "    \"Dataset\": [\"Training\", \"Validation\", \"Testing\", \"Training\", \"Validation\", \"Testing\", \"Training\", \"Validation\", \"Testing\"],\n",
    "    \"Heave\": [0.989, 0.980, 0.972, 0.959, 0.965, 0.969, 0.994, 0.994, 0.994],\n",
    "    \"Pitch\": [0.991, 0.959, 0.978, 0.892, 0.893, 0.901, 0.985, 0.980, 0.982],\n",
    "    \"Pendulum\": [0.944, 0.889, 0.935, 0.840, 0.841, 0.846, 0.989, 0.987, 0.976]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_r2_original = pd.DataFrame(original_r2)\n",
    "\n",
    "df_r2_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c698c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data for R² scores\n",
    "trimmed_r2 = {\n",
    "    \"Model\": [\"ARX\", \"ARX\", \"ARX\", \"XGBoost-NARX\", \"XGBoost-NARX\", \"XGBoost-NARX\", \"LSTM\", \"LSTM\", \"LSTM\"],\n",
    "    \"Dataset\": [\"Training\", \"Validation\", \"Testing\", \"Training\", \"Validation\", \"Testing\", \"Training\", \"Validation\", \"Testing\"],\n",
    "    \"Heave\": [0.989, 0.989, 0.991, 0.959, 0.966, 0.970, 0.994, 0.994, 0.995],\n",
    "    \"Pitch\": [0.991, 0.991, 0.992, 0.893, 0.894, 0.897, 0.985, 0.984, 0.986],\n",
    "    \"Pendulum\": [0.960, 0.962, 0.965, 0.841, 0.846, 0.845, 0.989, 0.988, 0.991]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_r2_trimmed = pd.DataFrame(trimmed_r2)\n",
    "\n",
    "df_r2_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each model\n",
    "colors = {\n",
    "    \"ARX\": \"blue\",\n",
    "    \"XGBoost-NARX\": \"red\",\n",
    "    \"LSTM\": \"green\"\n",
    "}\n",
    "\n",
    "# Create subplots with 3 columns, one for each DoF\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[\"Heave\", \"Pitch\", \"Pendulum\"])\n",
    "\n",
    "# Loop over each DoF and plot the markers for each model\n",
    "for idx, dof in enumerate(['Heave', 'Pitch', 'Pendulum'], 1):\n",
    "    for model in ['ARX', 'XGBoost-NARX', 'LSTM']:\n",
    "        # Get the data for original and trimmed for each model and DoF\n",
    "        original_data = df_r2_original[df_r2_original['Model'] == model][dof]\n",
    "        trimmed_data = df_r2_trimmed[df_r2_trimmed['Model'] == model][dof]\n",
    "\n",
    "        # Plot the markers for the original data\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_r2_original['Dataset'].unique(),\n",
    "            y=original_data,\n",
    "            mode='markers',\n",
    "            name=f'{model} - Original',\n",
    "            marker=dict(color=colors[model], symbol='circle', size=10),\n",
    "            legendgroup=model,\n",
    "            showlegend=(idx == 3)  # Show legend only for the last subplot (Pendulum)\n",
    "        ), row=1, col=idx)\n",
    "\n",
    "        # Plot the markers for the trimmed data\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_r2_trimmed['Dataset'].unique(),\n",
    "            y=trimmed_data,\n",
    "            mode='markers',\n",
    "            name=f'{model} - Trimmed',\n",
    "            marker=dict(color=colors[model], symbol='x', size=10),\n",
    "            legendgroup=model,\n",
    "            showlegend=(idx == 3)  # Show legend only for the last subplot (Pendulum)\n",
    "        ), row=1, col=idx)\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    xaxis_title=\"Dataset\",\n",
    "    yaxis_title=\"R² Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=600,\n",
    "    legend_title=\"Model and Dataset\",\n",
    "    showlegend=True,\n",
    "    xaxis2=dict(title=\"Dataset\"),  \n",
    "    xaxis3=dict(title=\"\"),\n",
    ")\n",
    "\n",
    "# Hiding x-axis title for the first subplot (Heave)\n",
    "fig.update_xaxes(title=\"\", row=1, col=1)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326f480",
   "metadata": {},
   "source": [
    "# Noise handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = pd.read_csv('prepared_data/test_data.csv')\n",
    "# define test case\n",
    "trainin_case='Tp6p8s_Hs2m'\n",
    "df_case_test = df_test_full[df_test_full['test_name'] == trainin_case].copy()\n",
    "df_case_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(signal, f_samp=None):\n",
    "    \"\"\"\n",
    "    [FT, freq] = fourier(signal, f_samp)\n",
    "    evaluates Fourier transform with normalisation 2/N\n",
    "    \n",
    "    Input arguments:\n",
    "    signal : array-like, signal data (will be converted to column)\n",
    "    f_samp : float, sampling frequency in Hz (optional, required to evaluate freq)\n",
    "    \n",
    "    Outputs, expressed between f=0 and f=f_samp/2:\n",
    "    FT : complex numpy array, Fourier transform (contains floor(N/2)+1 points)\n",
    "    freq : numpy array, frequency vector in Hz (only if f_samp provided)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to column vector (1D numpy array)\n",
    "    signal = np.asarray(signal).flatten()\n",
    "    N = len(signal)\n",
    "    \n",
    "    # Apply FFT with 2/N normalization\n",
    "    FT = 2 * np.fft.fft(signal) / N\n",
    "    \n",
    "    # Constant (f=0) mode - divide DC component by 2\n",
    "    FT[0] = FT[0] / 2\n",
    "    \n",
    "    # Removing negative frequencies\n",
    "    # Last Fourier mode index\n",
    "    N_last = N // 2 + 1\n",
    "    FT = FT[:N_last]\n",
    "    \n",
    "    # Handle frequency vector output\n",
    "    if f_samp is not None:\n",
    "        T_d = N / f_samp\n",
    "        # Frequency vector\n",
    "        freq = np.arange(N_last) / T_d\n",
    "        return FT, freq\n",
    "    else:\n",
    "        return FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19989194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale noise to 10% of the clean signal\n",
    "def scale_noise_to_5_percent(clean_signal, white_noise):\n",
    "    # Calculate the RMS (Root Mean Square) of the clean signal\n",
    "    rms_clean_signal = np.sqrt(np.mean(clean_signal**2))\n",
    "\n",
    "    # Calculate the RMS of the white noise\n",
    "    rms_white_noise = np.sqrt(np.mean(white_noise**2))\n",
    "\n",
    "    # Scale the white noise to be 5% of the clean signal's amplitude\n",
    "    scaling_factor = 0.05 * (rms_clean_signal / rms_white_noise)\n",
    "\n",
    "    # Scale the white noise\n",
    "    scaled_white_noise = white_noise * scaling_factor\n",
    "\n",
    "    print(rms_clean_signal)\n",
    "    return scaled_white_noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clean signals from the dataframe\n",
    "eta = df_case_test['eta'].values\n",
    "eta_velocity = df_case_test['eta_velocity'].values\n",
    "eta_acceleration = df_case_test['eta_acceleration'].values\n",
    "\n",
    "# Length of the signal\n",
    "length = len(eta)  # Assuming all signals have the same length\n",
    "\n",
    "# Sampling frequency (Hz)\n",
    "fs = 800\n",
    "\n",
    "# Generate white noise (broad spectrum)\n",
    "white_noise = np.random.randn(length)\n",
    "\n",
    "\n",
    "# Scale the white noise for each signal\n",
    "scaled_noise_eta = scale_noise_to_5_percent(eta, white_noise)\n",
    "scaled_noise_eta_velocity = scale_noise_to_5_percent(eta_velocity, white_noise)\n",
    "scaled_noise_eta_acceleration = scale_noise_to_5_percent(eta_acceleration, white_noise)\n",
    "\n",
    "# Add the scaled white noise to each clean signal\n",
    "noisy_eta = eta + scaled_noise_eta\n",
    "noisy_eta_velocity = eta_velocity + scaled_noise_eta_velocity\n",
    "noisy_eta_acceleration = eta_acceleration + scaled_noise_eta_acceleration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Plot for eta\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=eta, mode='lines', name='Clean eta'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=noisy_eta, mode='lines', name='Noisy eta (5% White Noise)', line=dict(color='orange')))\n",
    "\n",
    "# Plot for eta_velocity\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=eta_velocity, mode='lines', name='Clean eta_velocity'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=noisy_eta_velocity, mode='lines', name='Noisy eta_velocity (5% White Noise)', line=dict(color='orange')))\n",
    "\n",
    "# Plot for eta_acceleration\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=eta_acceleration, mode='lines', name='Clean eta_acceleration'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=noisy_eta_acceleration, mode='lines', name='Noisy eta_acceleration (5% White Noise)', line=dict(color='orange')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Clean and Noisy Signals with 5% White Noise\",\n",
    "    xaxis_title=\"Time (samples)\",\n",
    "    yaxis_title=\"Amplitude\",\n",
    "    template=\"plotly_white\",\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf226e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_test_noisy=df_case_test.copy()\n",
    "df_case_test_noisy['eta']=noisy_eta \n",
    "df_case_test_noisy['eta_velocity']=noisy_eta_velocity \n",
    "df_case_test_noisy['eta_acceleration']=noisy_eta_acceleration\n",
    "df_case_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478449b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the length of the signal\n",
    "length = len(eta)\n",
    "\n",
    "# Create a subplot with 3 rows and 1 column (for eta, eta_velocity, eta_acceleration)\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1,\n",
    "                    subplot_titles=['η ', 'η̇ ', 'η̈ '])\n",
    "\n",
    "# Plot for eta (Row 1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=eta, mode='lines', name='Clean η (eta)', line=dict(color='red', width=5)),  # Thicker red line\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=df_case_test_noisy['eta'], mode='lines', name='Noisy η (eta) (5% White Noise)', line=dict(color='black', width=3)),  # Black with normal thickness\n",
    "              row=1, col=1)\n",
    "\n",
    "# Plot for eta_velocity (Row 2) -> Use η̇ (eta_dot)\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=eta_velocity, mode='lines', name='Clean η̇ (eta_dot)', line=dict(color='red', width=5)),  # Thicker red line\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=df_case_test_noisy['eta_velocity'], mode='lines', name='Noisy η̇ (eta_dot) (5% White Noise)', line=dict(color='black', width=3)),  # Black with normal thickness\n",
    "              row=2, col=1)\n",
    "\n",
    "# Plot for eta_acceleration (Row 3) -> Use η̈ (eta_ddot)\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=eta_acceleration, mode='lines', name='Clean ', line=dict(color='red', width=5)),  # Thicker red line\n",
    "              row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(length), y=df_case_test_noisy['eta_acceleration'], mode='lines', name='Noisy (5% White Noise)', line=dict(color='black', width=3)),  # Black with normal thickness\n",
    "              row=3, col=1)\n",
    "\n",
    "# Update layout for the overall figure\n",
    "fig.update_layout(\n",
    "    title=\"Clean and Noisy Signals\",\n",
    "    xaxis_title=\"Time Steps\",  # Adding title to the x-axis for the last subplot\n",
    "    yaxis_title=\"Amplitude\",  # General title for y-axis\n",
    "    template=\"plotly_white\",\n",
    "    height=600,\n",
    "    width=1000,  # Height to accommodate 3 subplots\n",
    "    showlegend=True,\n",
    "    title_x=0.5  # Center the main title\n",
    ")\n",
    "\n",
    "# Update y-axis titles with units for each subplot\n",
    "fig.update_yaxes(title_text=\"η (m)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"η̇ (m/s)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"η̈ (m/s²)\", row=3, col=1)\n",
    "\n",
    "# Make x-axis label visible only for the last subplot (Row 3)\n",
    "fig.update_xaxes(title_text=\"Time Steps\", row=3, col=1)  # Only visible in the last subplot\n",
    "\n",
    "# Hide x-axis label for the first two subplots\n",
    "fig.update_xaxes(showticklabels=False, row=1, col=1)  # Hide x-axis label for the first subplot\n",
    "fig.update_xaxes(showticklabels=False, row=2, col=1)  # Hide x-axis label for the second subplot\n",
    "# Show legend only for the last subplot (Row 3)\n",
    "fig.update_traces(showlegend=False, row=1, col=1)  # Hide legend for the first subplot\n",
    "fig.update_traces(showlegend=False, row=2, col=1)  # Hide legend for the second subplot\n",
    "# Control x-axis range for the last subplot (Row 3)\n",
    "fig.update_xaxes(range=[2500, 2750], row=3, col=1)  # Set x-axis range for the last subplot\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the noisy data to a CSV file\n",
    "df_case_test_noisy.to_csv('df_case_test_noisy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7827246",
   "metadata": {},
   "source": [
    "Now we load true and predicted Data after using models to predcit on the noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "# ARX\n",
    "ARX_heave_true=pd.read_csv('Results/noisy test/ARX_heave_true.csv')[5:-33]\n",
    "ARX_heave_pred=pd.read_csv('Results/noisy test/ARX_heave_pred.csv')[5:-33]\n",
    "\n",
    "\n",
    "ARX_pitch_true=pd.read_csv('Results/noisy test/ARX_pitch_true.csv')[5:-33]\n",
    "ARX_pitch_pred=pd.read_csv('Results/noisy test/ARX_pitch_pred.csv')[5:-33]\n",
    "\n",
    "ARX_pendulum_true=pd.read_csv('Results/noisy test/ARX_pendulum_true.csv')[5:-38]\n",
    "ARX_pendulum_pred=pd.read_csv('Results/noisy test/ARX_pendulum_pred.csv')[5:-38]\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "XGB_true=pd.read_csv('Results/noisy test/XGB_true.csv')[5:-30]\n",
    "XGB_pred=pd.read_csv('Results/noisy test/XGB_pred.csv')[5:-30]\n",
    "\n",
    "\n",
    "#LSTM\n",
    "LSTM_true=pd.read_csv('Results/noisy test/LSTM_true.csv')\n",
    "LSTM_pred=pd.read_csv('Results/noisy test/LSTM_pred.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b95201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that all True data sets are identical\n",
    "\n",
    "a = ARX_pendulum_true['pendulum'].to_numpy()\n",
    "b = XGB_true['pendulum'].to_numpy()\n",
    "c = LSTM_true['pendulum'].to_numpy()  # this is just 'a' again\n",
    "\n",
    "are_all_equal = np.all((a == b) & (b == c))  # or (a == b) & (a == c)\n",
    "\n",
    "print(\"All equal:\", are_all_equal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ARX adjusted predictions\n",
    "arx_preds = {\n",
    "    'heave': ARX_heave_pred['heave'],\n",
    "    'pitch': ARX_pitch_pred['pitch'],\n",
    "    'pendulum': ARX_pendulum_pred['pendulum'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DoFs and corresponding y-axis units\n",
    "dofs = ['heave', 'pitch', 'pendulum']\n",
    "y_units = {\n",
    "    'heave': 'm',\n",
    "    'pitch': '°',\n",
    "    'pendulum': '°'\n",
    "}\n",
    "\n",
    "# Loop through each DoF\n",
    "for dof in dofs:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Ground Truth (solid black)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=LSTM_true[dof],\n",
    "        mode='lines',\n",
    "        name=f'{dof.capitalize()} - Ground Truth',\n",
    "        line=dict(color='black', dash='solid')\n",
    "    ))\n",
    "\n",
    "    # ARX Prediction (dashed blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=arx_preds[dof],\n",
    "        mode='lines',\n",
    "        name='ARX Prediction',\n",
    "        line=dict(color='blue', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # XGBoost Prediction (dashed red)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=XGB_pred[dof],\n",
    "        mode='lines',\n",
    "        name='XGBoost-NARX Prediction',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # LSTM Prediction (dashed green)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=LSTM_pred[f'{dof}_pred'],\n",
    "        mode='lines',\n",
    "        name='LSTM Prediction',\n",
    "        line=dict(color='green', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # Calculate dynamic y-axis limits (min/max of all predictions + ground truth)\n",
    "    y_min = min(LSTM_true[dof])\n",
    "    y_max = max(LSTM_true[dof])\n",
    "\n",
    "    # Optional: Add a padding factor (e.g., 5% more than the min/max range)\n",
    "    padding = 0.05  # 5% padding\n",
    "    y_min -= (y_max - y_min) * padding\n",
    "    y_max += (y_max - y_min) * padding\n",
    "\n",
    "    # Layout with dynamic y-axis range\n",
    "    fig.update_layout(\n",
    "        title=f'Noisy Data Predictions - {dof.capitalize()}',\n",
    "        xaxis_title='Time Step',\n",
    "        yaxis_title=f'{dof.capitalize()} [{y_units[dof]}]',\n",
    "        yaxis=dict(range=[y_min, y_max]),\n",
    "        xaxis=dict(range=[0,400]),# Dynamically set y-axis range\n",
    "        template='plotly_white',\n",
    "        \n",
    "        \n",
    "    )\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"Results/noisy test/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fig.write_image(f\"{output_dir}{dof}_absolute_error_plot.png\")\n",
    "    fig.write_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# Define DoFs and corresponding y-axis units\n",
    "dofs = ['heave', 'pitch', 'pendulum']\n",
    "y_units = {\n",
    "    'heave': 'm',\n",
    "    'pitch': '°',\n",
    "    'pendulum': '°'\n",
    "}\n",
    "\n",
    "# Loop through each DoF\n",
    "for dof in dofs:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Ground Truth (solid black)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=LSTM_true[dof],\n",
    "        mode='lines',\n",
    "        name=f'{dof.capitalize()} - Ground Truth',\n",
    "        line=dict(color='black', dash='solid')\n",
    "    ))\n",
    "\n",
    "    # ARX Prediction (dashed blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=arx_preds[dof],\n",
    "        mode='lines',\n",
    "        name='ARX Prediction',\n",
    "        line=dict(color='blue', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # XGBoost Prediction (dashed red)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=XGB_pred[dof],\n",
    "        mode='lines',\n",
    "        name='XGBoost-NARX Prediction',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # LSTM Prediction (dashed green)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=LSTM_pred[f'{dof}_pred'],\n",
    "        mode='lines',\n",
    "        name='LSTM Prediction',\n",
    "        line=dict(color='green', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # Calculate dynamic y-axis limits (min/max of all predictions + ground truth)\n",
    "    y_min = min(LSTM_true[dof])\n",
    "    y_max = max(LSTM_true[dof])\n",
    "\n",
    "    # Optional: Add a padding factor (e.g., 5% more than the min/max range)\n",
    "    padding = 0.05  # 5% padding\n",
    "    y_min -= (y_max - y_min) * padding\n",
    "    y_max += (y_max - y_min) * padding\n",
    "\n",
    "    # Layout with dynamic y-axis range\n",
    "    fig.update_layout(\n",
    "        title=f'Noisy Data Predictions - {dof.capitalize()}',\n",
    "        xaxis_title='Time Step',\n",
    "        yaxis_title=f'{dof.capitalize()} [{y_units[dof]}]',\n",
    "        yaxis=dict(range=[y_min, y_max]),\n",
    "        xaxis=dict(range=[0, 400]),  # Dynamically set x-axis range\n",
    "        template='plotly_white',\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c81d0",
   "metadata": {},
   "source": [
    "# Data size sensitivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153683bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "\n",
    "# Set the path where your CSV files are located\n",
    "path = 'Results/data_tests'\n",
    "\n",
    "# Use glob to find all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "# Dictionary to store each CSV as a separate DataFrame\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each file and load it into the dictionary\n",
    "for csv_file in csv_files:\n",
    "    # Get the file name without the extension to use as the key\n",
    "    file_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "    # Read the CSV file and assign it to the dictionary\n",
    "    dfs[file_name] = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "print(dfs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4943274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['df_data_test_results_lstm_final_ver2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_name=['data_lenght', 'train_heave_r2', 'train_pitch_r2', 'train_pendulum_r2',\n",
    "          'test_heave_r2', 'test_pitch_r2', 'test_pendulum_r2']\n",
    "\n",
    "new_name=['lenght', 'r2_train_heave', 'r2_train_pitch','r2_train_pendulum',\n",
    "          'r2_test_heave', 'r2_test_pitch', 'r2_test_pendulum']\n",
    "# Create a dictionary mapping old column names to new ones\n",
    "rename_dict = dict(zip(old_name, new_name))\n",
    "\n",
    "# Rename columns for a specific DataFrame\n",
    "dfs['df_data_test_results_lstm_final_ver2'].rename(columns=rename_dict, inplace=True)\n",
    "dfs['df_data_test_results_lstm_final_ver2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_name=['r2_val_heave', 'r2_val_pitch', 'r2_val_pendulum']\n",
    "\n",
    "new_name=['r2_test_heave', 'r2_test_pitch', 'r2_test_pendulum']\n",
    "# Create a dictionary mapping old column names to new ones\n",
    "rename_dict = dict(zip(old_name, new_name))\n",
    "\n",
    "# Rename columns for a specific DataFrame\n",
    "dfs['metrics_df_Xgboost_3dof_data_test_new_parralel_ver2'].rename(columns=rename_dict, inplace=True)\n",
    "dfs['metrics_df_Xgboost_3dof_data_test_series_ver2'].rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name, df in dfs.items():\n",
    "    print(file_name)\n",
    "    # Sort the DataFrame by the 'lenght' column\n",
    "    dfs[file_name] = df.sort_values(by='lenght', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ARX adjusted predictions\n",
    "arx_parralel = {\n",
    "    'heave': dfs['metrics_df_ARX_heave_data_test_parralel-'],\n",
    "    'pitch': dfs['metrics_df_ARX_picth_data_test_parralel-'],\n",
    "    'pendulum': dfs['metrics_df_ARX_pendulum_data_test_parallel-'],\n",
    "}\n",
    "\n",
    "arx_series = {\n",
    "    'heave': dfs['metrics_df_ARX_heave_data_test_series-'],\n",
    "    'pitch': dfs['metrics_df_ARX_picth_data_test_seiries-'],\n",
    "    'pendulum': dfs['metrics_df_ARX_pendulum_data_test_pseries-'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7465ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model info dictionary for customizing line styles\n",
    "model_info = {\n",
    "    \"ARX\": {\"color\": \"blue\", \"symbol\": \"x\", \"dash\": \"dash\"},\n",
    "    \"XGB\": {\"color\": \"red\", \"symbol\": \"circle\", \"dash\": \"dot\"},\n",
    "    \"LSTM\": {\"color\": \"green\", \"symbol\": \"square\", \"dash\": \"solid\"},\n",
    "}\n",
    "\n",
    "# List of degrees of freedom (DOFs)\n",
    "dofs = ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "# Loop through each DOF and create a plot for testing only\n",
    "for dof in dofs:\n",
    "    # Create a figure for a single plot (no subplots)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Testing R² - ARX\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=arx_series[dof]['lenght'],\n",
    "        y=arx_series[dof][f'r2_test_{dof}'],\n",
    "        name='ARX Test R²',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"ARX\"][\"color\"], dash=model_info[\"ARX\"][\"dash\"]),\n",
    "        marker=dict(symbol=model_info[\"ARX\"][\"symbol\"]),\n",
    "    ))\n",
    "\n",
    "    # Testing R² - XGBoost (NARX)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfs['metrics_df_Xgboost_3dof_data_test_series_ver2']['lenght'],\n",
    "        y=dfs['metrics_df_Xgboost_3dof_data_test_series_ver2'][f'r2_test_{dof}'],\n",
    "        name='XGBoost-NARX Test R²',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"XGB\"][\"color\"], dash=model_info[\"XGB\"][\"dash\"]),\n",
    "        marker=dict(symbol=model_info[\"XGB\"][\"symbol\"]),\n",
    "    ))\n",
    "\n",
    "    # Testing R² - LSTM\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfs['df_data_test_results_lstm_final_ver2']['lenght'],\n",
    "        y=dfs['df_data_test_results_lstm_final_ver2'][f'r2_test_{dof}'],\n",
    "        name='LSTM Test R²',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"LSTM\"][\"color\"], dash=model_info[\"LSTM\"][\"dash\"]),\n",
    "        marker=dict(symbol=model_info[\"LSTM\"][\"symbol\"]),\n",
    "    ))\n",
    "\n",
    "    # Set the y-axis range from -0.1 to 1.1\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            range=[-0.1, 1.1]  # Limit y-axis from -0.1 to 1.1\n",
    "        ),\n",
    "        width=850,\n",
    "        height=350,\n",
    "        title=f\"Performance Sensitivity to Training Data Size for {dof.capitalize()}\",  # Updated title based on DOF\n",
    "        xaxis_title=\"Training Data Length\",  # X-axis title\n",
    "        yaxis_title=\"Test R² \",  # Y-axis title\n",
    "        showlegend=True,  # Show the legend\n",
    "        template='plotly_white',  # White background template for better visuals\n",
    "    )\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"Results/data_sesitivity/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fig.write_image(f\"{output_dir}{dof}_data.png\")\n",
    "    # Show the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8e6fd",
   "metadata": {},
   "source": [
    "# Sensitivity to dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb71f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "\n",
    "# Set the path where your CSV files are located\n",
    "path = 'Results/dt_tests'\n",
    "\n",
    "# Use glob to find all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "# Dictionary to store each CSV as a separate DataFrame\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each file and load it into the dictionary\n",
    "for csv_file in csv_files:\n",
    "    # Get the file name without the extension to use as the key\n",
    "    file_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "    # Read the CSV file and assign it to the dictionary\n",
    "    dfs[file_name] = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "print(dfs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name, df in dfs.items():\n",
    "    print(file_name)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_name=['train_heave_r2', 'train_pitch_r2', 'train_pendulum_r2',\n",
    "          'test_heave_r2', 'test_pitch_r2', 'test_pendulum_r2']\n",
    "\n",
    "new_name=['r2_train_heave', 'r2_train_pitch','r2_train_pendulum',\n",
    "          'r2_test_heave', 'r2_test_pitch', 'r2_test_pendulum']\n",
    "# Create a dictionary mapping old column names to new ones\n",
    "rename_dict = dict(zip(old_name, new_name))\n",
    "\n",
    "dfs['df_dt_test_results_lstm'].rename(columns=rename_dict, inplace=True)\n",
    "dfs['df_dt_test_results_lstm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab720a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ARX adjusted predictions\n",
    "arx_parallel = {\n",
    "    'heave': dfs['metrics_df_ARX_heave_dt_senstivery_test_parralel-'],\n",
    "    'pitch': dfs['metrics_df_ARX_picth_dt_senstivery_test_parralel-'],\n",
    "    'pendulum': dfs['metrics_df_ARX_pendulum_dt_senstivery_test_parralel'],\n",
    "}\n",
    "\n",
    "arx_series = {\n",
    "    'heave': dfs['metrics_df_ARX_heave_dt_senstivery_test-'],\n",
    "    'pitch': dfs['metrics_df_ARX_picth_dt_senstivery_test-'],\n",
    "    'pendulum': dfs['metrics_df_ARX_pendulum_dt_senstivery_test-'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Update model_info with separate dash styles\n",
    "model_info = {\n",
    "    \"ARX\":  {\"color\": \"blue\",  \"symbol\": \"circle\", \"dash_series\": \"solid\",   \"dash_parallel\": \"dash\"},\n",
    "    \"XGB\":  {\"color\": \"red\",   \"symbol\": \"circle\", \"dash_series\": \"solid\",   \"dash_parallel\": \"dash\"},\n",
    "    \"LSTM\": {\"color\": \"green\", \"symbol\": \"square\",\"dash_series\": \"solid\",   \"dash_parallel\": None},  # no parallel\n",
    "}\n",
    "\n",
    "# common hover template\n",
    "hover_template = 'dt: %{x:.3f}s<br>R²: %{y:.3f}<extra></extra>'\n",
    "\n",
    "for dof in dofs:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # ARX Series\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=arx_series[dof]['dt'],\n",
    "        y=arx_series[dof][f'r2_test_{dof}'],\n",
    "        name='ARX (Series)',\n",
    "        legendgroup='ARX',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"ARX\"][\"color\"],\n",
    "                  dash=model_info[\"ARX\"][\"dash_series\"],\n",
    "                  width=2),\n",
    "        marker=dict(symbol=model_info[\"ARX\"][\"symbol\"], size=6),\n",
    "        hovertemplate=hover_template\n",
    "    ))\n",
    "\n",
    "    # ARX Parallel\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=arx_parallel[dof]['dt'],\n",
    "        y=arx_parallel[dof][f'r2_test_{dof}'],\n",
    "        name='ARX (Parallel)',\n",
    "        legendgroup='ARX',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"ARX\"][\"color\"],\n",
    "                  dash=model_info[\"ARX\"][\"dash_parallel\"],\n",
    "                  width=2),\n",
    "        marker=dict(symbol=\"x\", size=8),\n",
    "        hovertemplate=hover_template\n",
    "    ))\n",
    "\n",
    "    # XGBoost Series\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfs['metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new']['dt'],\n",
    "        y=dfs['metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new'][f'r2_test_{dof}'],\n",
    "        name='XGB (Series)',\n",
    "        legendgroup='XGB',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"XGB\"][\"color\"],\n",
    "                  dash=model_info[\"XGB\"][\"dash_series\"],\n",
    "                  width=2),\n",
    "        marker=dict(symbol=model_info[\"XGB\"][\"symbol\"], size=6),\n",
    "        hovertemplate=hover_template\n",
    "    ))\n",
    "\n",
    "    # XGBoost Parallel\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfs['metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel']['dt'],\n",
    "        y=dfs['metrics_df_XGBOOST_3dof_dt_test_nb0_nf10_new_parralel'][f'r2_test_{dof}'],\n",
    "        name='XGB (Parallel)',\n",
    "        legendgroup='XGB',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"XGB\"][\"color\"],\n",
    "                  dash=model_info[\"XGB\"][\"dash_parallel\"],\n",
    "                  width=2),\n",
    "        marker=dict(symbol=\"x\", size=6),\n",
    "        hovertemplate=hover_template\n",
    "    ))\n",
    "\n",
    "    # LSTM (only series)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfs['df_dt_test_results_lstm']['dt'],\n",
    "        y=dfs['df_dt_test_results_lstm'][f'r2_test_{dof}'],\n",
    "        name='LSTM',\n",
    "        legendgroup='LSTM',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color=model_info[\"LSTM\"][\"color\"],\n",
    "                  dash=model_info[\"LSTM\"][\"dash_series\"],\n",
    "                  width=2),\n",
    "        marker=dict(symbol=model_info[\"LSTM\"][\"symbol\"], size=6),\n",
    "        hovertemplate=hover_template\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(range=[-0.1, 1.1]),\n",
    "        width=850, height=350,\n",
    "        title=f\"Performance Sensitivity to Training Reselution for {dof.capitalize()}\",\n",
    "        xaxis_title=\"dt [s]\", yaxis_title=\"Test R²\",\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "     # Create the directory if it doesn't exist\n",
    "    output_dir = \"Results/dt_sesitivity/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fig.write_image(f\"{output_dir}{dof}_dt.png\")\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547dd40",
   "metadata": {},
   "source": [
    "# Enviromental Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "\n",
    "# Set the path where your CSV files are located\n",
    "path = 'Results/co2'\n",
    "\n",
    "# Use glob to find all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "# Dictionary to store each CSV as a separate DataFrame\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each file and load it into the dictionary\n",
    "for csv_file in csv_files:\n",
    "    # Get the file name without the extension to use as the key\n",
    "    file_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "    # Read the CSV file and assign it to the dictionary\n",
    "    dfs[file_name] = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "dfs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['arx_heave_parralel_pred'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efacafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the extracted emissions data\n",
    "emissions_data = []\n",
    "\n",
    "# Loop through each DataFrame in the dictionary\n",
    "for file_name, df in dfs.items():\n",
    "    # Extract the emission and emissions rate value from the last row\n",
    "    # units in files are  kg co2eq , kg CO2eq/s and KWh\n",
    "    emissions_value = df['emissions'].iloc[-1]  \n",
    "    emissions_rate=df['emissions_rate'] .iloc[-1]  \n",
    "    energy_used=df['energy_consumed'] .iloc[-1]  \n",
    "    # Append the file name and the emissions value to the list\n",
    "    emissions_data.append({'file_name': file_name, 'emissions': emissions_value ,'emissions_rate': emissions_rate , 'energy_consumed':energy_used})\n",
    "\n",
    "# Create a new DataFrame to store the extracted emissions\n",
    "emissions_df = pd.DataFrame(emissions_data)\n",
    "\n",
    "emissions_df['calculted_emisions']=emissions_df['energy_consumed']*146\n",
    "\n",
    "emissions_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
