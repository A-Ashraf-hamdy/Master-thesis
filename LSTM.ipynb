{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b8d2fa",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.colors as pc\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import random\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.trial import FixedTrial\n",
    "\n",
    "# Jupyter magic command \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36508088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Optionally enable memory growth (recommended if you experience GPU memory allocation issues)\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    " #   try:\n",
    "        # Enable memory growth for each GPU\n",
    " #       for gpu in gpus:\n",
    " #           tf.config.experimental.set_memory_growth(gpu, True)\n",
    " #   except RuntimeError as e:\n",
    " #       print(e)\n",
    "\n",
    "# Import Keras components including the LSTM layer \n",
    "# (in TF 2.x, LSTM is GPU-optimized if possible)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape , Dropout , Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint , Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "# Optionally, for explicit GPU-optimized LSTM (useful if you're using an older TF version):21\n",
    "# from tensorflow.keras.layers import CuDNNLSTM\n",
    "\n",
    "# For monitoring GPU availability (optional)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02393b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set environment variable to disable GPU (force using CPU)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Now, TensorFlow will use the CPU for computations, regardless of GPU availability.\n",
    "# Optionally, for monitoring GPU availability (optional)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set random seed for built-in random module\n",
    "random.seed(42)\n",
    "\n",
    "# 2. Set random seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# 3. Set random seed for TensorFlow\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfacba73",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b719ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_exog_sequences(\n",
    "    input_df: pd.DataFrame,\n",
    "    output_df: pd.DataFrame,\n",
    "    input_cols: list,\n",
    "    output_cols: list,\n",
    "    nb: int,\n",
    "    nf: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Build sliding windows of exogenous inputs + floater motion targets.\n",
    "    Returns only X and Y—no per-window init_vals, since we'll do a warm start.\n",
    "\n",
    "    Args:\n",
    "      input_df: DataFrame with your input features (e.g. wave elev), shape (N, ...)\n",
    "      output_df: DataFrame with your target features, shape (N, num_outputs)\n",
    "      input_cols: list of column names in input_df to use\n",
    "      output_cols: list of column names in output_df to predict\n",
    "      nb: number of past lags\n",
    "      nf: number of future steps\n",
    "\n",
    "    Returns:\n",
    "      X: np.ndarray, shape (samples, nb+nf, len(input_cols))\n",
    "      Y: np.ndarray, shape (samples, nf, len(output_cols))\n",
    "    \"\"\"\n",
    "    L= nb + 1 + nf                       # total window length    \n",
    "    Xs, Ys = [], []\n",
    "    X_arr = input_df[input_cols].to_numpy()\n",
    "    Y_arr = output_df[output_cols].to_numpy()\n",
    "    N = len(X_arr)\n",
    "\n",
    "    for i in range(nb, N - nf):\n",
    "        window = X_arr[i - nb : i + nf + 1]   # shape (L, features)\n",
    "        Xs.append(window)\n",
    "        Ys.append(Y_arr[i])                  # single row of shape (outputs,)\n",
    "\n",
    "    X = np.stack(Xs, axis=0)   # (num_samples, L, features)\n",
    "    Y = np.stack(Ys, axis=0)   # (num_samples, outputs)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afcae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps, future_steps):\n",
    "    if len(X) < time_steps + future_steps:\n",
    "        raise ValueError(\"Not enough data for the given time steps and future steps!\")\n",
    "\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps - future_steps + 1):  # Adjusted index range\n",
    "        Xs.append(X.iloc[i:i + time_steps].values)  # Use `.iloc` for Pandas indexing\n",
    "        ys.append(y.iloc[i + time_steps:i + time_steps + future_steps].values)  # Predict `future_steps`\n",
    "        \n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba1c8f",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train_full = pd.read_csv('prepared_data/train_data.csv')\n",
    "df_val_full = pd.read_csv('prepared_data/val_data.csv')\n",
    "df_test_full = pd.read_csv('prepared_data/test_data.csv')\n",
    "\n",
    "print(df_train_full.head())\n",
    "print(df_val_full.head())\n",
    "print(df_test_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test case\n",
    "case='Tp6p8s_Hs2m'\n",
    "df_case_train = df_train_full[df_train_full['test_name'] == case].copy()\n",
    "df_case_val = df_val_full[df_val_full['test_name'] == case].copy()\n",
    "df_case_test = df_test_full[df_test_full['test_name'] == case].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5855bd1",
   "metadata": {},
   "source": [
    "# Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Feature and target columns\n",
    "input_cols = ['eta']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "\n",
    "# Extract training data as DataFrames\n",
    "X_train = df_train_full[input_cols]\n",
    "y_train = df_train_full[output_cols]\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X.fit(X_train)\n",
    "scaler_y.fit(y_train)\n",
    "\n",
    "# Transform all sets (keeps DataFrame structure)\n",
    "X_train_scaled = pd.DataFrame(scaler_X.transform(X_train), columns=input_cols)\n",
    "y_train_scaled = pd.DataFrame(scaler_y.transform(y_train), columns=output_cols)\n",
    "\n",
    "X_val_scaled = pd.DataFrame(scaler_X.transform(df_val_full[input_cols]), columns=input_cols)\n",
    "y_val_scaled = pd.DataFrame(scaler_y.transform(df_val_full[output_cols]), columns=output_cols)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(scaler_X.transform(df_test_full[input_cols]), columns=input_cols)\n",
    "y_test_scaled = pd.DataFrame(scaler_y.transform(df_test_full[output_cols]), columns=output_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bae71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation functions using DataFrame input/output\n",
    "scaler_X_func = lambda df: pd.DataFrame(scaler_X.transform(df[input_cols]), columns=input_cols)\n",
    "scaler_y_func = lambda df: pd.DataFrame(scaler_y.transform(df[output_cols]), columns=output_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_X_vel = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "# Feature and target columns\n",
    "input_cols = ['eta','eta_velocity']\n",
    "\n",
    "\n",
    "# Extract training data as DataFrames\n",
    "X_train = df_train_full[input_cols]\n",
    "\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X_vel.fit(X_train)\n",
    "\n",
    "# Define the transformation functions using DataFrame input/output\n",
    "scaler_X_func_vel = lambda df: pd.DataFrame(scaler_X_vel.transform(df[input_cols]), columns=input_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc27a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_X_all = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "# Feature and target columns\n",
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "\n",
    "\n",
    "# Extract training data as DataFrames\n",
    "X_train = df_train_full[input_cols]\n",
    "\n",
    "\n",
    "# Fit scalers\n",
    "scaler_X_all.fit(X_train)\n",
    "\n",
    "# Define the transformation functions using DataFrame input/output\n",
    "scaler_X_func_all = lambda df: pd.DataFrame(scaler_X_all.transform(df[input_cols]), columns=input_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['heave']\n",
    "# Initialize scalers\n",
    "scaler_y_heave = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "scaler_y_heave.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "\n",
    "scaler_y_func_heave = lambda df: pd.DataFrame(scaler_y_heave.transform(df[output_cols]),columns=output_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pitch']\n",
    "# Initialize scalers\n",
    "\n",
    "scaler_y_pitch = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_pitch.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "\n",
    "scaler_y_func_pitch = lambda df: pd.DataFrame(scaler_y_pitch.transform(df[output_cols]),columns=output_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pendulum']\n",
    "# Initialize scalers\n",
    "\n",
    "scaler_y_pend = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_pend.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "\n",
    "scaler_y_func_pend = lambda df: pd.DataFrame(scaler_y_pend.transform(df[output_cols]),columns=output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e7593",
   "metadata": {},
   "source": [
    "# Building LSTM Model for the 3 dof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779e797",
   "metadata": {},
   "source": [
    "## $\\eta$ only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eef554",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d307ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_train_seq, y_train_seq = adjust_for_batch(X_train_seq, y_train_seq)\n",
    "X_val_seq, Y_val_seq = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set policy for mixed precision\n",
    "policy = mixed_precision.Policy('float32')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "strategy = tf.distribute.get_strategy()  # Use default strategy (single-device, CPU)\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a882ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to enable GPU memory growth\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Enable GPU (0) if you want to use GPU\n",
    "\n",
    "# Now set memory growth for the GPU before initializing any device\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Check if the GPU is being used\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the model will be saved\n",
    "checkpoint_path = \"model_checkpoint_eta_only_nb5_nf5.h5\"\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,              # File path to save the model\n",
    "    save_best_only=True,          # Save only the best model (based on the monitored metric)\n",
    "    monitor='val_loss',           # Monitor validation loss to track the best model\n",
    "    save_weights_only=False,       # Save only the model weights (you can change to False to save the whole model)\n",
    "    verbose=1                     # Display a message when the model is saved\n",
    ")\n",
    "class ResetStatesCallback(Callback): # resert LSTM states at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.reset_states()\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # The metric to monitor, you can also use 'val_accuracy' or any other metric\n",
    "        patience=10,               # Number of epochs with no improvement before stopping\n",
    "        verbose=1,                 # Show a message when stopping\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Model Parameters ---\n",
    "\n",
    "Lstm_1= 64\n",
    "\n",
    "LSTM_2= 64\n",
    "dropout = 0.2\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  # e.g. 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "# --- Model input ---\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Shared LSTM Layers ---\n",
    "x = LSTM(Lstm_1, stateful=True, return_sequences=True, name='shared_lstm_1')(seq_in)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "x = LSTM(LSTM_2, stateful=True, return_sequences=False, name='shared_lstm_2')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "# --- 2. Separate Dense Output Layers ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(x)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(x)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(x)\n",
    "\n",
    "# --- 3. Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7922ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping,checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbaa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_only_nb5_nf5.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfda525",
   "metadata": {},
   "source": [
    "## $\\eta$ and $\\dot{\\eta}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta','eta_velocity']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a08a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_vel(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_vel(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_train_seq, y_train_seq = adjust_for_batch(X_train_seq, y_train_seq)\n",
    "X_val_seq, Y_val_seq = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef578978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the model will be saved\n",
    "checkpoint_path = \"model_checkpoint_eta_Vel_nb5_nf5.h5\"\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,              # File path to save the model\n",
    "    save_best_only=True,          # Save only the best model (based on the monitored metric)\n",
    "    monitor='val_loss',           # Monitor validation loss to track the best model\n",
    "    save_weights_only=False,       # Save only the model weights (you can change to False to save the whole model)\n",
    "    verbose=1                     # Display a message when the model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee52d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Model Parameters ---\n",
    "\n",
    "Lstm_1= 64\n",
    "\n",
    "LSTM_2= 64\n",
    "dropout = 0.2\n",
    "\n",
    "#learning_rate = 1e-4\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  # e.g. 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "# --- Model input ---\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Shared LSTM Layers ---\n",
    "x = LSTM(Lstm_1, stateful=True, return_sequences=True, name='shared_lstm_1')(seq_in)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "x = LSTM(LSTM_2, stateful=True, return_sequences=False, name='shared_lstm_2')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "# --- 2. Separate Dense Output Layers ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(x)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(x)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(x)\n",
    "\n",
    "# --- 3. Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b62769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86418154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping,checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_Vel_nb5_nf5.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fbf8a6",
   "metadata": {},
   "source": [
    "## # $\\eta$, $\\dot{\\eta}$, and $\\ddot{\\eta}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_all(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_train_seq, y_train_seq = adjust_for_batch(X_train_seq, y_train_seq)\n",
    "X_val_seq, Y_val_seq = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the model will be saved\n",
    "checkpoint_path = \"model_checkpoint_eta_Vel_acc_nb5_nf5.h5\"\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,              # File path to save the model\n",
    "    save_best_only=True,          # Save only the best model (based on the monitored metric)\n",
    "    monitor='val_loss',           # Monitor validation loss to track the best model\n",
    "    save_weights_only=False,       # Save only the model weights (you can change to False to save the whole model)\n",
    "    verbose=1                     # Display a message when the model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Model Parameters ---\n",
    "\n",
    "Lstm_1= 64\n",
    "\n",
    "LSTM_2= 64\n",
    "dropout = 0.2\n",
    "\n",
    "#learning_rate = 1e-4\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  # e.g. 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "# --- Model input ---\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Shared LSTM Layers ---\n",
    "x = LSTM(Lstm_1, stateful=True, return_sequences=True, name='shared_lstm_1')(seq_in)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "x = LSTM(LSTM_2, stateful=True, return_sequences=False, name='shared_lstm_2')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "# --- 2. Separate Dense Output Layers ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(x)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(x)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(x)\n",
    "\n",
    "# --- 3. Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping,checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_Vel_acc_nb5_nf5.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99822352",
   "metadata": {},
   "source": [
    "# Building a new model with 3 parralel layer followed by a shared layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdefbef",
   "metadata": {},
   "source": [
    "## $\\eta$ and $\\dot{\\eta}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta','eta_velocity']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_vel(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_vel(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_train_seq, y_train_seq = adjust_for_batch(X_train_seq, y_train_seq)\n",
    "X_val_seq, Y_val_seq = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path where the model will be saved\n",
    "checkpoint_path_3 = \"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel.h5\"\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback_3 = ModelCheckpoint(\n",
    "    checkpoint_path_3,              # File path to save the model\n",
    "    save_best_only=True,          # Save only the best model (based on the monitored metric)\n",
    "    monitor='val_loss',           # Monitor validation loss to track the best model\n",
    "    save_weights_only=False,       # Save only the model weights (you can change to False to save the whole model)\n",
    "    verbose=1                     # Display a message when the model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Parameters ---\n",
    "\n",
    "heave_LSTM = 96\n",
    "pitch_LSTM = 128\n",
    "pend_LSTM = 64\n",
    "\n",
    "shared_LSTM = 96\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Per-output LSTM layers ---\n",
    "heave_branch = LSTM(heave_LSTM,stateful=True, return_sequences=True,  batch_size=batch_size, name='heave_lstm')(seq_in)\n",
    "heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "pitch_branch = LSTM(pitch_LSTM,stateful=True, return_sequences=True,batch_size=batch_size,  name='pitch_lstm')(seq_in)\n",
    "pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "    \n",
    "pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=True, batch_size=batch_size, name='pendulum_lstm')(seq_in)\n",
    "pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "# --- 2. Concatenate outputs ---\n",
    "combined = Concatenate(name='combined_lstm_concat')([heave_branch, pitch_branch, pend_branch])\n",
    "\n",
    "# --- 3. Shared LSTM layer to learn coupling ---\n",
    "shared_lstm = LSTM(shared_LSTM, stateful=True,return_sequences=False,batch_size=batch_size,  name='shared_lstm')(combined)\n",
    "shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "# --- 4. Final Dense outputs ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(shared_lstm)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(shared_lstm)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(shared_lstm)\n",
    "\n",
    "# --- Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping,checkpoint_callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_vel_nb5_nf5_new_archt_new.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fe9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5dbf3f",
   "metadata": {},
   "source": [
    "## $\\eta$, $\\dot{\\eta}$ , and $\\ddot{\\eta}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_all(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_train_seq, y_train_seq = adjust_for_batch(X_train_seq, y_train_seq)\n",
    "X_val_seq, Y_val_seq = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1717da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path where the model will be saved\n",
    "checkpoint_path_3 = \"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc.h5\"\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback_3 = ModelCheckpoint(\n",
    "    checkpoint_path_3,              # File path to save the model\n",
    "    save_best_only=True,          # Save only the best model (based on the monitored metric)\n",
    "    monitor='val_loss',           # Monitor validation loss to track the best model\n",
    "    save_weights_only=False,       # Save only the model weights (you can change to False to save the whole model)\n",
    "    verbose=1                     # Display a message when the model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Parameters ---\n",
    "\n",
    "heave_LSTM = 64\n",
    "pitch_LSTM = 64\n",
    "pend_LSTM = 64\n",
    "\n",
    "shared_LSTM = 64\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model \n",
    "\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Per-output LSTM layers ---\n",
    "heave_branch = LSTM(heave_LSTM,stateful=True, return_sequences=True,  batch_size=batch_size, name='heave_lstm')(seq_in)\n",
    "heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "pitch_branch = LSTM(pitch_LSTM,stateful=True, return_sequences=True,batch_size=batch_size,  name='pitch_lstm')(seq_in)\n",
    "pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "    \n",
    "pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=True, batch_size=batch_size, name='pendulum_lstm')(seq_in)\n",
    "pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "# --- 2. Concatenate outputs ---\n",
    "combined = Concatenate(name='combined_lstm_concat')([heave_branch, pitch_branch, pend_branch])\n",
    "\n",
    "# --- 3. Shared LSTM layer to learn coupling ---\n",
    "shared_lstm = LSTM(shared_LSTM, stateful=True,return_sequences=False,batch_size=batch_size,  name='shared_lstm')(combined)\n",
    "shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "# --- 4. Final Dense outputs ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(shared_lstm)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(shared_lstm)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(shared_lstm)\n",
    "\n",
    "# --- Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ba6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f950f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping,checkpoint_callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4523ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_vel_acc_nb5_nf5_new_archt_new.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc12477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61731d01",
   "metadata": {},
   "source": [
    "### Adjusting stopping creteria only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04493b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path where the model will be saved\n",
    "checkpoint_path_3 = \"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc_more_stopping_creteria.h5\"\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback_3 = ModelCheckpoint(\n",
    "    checkpoint_path_3,              # File path to save the model\n",
    "    save_best_only=True,          # Save only the best model (based on the monitored metric)\n",
    "    monitor='val_loss',           # Monitor validation loss to track the best model\n",
    "    save_weights_only=False,       # Save only the model weights (you can change to False to save the whole model)\n",
    "    verbose=1                     # Display a message when the model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping_more = EarlyStopping(\n",
    "        monitor='val_loss',        # The metric to monitor, you can also use 'val_accuracy' or any other metric\n",
    "        patience=15,               # Number of epochs with no improvement before stopping\n",
    "        verbose=1,                 # Show a message when stopping\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=200,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping_more,checkpoint_callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_vel_acc_nb5_nf5_new_archt_new_mor_stopping.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f06684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc_more_stopping_creteria.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df_true_scale.to_csv('LSTM_train_true.csv')\n",
    "df_train_pred_true_scale.to_csv('LSTM_train_pred.csv')\n",
    "\n",
    "y_val_df_true_scale.to_csv('LSTM_val_true.csv')\n",
    "df_val_pred_true_scale.to_csv('LSTM_val_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from two runs into one dictionary\n",
    "# Load history dictionary from JSON file\n",
    "with open('history_dict_new_1nd_run_eta_vel_acc_nb5_nf5_new_archt_new.json', 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "with open('history_dict_new_1nd_run_eta_vel_acc_nb5_nf5_new_archt_new_mor_stopping.json', 'r') as f:\n",
    "    history_dict_1 = json.load(f)\n",
    "                               # Merge histories \n",
    "history_combined = {}\n",
    "\n",
    "# List of history dicts\n",
    "history_list = [history_dict,history_dict_1]\n",
    "\n",
    "# Keys to merge (assuming all histories share the same keys)\n",
    "keys = history_dict.keys()\n",
    "\n",
    "# Combine each key manually\n",
    "for key in keys:\n",
    "    history_combined[key] = history_dict.get(key, []) + \\\n",
    "                            history_dict_1.get(key, []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heave Loss Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history_combined['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_combined['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"heave_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Pitch Loss Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history_combined['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_combined['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pitch_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Pendulum Loss Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history_combined['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_combined['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pendulum_loss.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee88cc2",
   "metadata": {},
   "source": [
    "### Adding more units for pitch layer and adjusting stopping creteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389361a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Parameters ---\n",
    "\n",
    "heave_LSTM = 64\n",
    "pitch_LSTM = 96\n",
    "pend_LSTM = 64\n",
    "\n",
    "shared_LSTM = 64\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  # e.g. 1\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d45ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input layer\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Shared LSTM layer to extract shared temporal features ---\n",
    "shared_lstm = LSTM(shared_LSTM, stateful=True, return_sequences=True, name='shared_lstm')(seq_in)\n",
    "shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "# --- 2. Per-output LSTM branches (individual DoF refinement) ---\n",
    "heave_branch = LSTM(heave_LSTM, stateful=True, return_sequences=False, name='heave_lstm')(shared_lstm)\n",
    "heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "pitch_branch = LSTM(pitch_LSTM, stateful=True, return_sequences=False, name='pitch_lstm')(shared_lstm)\n",
    "pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "\n",
    "pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=False, name='pendulum_lstm')(shared_lstm)\n",
    "pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "# --- 3. Final Dense outputs ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(heave_branch)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(pitch_branch)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(pend_branch)\n",
    "\n",
    "# --- 4. Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=200,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping_more,checkpoint_callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "history_dict = history.history\n",
    "# Heave Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history_dict['heave_loss'], label='Training Heave Loss')\n",
    "plt.plot(history_dict['val_heave_loss'], label='Validation Heave Loss')\n",
    "plt.title('Heave Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [m²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pitch Loss\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history_dict['pitch_loss'], label='Training Pitch Loss')\n",
    "plt.plot(history_dict['val_pitch_loss'], label='Validation Pitch Loss')\n",
    "plt.title('Pitch Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "# Pendulum Loss\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history_dict['pendulum_loss'], label='Training Pendulum Loss')\n",
    "plt.plot(history_dict['val_pendulum_loss'], label='Validation Pendulum Loss')\n",
    "plt.title('Pendulum Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE [°²]')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22116814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save dictionary to a .json file\n",
    "with open('history_dict_new_1nd_run_eta_vel_acc_nb5_nf5_new_archt_new_more_units.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Reset & warm-start as before\n",
    "# This resets the states of the model, which is particularly important for stateful LSTMs.\n",
    "model.reset_states()\n",
    "\n",
    "# 2) Evaluate the model on the full validation set\n",
    "test_results = model.evaluate(\n",
    "    X_val_seq,  # The validation data\n",
    "    {\n",
    "        'heave': Y_val_seq[:, 0],   # True values for 'heave'\n",
    "        'pitch': Y_val_seq[:, 1],   # True values for 'pitch'\n",
    "        'pendulum': Y_val_seq[:, 2]  # True values for 'pendulum'\n",
    "    },\n",
    "    batch_size=batch_size,  # The batch size to use during evaluation\n",
    "    verbose=1  # Verbosity level of evaluation\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc_more_units.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Reset states before prediction ----\n",
    "model.reset_states()\n",
    "\n",
    "# ---- 2) Predict on training data ----\n",
    "y_train_preds = model.predict(X_train_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 3) Predict on validation data ----\n",
    "model.reset_states()\n",
    "y_val_preds = model.predict(X_val_seq, batch_size=batch_size, verbose=1)\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "\n",
    "# ---- 4) Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "df_train_pred = pd.DataFrame(y_train_pred_arr, columns=pred_cols)\n",
    "df_val_pred = pd.DataFrame(y_val_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "train_pred_true_scale = scaler_y.inverse_transform(df_train_pred)\n",
    "val_pred_true_scale = scaler_y.inverse_transform(df_val_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "df_train_pred_true_scale = pd.DataFrame(train_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "df_val_pred_true_scale = pd.DataFrame(val_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_train_true_scale = scaler_y.inverse_transform(y_train_seq)\n",
    "y_val_true_scale = scaler_y.inverse_transform(Y_val_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=output_cols)\n",
    "y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# R² Scores\n",
    "r2_scores_train = {\n",
    "    col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "r2_scores_val = {\n",
    "    col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse_train = {\n",
    "    col: np.sqrt(mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "rmse_val = {\n",
    "    col: np.sqrt(mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"Training R² Scores:\", r2_scores_train)\n",
    "print(\"Validation R² Scores:\", r2_scores_val)\n",
    "print(\"Training RMSE:\", rmse_train)\n",
    "print(\"Validation RMSE:\", rmse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198459d",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe236bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Best model\n",
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc_more_stopping_creteria.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta' , 'eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "cases=df_test_full['test_name'].unique()\n",
    "cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect the results for each test case\n",
    "results = []\n",
    "for case in cases:\n",
    "    # Filter the DataFrame for the current test case\n",
    "    df_test_case = df_test_full[df_test_full['test_name'] == case].copy()\n",
    "\n",
    "    test_input = df_test_case[input_cols]\n",
    "    test_output= df_test_case[output_cols]\n",
    "\n",
    "    # scale the data\n",
    "    input_df_scaled = scaler_X_func_all(test_input)\n",
    "    output_df_scaled = scaler_y_func(test_output)\n",
    "\n",
    "    X_test_seq,y_test_seq= create_exog_sequences(\n",
    "        input_df = input_df_scaled,\n",
    "        output_df=output_df_scaled,\n",
    "        input_cols= input_cols,\n",
    "        output_cols = output_cols,\n",
    "        nb = nb,\n",
    "        nf= nf\n",
    "    )\n",
    "\n",
    "\n",
    "    # Remove extra samples at the end to match batch size\n",
    "    def adjust_for_batch(X, y):\n",
    "        n = (X.shape[0] // batch_size) * batch_size\n",
    "        return X[:n], y[:n]\n",
    "\n",
    "    X_test_seq, y_test_seq = adjust_for_batch(X_test_seq, y_test_seq)\n",
    "\n",
    "    # Get the raw predictions for the test set\n",
    "    model.reset_states()\n",
    "\n",
    "    y_test_preds = model.predict(\n",
    "        X_test_seq,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        \n",
    "    )\n",
    "\n",
    "    heave_pred_test, pitch_pred_test, pend_pred_test = [arr.squeeze(-1) for arr in y_test_preds]\n",
    "\n",
    "    # combine into a single array or DataFrame\n",
    "\n",
    "    y_test_pred_arr = np.vstack([heave_pred_test, pitch_pred_test, pend_pred_test]).T\n",
    "    df_test_pred = pd.DataFrame(\n",
    "        y_test_pred_arr,\n",
    "        columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    # get true data \n",
    "\n",
    "    y_test_df=pd.DataFrame(y_test_seq, columns=['heave', 'pitch', 'pendulum'])\n",
    "\n",
    "    # scale back tp oridinal scale\n",
    "\n",
    "    y_test_predict_true_scale= scaler_y.inverse_transform(df_test_pred)\n",
    "    y_test_true_scale= scaler_y.inverse_transform(y_test_df)\n",
    "\n",
    "    # Convert NumPy arrays back to DataFrames with column names\n",
    "\n",
    "    df_test_pred_true_scale = pd.DataFrame(\n",
    "        y_test_predict_true_scale, \n",
    "        columns=[f\"{col}_pred\" for col in output_cols]  \n",
    "    )\n",
    "\n",
    "    y_test_df_true_scale = pd.DataFrame(y_test_true_scale, columns= output_cols)\n",
    "\n",
    "    # R2 score \n",
    "    r2_scores_test = {col: r2_score(y_test_df_true_scale[col], df_test_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    # MSE on training data\n",
    "    mse_test = {col: mean_squared_error(y_test_df_true_scale[col], df_test_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"test_case :\", case)\n",
    "    print(\"Test R² Scores:\", r2_scores_test)\n",
    "    print(\"Test MSE:\", mse_test)\n",
    "    \n",
    "    # Collect the results in a dictionary (one dictionary per case)\n",
    "    result = {\n",
    "        'test_case': case,\n",
    "        'heave_r2': r2_scores_test.get('heave', None),\n",
    "        'pitch_r2': r2_scores_test.get('pitch', None),\n",
    "        'pendulum_r2': r2_scores_test.get('pendulum', None),\n",
    "        'heave_mse': mse_test.get('heave', None),\n",
    "        'pitch_mse': mse_test.get('pitch', None),\n",
    "        'pendulum_mse': mse_test.get('pendulum', None),\n",
    "    }\n",
    "    \n",
    "    # Append the result dictionary to the results list\n",
    "    results.append(result)\n",
    "    \n",
    "    # save predictions and true values to CSV files\n",
    "    y_test_df_true_scale.to_csv(f'Results/LSTM/Testing/LSTM_true_{case}.csv', index=False)\n",
    "    df_test_pred_true_scale.to_csv(f'Results/LSTM/Testing/LSTM_pred_{case}.csv', index=False)\n",
    "    \n",
    "# create a DataFrame from the results list\n",
    "df_test_results_initial_not_zero = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7885ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to a CSV file\n",
    "df_test_results_initial_not_zero.to_csv(f'Results/LSTM/Testing/LSTMdf_test_results_initial_not_zero.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4f328",
   "metadata": {},
   "source": [
    "# Compuatational Time and Co2 emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the codecarbon library and print its version\n",
    "import codecarbon\n",
    "print(codecarbon.__version__)\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e7dd8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the Training and Validation data for the model\n",
    "\n",
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "# Create 4 rows of zeros for each DataFrame\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_all(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_train_seq, y_train_seq = adjust_for_batch(X_train_seq, y_train_seq)\n",
    "X_val_seq, Y_val_seq = adjust_for_batch(X_val_seq, Y_val_seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15eeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Parameters ---\n",
    "\n",
    "heave_LSTM = 64\n",
    "pitch_LSTM = 64\n",
    "pend_LSTM = 64\n",
    "\n",
    "shared_LSTM = 64\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# build model \n",
    "\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Per-output LSTM layers ---\n",
    "heave_branch = LSTM(heave_LSTM,stateful=True, return_sequences=True,  batch_size=batch_size, name='heave_lstm')(seq_in)\n",
    "heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "pitch_branch = LSTM(pitch_LSTM,stateful=True, return_sequences=True,batch_size=batch_size,  name='pitch_lstm')(seq_in)\n",
    "pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "    \n",
    "pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=True, batch_size=batch_size, name='pendulum_lstm')(seq_in)\n",
    "pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "# --- 2. Concatenate outputs ---\n",
    "combined = Concatenate(name='combined_lstm_concat')([heave_branch, pitch_branch, pend_branch])\n",
    "\n",
    "# --- 3. Shared LSTM layer to learn coupling ---\n",
    "shared_lstm = LSTM(shared_LSTM, stateful=True,return_sequences=False,batch_size=batch_size,  name='shared_lstm')(combined)\n",
    "shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "# --- 4. Final Dense outputs ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(shared_lstm)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(shared_lstm)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(shared_lstm)\n",
    "\n",
    "# --- Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback()]\n",
    ")\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370fec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"lstm_train.csv\"    # Custom filename\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "# build model \n",
    "\n",
    "seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "# --- 1. Per-output LSTM layers ---\n",
    "heave_branch = LSTM(heave_LSTM,stateful=True, return_sequences=True,  batch_size=batch_size, name='heave_lstm')(seq_in)\n",
    "heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "pitch_branch = LSTM(pitch_LSTM,stateful=True, return_sequences=True,batch_size=batch_size,  name='pitch_lstm')(seq_in)\n",
    "pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "    \n",
    "pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=True, batch_size=batch_size, name='pendulum_lstm')(seq_in)\n",
    "pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "# --- 2. Concatenate outputs ---\n",
    "combined = Concatenate(name='combined_lstm_concat')([heave_branch, pitch_branch, pend_branch])\n",
    "\n",
    "# --- 3. Shared LSTM layer to learn coupling ---\n",
    "shared_lstm = LSTM(shared_LSTM, stateful=True,return_sequences=False,batch_size=batch_size,  name='shared_lstm')(combined)\n",
    "shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "# --- 4. Final Dense outputs ---\n",
    "heave_out = Dense(1, activation='tanh', name='heave')(shared_lstm)\n",
    "pitch_out = Dense(1, activation='tanh', name='pitch')(shared_lstm)\n",
    "pendulum_out = Dense(1, activation='tanh', name='pendulum')(shared_lstm)\n",
    "\n",
    "# --- Define the model ---\n",
    "model = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "# complie model \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    {\n",
    "        'heave': y_train_seq[:, 0],\n",
    "        'pitch': y_train_seq[:, 1],\n",
    "        'pendulum': y_train_seq[:, 2]\n",
    "    },\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq,\n",
    "        {\n",
    "            'heave': Y_val_seq[:, 0],\n",
    "            'pitch': Y_val_seq[:, 1],\n",
    "            'pendulum': Y_val_seq[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback()]\n",
    ")\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b5295",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342af560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Best model\n",
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc_more_stopping_creteria.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_test='Tp6p8s_Hs1m'\n",
    "df_case_test=df_train_full[df_train_full['test_name']==case_test].reset_index(drop=True)\n",
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "# Create 4 rows of zeros for each DataFrame\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_test[input_cols]\n",
    "output= df_case_test[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_test_seq,y_test_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_test_seq, y_test_seq = adjust_for_batch(X_test_seq, y_test_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# predictoins \n",
    "model.reset_states()\n",
    "\n",
    "y_test_preds = model.predict(\n",
    "        X_test_seq,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "            )\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d35072",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"lstm_pred.csv\"    # Custom filename\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "# predictoins \n",
    "model.reset_states()\n",
    "\n",
    "y_test_preds = model.predict(\n",
    "        X_test_seq,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "            )\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heave_pred_test, pitch_pred_test, pend_pred_test = [arr.squeeze(-1) for arr in y_test_preds]\n",
    "\n",
    "# combine into a single array or DataFrame\n",
    "\n",
    "y_test_pred_arr = np.vstack([heave_pred_test, pitch_pred_test, pend_pred_test]).T\n",
    "df_test_pred = pd.DataFrame(\n",
    "    y_test_pred_arr,\n",
    "    columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    ")\n",
    " # get true data \n",
    "\n",
    "y_test_df=pd.DataFrame(y_test_seq, columns=['heave', 'pitch', 'pendulum'])\n",
    "\n",
    "# scale back tp oridinal scale\n",
    "\n",
    "y_test_predict_true_scale= scaler_y.inverse_transform(df_test_pred)\n",
    "y_test_true_scale= scaler_y.inverse_transform(y_test_df)\n",
    "\n",
    "# Convert NumPy arrays back to DataFrames with column names\n",
    "\n",
    "df_test_pred_true_scale = pd.DataFrame(\n",
    "    y_test_predict_true_scale, \n",
    "    columns=[f\"{col}_pred\" for col in output_cols]  \n",
    ")\n",
    "\n",
    "y_test_df_true_scale = pd.DataFrame(y_test_true_scale, columns= output_cols)\n",
    "\n",
    "df_test_pred_true_scale.to_csv('Results/testing for comparison/lstm_pred.csv')\n",
    "y_test_df_true_scale.to_csv('Results/testing for comparison/lstm_true.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37070e4f",
   "metadata": {},
   "source": [
    "# Data senstivety Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Parameters ---\n",
    "\n",
    "heave_LSTM = 64\n",
    "pitch_LSTM = 64\n",
    "pend_LSTM = 64\n",
    "\n",
    "shared_LSTM = 64\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd14ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the Training Data\n",
    "# Create 4 rows of zeros for each DataFrame\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32625f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght=np.array([0.002,0.004,0.008,0.01 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9,1])*X_train_seq.shape[0]\n",
    "lenght=lenght.astype(int)\n",
    "lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_all(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_val_seq_full, Y_val_seq_full = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght_val=np.array([0.005,0.005,0.008,0.01 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9,1])*X_val_seq.shape[0]\n",
    "lenght_val=lenght_val.astype(int)\n",
    "lenght_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "test_input = df_case_test[input_cols]\n",
    "test_output= df_case_test[output_cols]\n",
    "# scale the data\n",
    "test_input_df_scaled = scaler_X_func_all(test_input)\n",
    "test_output_df_scaled = scaler_y_func(test_output)\n",
    "\n",
    "X_test_seq , Y_test_seq = create_exog_sequences(\n",
    "    input_df = test_input_df_scaled,\n",
    "    output_df= test_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_test_seq, Y_test_seq = adjust_for_batch(X_test_seq, Y_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect the results for each test case\n",
    "results = []\n",
    "for l in [0,1,2,3]:\n",
    "    \n",
    "    # build model \n",
    "\n",
    "    seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "    # --- 1. Per-output LSTM layers ---\n",
    "    heave_branch = LSTM(heave_LSTM,stateful=True, return_sequences=True,  batch_size=batch_size, name='heave_lstm')(seq_in)\n",
    "    heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "    pitch_branch = LSTM(pitch_LSTM,stateful=True, return_sequences=True,batch_size=batch_size,  name='pitch_lstm')(seq_in)\n",
    "    pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "        \n",
    "    pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=True, batch_size=batch_size, name='pendulum_lstm')(seq_in)\n",
    "    pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "    # --- 2. Concatenate outputs ---\n",
    "    combined = Concatenate(name='combined_lstm_concat')([heave_branch, pitch_branch, pend_branch])\n",
    "\n",
    "    # --- 3. Shared LSTM layer to learn coupling ---\n",
    "    shared_lstm = LSTM(shared_LSTM, stateful=True,return_sequences=False,batch_size=batch_size,  name='shared_lstm')(combined)\n",
    "    shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "    # --- 4. Final Dense outputs ---\n",
    "    heave_out = Dense(1, activation='tanh', name='heave')(shared_lstm)\n",
    "    pitch_out = Dense(1, activation='tanh', name='pitch')(shared_lstm)\n",
    "    pendulum_out = Dense(1, activation='tanh', name='pendulum')(shared_lstm)\n",
    "\n",
    "    # --- Define the model ---\n",
    "    model_data_test = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "\n",
    "    \n",
    "    X_train_seq_adjusted=X_train_seq[0:lenght[l],:,:]\n",
    "    y_train_seq_adjusted=y_train_seq[0:lenght[l],:]\n",
    "    \n",
    "    X_val_seq_adjusted=X_val_seq[0:lenght_val[l],:,:]\n",
    "    Y_val_seq_adjusted=Y_val_seq[0:lenght_val[l],:]\n",
    "    \n",
    "    \n",
    "    X_train_seq_adjusted, y_train_seq_adjusted = adjust_for_batch(X_train_seq_adjusted, y_train_seq_adjusted)\n",
    "    X_val_seq_adjusted, Y_val_seq_adjusted = adjust_for_batch(X_val_seq_adjusted, Y_val_seq_adjusted)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # compile the model\n",
    "    model_data_test.compile (optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n",
    "\n",
    "    \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "   \n",
    "    print(\"NaNs in X_train_seq_adjusted:\", np.any(np.isnan(X_train_seq_adjusted)))\n",
    "    print(\"NaNs in y_train_seq_adjusted:\", np.any(np.isnan(y_train_seq_adjusted)))\n",
    "    print(\"NaNs in X_val_seq_adjusted:\", np.any(np.isnan(X_val_seq_adjusted)))\n",
    "    print(\"NaNs in Y_val_seq_adjusted:\", np.any(np.isnan(Y_val_seq_adjusted)))\n",
    "   \n",
    "    \n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    # Train model\n",
    "    history = model_data_test.fit(\n",
    "    X_train_seq_adjusted,\n",
    "    {\n",
    "        'heave': y_train_seq_adjusted[:, 0],\n",
    "        'pitch': y_train_seq_adjusted[:, 1],\n",
    "        'pendulum': y_train_seq_adjusted[:, 2]\n",
    "    },\n",
    "    epochs=500,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq_adjusted,\n",
    "        {\n",
    "            'heave': Y_val_seq_adjusted[:, 0],\n",
    "            'pitch': Y_val_seq_adjusted[:, 1],\n",
    "            'pendulum': Y_val_seq_adjusted[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping_more]\n",
    "        )   \n",
    "    \n",
    "    \n",
    "        \n",
    "    # Get the raw predictions for the training set\n",
    "    model_data_test.reset_states()\n",
    "    print(f'-----predicting on training data----')\n",
    "    \n",
    "    y_train_preds = model_data_test.predict(\n",
    "        X_train_seq_adjusted,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # 3) y_train_preds is a list of three arrays (one per output head)\n",
    "\n",
    "    heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "\n",
    "    # 4) (Optional) combine into a single array or DataFrame\n",
    "\n",
    "\n",
    "    y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "    df_train_pred = pd.DataFrame(\n",
    "        y_train_pred_arr,\n",
    "        columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    print(df_train_pred.head())\n",
    "\n",
    "    # 1) Reset & warm‑start again, just like for evaluation\n",
    "    model_data_test.reset_states()\n",
    "\n",
    "    print(f'-----predicting on testing data----')\n",
    "\n",
    "    # 2) Get the raw predictions for the testing set\n",
    "    y_test_preds = model_data_test.predict(\n",
    "    X_test_seq,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "\n",
    "    )\n",
    "\n",
    "    # 3) y_val_preds is a list of three arrays (one per output head)\n",
    "    heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_test_preds]\n",
    "\n",
    "    # 4) (Optional) combine into a single array or DataFrame\n",
    "    y_test_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "    df_y_test_pred = pd.DataFrame(\n",
    "    y_test_pred_arr,\n",
    "    columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    print(df_y_test_pred.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1) Reset & warm‑start again, just like for evaluation\n",
    "    model_data_test.reset_states()\n",
    "\n",
    "    print(f'-----predicting on validation data----')\n",
    "\n",
    "    # 2) Get the raw predictions for the validation set\n",
    "    y_val_preds = model_data_test.predict(\n",
    "    X_val_seq_adjusted,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "\n",
    "    )\n",
    "\n",
    "    # 3) y_val_preds is a list of three arrays (one per output head)\n",
    "    heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "\n",
    "    # 4) (Optional) combine into a single array or DataFrame\n",
    "    y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "    df_val_pred = pd.DataFrame(\n",
    "    y_val_pred_arr,\n",
    "    columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    print(df_val_pred.head())\n",
    "    \n",
    "               \n",
    "    \n",
    "    # scale back the predictions to original scale\n",
    "    train_pred_true_scale= scaler_y.inverse_transform(df_train_pred)\n",
    "    test_pred_true_scale= scaler_y.inverse_transform(df_y_test_pred)\n",
    "    val_pred_true_scale= scaler_y.inverse_transform(df_val_pred)\n",
    "    output_cols = ['heave', 'pitch', 'pendulum']  # replace with your actual target column names\n",
    "\n",
    "    # Convert NumPy arrays back to DataFrames with column names\n",
    "    df_train_pred_true_scale = pd.DataFrame(\n",
    "    train_pred_true_scale, \n",
    "    columns=[f\"{col}_pred\" for col in output_cols]  # creates columns: 'heave_pred', 'pitch_pred', etc.\n",
    "    )\n",
    "\n",
    "    df_test_pred_true_scale = pd.DataFrame(\n",
    "    test_pred_true_scale,\n",
    "    columns=[f\"{col}_pred\" for col in output_cols])\n",
    "    \n",
    "    df_val_pred_true_scale = pd.DataFrame(\n",
    "    val_pred_true_scale,\n",
    "    columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "    # get df with the traing and validation data at corrensponding time steps\n",
    "    y_train_df=pd.DataFrame(y_train_seq_adjusted, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_val_df=pd.DataFrame(Y_val_seq_adjusted, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_test_df=pd.DataFrame(Y_test_seq, columns=['heave', 'pitch', 'pendulum'])\n",
    "\n",
    "    # scale back tp oridinal scale\n",
    "    y_train_true_scale= scaler_y.inverse_transform(y_train_df)\n",
    "    y_test_true_scale= scaler_y.inverse_transform(y_test_df)\n",
    "    y_val_true_scale= scaler_y.inverse_transform(y_val_df)\n",
    "\n",
    "    # convert to df again\n",
    "    y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_test_df_true_scale = pd.DataFrame(y_test_true_scale, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=['heave', 'pitch', 'pendulum'])\n",
    "    \n",
    "\n",
    "    # Define your output column names\n",
    "    output_cols = ['heave', 'pitch', 'pendulum']\n",
    "    # R2 score on training data\n",
    "    r2_scores_train = {col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Training R² Scores:\", r2_scores_train)\n",
    "    # R2 score on validation data\n",
    "    r2_scores_val = {col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    # R2 score on full validation data\n",
    "    r2_scores_test = {col: r2_score(y_test_df_true_scale[col], df_test_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Testing R² Scores:\", r2_scores_test)\n",
    "    # MSE on training data\n",
    "    mse_train = {col: mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Training RMSE:\", mse_train)\n",
    "    # MSE on validation data\n",
    "    mse_val = {col: mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    # MSE on full validation data\n",
    "    mse_test = {col: mean_squared_error(y_test_df_true_scale[col], df_test_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Testing RMSE:\", mse_test)\n",
    "    \n",
    "    # Collect the results in a dictionaru\n",
    "    result = {\n",
    "        'data_lenght': lenght[l],\n",
    "        'train_heave_r2': r2_scores_train.get('heave', None),\n",
    "        'train_pitch_r2': r2_scores_train.get('pitch', None),\n",
    "        'train_pendulum_r2': r2_scores_train.get('pendulum', None),\n",
    "        'train_heave_mse': mse_train.get('heave', None),\n",
    "        'train_pitch_mse': mse_train.get('pitch', None),\n",
    "        'train_pendulum_mse': mse_train.get('pendulum', None),\n",
    "        'val_heave_r2': r2_scores_val.get('heave', None),\n",
    "        'val_pitch_r2': r2_scores_val.get('pitch', None),\n",
    "        'val_pendulum_r2': r2_scores_val.get('pendulum', None),\n",
    "        'val_heave_mse': mse_val.get('heave', None),\n",
    "        'val_pitch_mse': mse_val.get('pitch', None),\n",
    "        'val_pendulum_mse': mse_val.get('pendulum', None),\n",
    "        \n",
    "        'test_heave_r2': r2_scores_test.get('heave', None),\n",
    "        'test_pitch_r2': r2_scores_test.get('pitch', None),\n",
    "        'test_pendulum_r2': r2_scores_test.get('pendulum', None),\n",
    "        'test_heave_mse': mse_test.get('heave', None),\n",
    "        'test_pitch_mse': mse_test.get('pitch', None),\n",
    "        'test_pendulum_mse': mse_test.get('pendulum', None),\n",
    "                \n",
    "    }\n",
    "    \n",
    "    # Append the result dictionary to the results list\n",
    "    results.append(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the results list\n",
    "df_data_test_results = pd.DataFrame(results) \n",
    "df_data_test_results.to_csv('Results/LSTM/data test/df_data_test_results_lstm_final_ver2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968222f",
   "metadata": {},
   "source": [
    "# dt senstivety Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea86524",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta','eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Parameters ---\n",
    "\n",
    "heave_LSTM = 64\n",
    "pitch_LSTM = 64\n",
    "pend_LSTM = 64\n",
    "\n",
    "shared_LSTM = 64\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# --- Create input shape info ---\n",
    "time_steps = nb + 1 + nf  # full window\n",
    "n_features = len(input_cols)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt steps\n",
    "steps=[1,2,3,4,5,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Training Data\n",
    "# Create 4 rows of zeros for each DataFrame\n",
    "zero_input = pd.DataFrame(np.zeros((5, len(input_cols))), columns=input_cols)\n",
    "zero_output = pd.DataFrame(np.zeros((5, len(output_cols))), columns=output_cols)\n",
    "\n",
    "input = df_case_train[input_cols]\n",
    "output= df_case_train[output_cols]\n",
    "\n",
    "input=pd.concat([zero_input, input], ignore_index=True)\n",
    "output=pd.concat([zero_input, output], ignore_index=True)\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_train_seq,y_train_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "val_input = df_case_val[input_cols]\n",
    "val_output= df_case_val[output_cols]\n",
    "# scale the data\n",
    "val_input_df_scaled = scaler_X_func_all(val_input)\n",
    "val_output_df_scaled = scaler_y_func(val_output)\n",
    "\n",
    "X_val_seq , Y_val_seq = create_exog_sequences(\n",
    "    input_df = val_input_df_scaled,\n",
    "    output_df= val_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_val_seq_full, Y_val_seq_full = adjust_for_batch(X_val_seq, Y_val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "test_input = df_case_test[input_cols]\n",
    "test_output= df_case_test[output_cols]\n",
    "# scale the data\n",
    "test_input_df_scaled = scaler_X_func_all(test_input)\n",
    "test_output_df_scaled = scaler_y_func(test_output)\n",
    "\n",
    "X_test_seq , Y_test_seq = create_exog_sequences(\n",
    "    input_df = test_input_df_scaled,\n",
    "    output_df= test_output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect the results for each test case\n",
    "results = []\n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step    \n",
    "    # build model \n",
    "\n",
    "    seq_in = Input(batch_shape=(batch_size, time_steps, n_features), name=\"sequence\")\n",
    "\n",
    "    # --- 1. Per-output LSTM layers ---\n",
    "    heave_branch = LSTM(heave_LSTM,stateful=True, return_sequences=True,  batch_size=batch_size, name='heave_lstm')(seq_in)\n",
    "    heave_branch = Dropout(dropout)(heave_branch)\n",
    "\n",
    "    pitch_branch = LSTM(pitch_LSTM,stateful=True, return_sequences=True,batch_size=batch_size,  name='pitch_lstm')(seq_in)\n",
    "    pitch_branch = Dropout(dropout)(pitch_branch)\n",
    "        \n",
    "    pend_branch = LSTM(pend_LSTM, stateful=True, return_sequences=True, batch_size=batch_size, name='pendulum_lstm')(seq_in)\n",
    "    pend_branch = Dropout(dropout)(pend_branch)\n",
    "\n",
    "    # --- 2. Concatenate outputs ---\n",
    "    combined = Concatenate(name='combined_lstm_concat')([heave_branch, pitch_branch, pend_branch])\n",
    "\n",
    "    # --- 3. Shared LSTM layer to learn coupling ---\n",
    "    shared_lstm = LSTM(shared_LSTM, stateful=True,return_sequences=False,batch_size=batch_size,  name='shared_lstm')(combined)\n",
    "    shared_lstm = Dropout(dropout)(shared_lstm)\n",
    "\n",
    "    # --- 4. Final Dense outputs ---\n",
    "    heave_out = Dense(1, activation='tanh', name='heave')(shared_lstm)\n",
    "    pitch_out = Dense(1, activation='tanh', name='pitch')(shared_lstm)\n",
    "    pendulum_out = Dense(1, activation='tanh', name='pendulum')(shared_lstm)\n",
    "\n",
    "    # --- Define the model ---\n",
    "    model_data_test = Model(inputs=seq_in, outputs=[heave_out, pitch_out, pendulum_out])\n",
    "\n",
    "\n",
    "    \n",
    "    X_train_seq_adjusted=X_train_seq[::step,:, :]\n",
    "    y_train_seq_adjusted=y_train_seq[::step,:]\n",
    "    \n",
    "    X_test_seq_adjusted=X_test_seq[::step,:, :]\n",
    "    Y_test_seq_adjusted=Y_test_seq[::step,:]\n",
    "    \n",
    "    X_val_seq_adjusted=X_val_seq[::step,:, :]\n",
    "    Y_val_seq_adjusted=Y_val_seq[::step,:]\n",
    "    \n",
    "    \n",
    "    X_train_seq_adjusted, y_train_seq_adjusted = adjust_for_batch(X_train_seq_adjusted, y_train_seq_adjusted)\n",
    "    X_val_seq_adjusted, Y_val_seq_adjusted = adjust_for_batch(X_val_seq_adjusted, Y_val_seq_adjusted)\n",
    "    X_test_seq_adjusted, Y_test_seq_adjusted = adjust_for_batch(X_test_seq_adjusted, Y_test_seq_adjusted)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # compile the model\n",
    "    model_data_test.compile (optimizer='adam',\n",
    "    loss={'heave':'mse','pitch':'mse','pendulum':'mse'},\n",
    "    metrics={'heave':'mae','pitch':'mae','pendulum':'mae'}\n",
    ")\n",
    "\n",
    "    \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {str(dt)} ----')\n",
    "   \n",
    "    print(\"NaNs in X_train_seq_adjusted:\", np.any(np.isnan(X_train_seq_adjusted)))\n",
    "    print(\"NaNs in y_train_seq_adjusted:\", np.any(np.isnan(y_train_seq_adjusted)))\n",
    "    print(\"NaNs in X_val_seq_adjusted:\", np.any(np.isnan(X_val_seq_adjusted)))\n",
    "    print(\"NaNs in Y_val_seq_adjusted:\", np.any(np.isnan(Y_val_seq_adjusted)))\n",
    "   \n",
    "    \n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    # Train model\n",
    "    history = model_data_test.fit(\n",
    "    X_train_seq_adjusted,\n",
    "    {\n",
    "        'heave': y_train_seq_adjusted[:, 0],\n",
    "        'pitch': y_train_seq_adjusted[:, 1],\n",
    "        'pendulum': y_train_seq_adjusted[:, 2]\n",
    "    },\n",
    "    epochs=500,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    validation_data=(\n",
    "        X_val_seq_adjusted,\n",
    "        {\n",
    "            'heave': Y_val_seq_adjusted[:, 0],\n",
    "            'pitch': Y_val_seq_adjusted[:, 1],\n",
    "            'pendulum': Y_val_seq_adjusted[:, 2]\n",
    "        }\n",
    "    ),\n",
    "    verbose=1,\n",
    "    callbacks=[ResetStatesCallback(), early_stopping_more]\n",
    "        )   \n",
    "    \n",
    "    \n",
    "        \n",
    "    # Get the raw predictions for the training set\n",
    "    model_data_test.reset_states()\n",
    "    print(f'-----predicting on training data----')\n",
    "    \n",
    "    y_train_preds = model_data_test.predict(\n",
    "        X_train_seq_adjusted,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # 3) y_train_preds is a list of three arrays (one per output head)\n",
    "\n",
    "    heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_train_preds]\n",
    "\n",
    "    # 4) (Optional) combine into a single array or DataFrame\n",
    "\n",
    "\n",
    "    y_train_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "    df_train_pred = pd.DataFrame(\n",
    "        y_train_pred_arr,\n",
    "        columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    print(df_train_pred.head())\n",
    "\n",
    "    # 1) Reset & warm‑start again, just like for evaluation\n",
    "    model_data_test.reset_states()\n",
    "\n",
    "    print(f'-----predicting on testing data----')\n",
    "\n",
    "    # 2) Get the raw predictions for the testing set\n",
    "    y_test_preds = model_data_test.predict(\n",
    "    X_test_seq_adjusted,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "\n",
    "    )\n",
    "\n",
    "    # 3) y_val_preds is a list of three arrays (one per output head)\n",
    "    heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_test_preds]\n",
    "\n",
    "    # 4) (Optional) combine into a single array or DataFrame\n",
    "    y_test_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "    df_y_test_pred = pd.DataFrame(\n",
    "    y_test_pred_arr,\n",
    "    columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    print(df_y_test_pred.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1) Reset & warm‑start again, just like for evaluation\n",
    "    model_data_test.reset_states()\n",
    "\n",
    "    print(f'-----predicting on validation data----')\n",
    "\n",
    "    # 2) Get the raw predictions for the validation set\n",
    "    y_val_preds = model_data_test.predict(\n",
    "    X_val_seq_adjusted,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "\n",
    "    )\n",
    "\n",
    "    # 3) y_val_preds is a list of three arrays (one per output head)\n",
    "    heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_val_preds]\n",
    "\n",
    "    # 4) (Optional) combine into a single array or DataFrame\n",
    "    y_val_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "    df_val_pred = pd.DataFrame(\n",
    "    y_val_pred_arr,\n",
    "    columns=['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "    )\n",
    "\n",
    "    print(df_val_pred.head())\n",
    "    \n",
    "               \n",
    "    \n",
    "    # scale back the predictions to original scale\n",
    "    train_pred_true_scale= scaler_y.inverse_transform(df_train_pred)\n",
    "    test_pred_true_scale= scaler_y.inverse_transform(df_y_test_pred)\n",
    "    val_pred_true_scale= scaler_y.inverse_transform(df_val_pred)\n",
    "    output_cols = ['heave', 'pitch', 'pendulum']  # replace with your actual target column names\n",
    "\n",
    "    # Convert NumPy arrays back to DataFrames with column names\n",
    "    df_train_pred_true_scale = pd.DataFrame(\n",
    "    train_pred_true_scale, \n",
    "    columns=[f\"{col}_pred\" for col in output_cols]  # creates columns: 'heave_pred', 'pitch_pred', etc.\n",
    "    )\n",
    "\n",
    "    df_test_pred_true_scale = pd.DataFrame(\n",
    "    test_pred_true_scale,\n",
    "    columns=[f\"{col}_pred\" for col in output_cols])\n",
    "    \n",
    "    df_val_pred_true_scale = pd.DataFrame(\n",
    "    val_pred_true_scale,\n",
    "    columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "    # get df with the traing and validation data at corrensponding time steps\n",
    "    y_train_df=pd.DataFrame(y_train_seq_adjusted, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_val_df=pd.DataFrame(Y_val_seq_adjusted, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_test_df=pd.DataFrame(Y_test_seq_adjusted, columns=['heave', 'pitch', 'pendulum'])\n",
    "\n",
    "    # scale back tp oridinal scale\n",
    "    y_train_true_scale= scaler_y.inverse_transform(y_train_df)\n",
    "    y_test_true_scale= scaler_y.inverse_transform(y_test_df)\n",
    "    y_val_true_scale= scaler_y.inverse_transform(y_val_df)\n",
    "\n",
    "    # convert to df again\n",
    "    y_train_df_true_scale = pd.DataFrame(y_train_true_scale, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_test_df_true_scale = pd.DataFrame(y_test_true_scale, columns=['heave', 'pitch', 'pendulum'])\n",
    "    y_val_df_true_scale = pd.DataFrame(y_val_true_scale, columns=['heave', 'pitch', 'pendulum'])\n",
    "    \n",
    "\n",
    "    # Define your output column names\n",
    "    output_cols = ['heave', 'pitch', 'pendulum']\n",
    "    # R2 score on training data\n",
    "    r2_scores_train = {col: r2_score(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Training R² Scores:\", r2_scores_train)\n",
    "    # R2 score on validation data\n",
    "    r2_scores_val = {col: r2_score(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    # R2 score on full validation data\n",
    "    r2_scores_test = {col: r2_score(y_test_df_true_scale[col], df_test_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Testing R² Scores:\", r2_scores_test)\n",
    "    # MSE on training data\n",
    "    mse_train = {col: mean_squared_error(y_train_df_true_scale[col], df_train_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Training RMSE:\", mse_train)\n",
    "    # MSE on validation data\n",
    "    mse_val = {col: mean_squared_error(y_val_df_true_scale[col], df_val_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    # MSE on full validation data\n",
    "    mse_test = {col: mean_squared_error(y_test_df_true_scale[col], df_test_pred_true_scale[f\"{col}_pred\"]) for col in output_cols}\n",
    "    print(\"Testing RMSE:\", mse_test)\n",
    "    \n",
    "    # Collect the results in a dictionaru\n",
    "    result = {\n",
    "        'dt': dt,\n",
    "        'train_heave_r2': r2_scores_train.get('heave', None),\n",
    "        'train_pitch_r2': r2_scores_train.get('pitch', None),\n",
    "        'train_pendulum_r2': r2_scores_train.get('pendulum', None),\n",
    "        'train_heave_mse': mse_train.get('heave', None),\n",
    "        'train_pitch_mse': mse_train.get('pitch', None),\n",
    "        'train_pendulum_mse': mse_train.get('pendulum', None),\n",
    "        'val_heave_r2': r2_scores_val.get('heave', None),\n",
    "        'val_pitch_r2': r2_scores_val.get('pitch', None),\n",
    "        'val_pendulum_r2': r2_scores_val.get('pendulum', None),\n",
    "        'val_heave_mse': mse_val.get('heave', None),\n",
    "        'val_pitch_mse': mse_val.get('pitch', None),\n",
    "        'val_pendulum_mse': mse_val.get('pendulum', None),\n",
    "        \n",
    "        'test_heave_r2': r2_scores_test.get('heave', None),\n",
    "        'test_pitch_r2': r2_scores_test.get('pitch', None),\n",
    "        'test_pendulum_r2': r2_scores_test.get('pendulum', None),\n",
    "        'test_heave_mse': mse_test.get('heave', None),\n",
    "        'test_pitch_mse': mse_test.get('pitch', None),\n",
    "        'test_pendulum_mse': mse_test.get('pendulum', None),\n",
    "                \n",
    "    }\n",
    "    \n",
    "    # Append the result dictionary to the results list\n",
    "    results.append(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb205097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the results list\n",
    "df_dt_test_results = pd.DataFrame(results) \n",
    "df_dt_test_results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt_test_results.to_csv('Results/dt_tests/df_dt_test_results_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de5755",
   "metadata": {},
   "source": [
    "# Noise Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Best model\n",
    "model = load_model(\"model_checkpoint_eta_only_nb5_nf5_new_archt_eta_vel_acc_more_stopping_creteria.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39add7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the noisy test data\n",
    "df_case_test_noisy=pd.read_csv('Results/df_case_test_noisy.csv')\n",
    "df_case_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341092fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta' , 'eta_velocity','eta_acceleration']\n",
    "output_cols = ['heave', 'pitch', 'pendulum']\n",
    "nb= 5\n",
    "nf=5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the Testing Data\n",
    "input = df_case_test_noisy[input_cols]\n",
    "output= df_case_test_noisy[output_cols]\n",
    "\n",
    "# scale the data\n",
    "input_df_scaled = scaler_X_func_all(input)\n",
    "output_df_scaled = scaler_y_func(output)\n",
    "\n",
    "X_test_seq,y_test_seq= create_exog_sequences(\n",
    "    input_df = input_df_scaled,\n",
    "    output_df=output_df_scaled,\n",
    "    input_cols= input_cols,\n",
    "    output_cols = output_cols,\n",
    "    nb = nb,\n",
    "    nf= nf\n",
    ")\n",
    "#Remove extra samples at the end to match batch size\n",
    "def adjust_for_batch(X, y):\n",
    "    n = (X.shape[0] // batch_size) * batch_size\n",
    "    return X[:n], y[:n]\n",
    "\n",
    "X_test_seq, y_test_seq = adjust_for_batch(X_test_seq, y_test_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictoins \n",
    "model.reset_states()\n",
    "\n",
    "y_test_preds = model.predict(\n",
    "        X_test_seq,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93429ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heave_pred, pitch_pred, pend_pred = [arr.squeeze(-1) for arr in y_test_preds]\n",
    "y_test_pred_arr = np.vstack([heave_pred, pitch_pred, pend_pred]).T\n",
    "# Create prediction DataFrames (scaled) ----\n",
    "pred_cols = ['heave_pred', 'pitch_pred', 'pendulum_pred']\n",
    "\n",
    "df_noise_test_pred = pd.DataFrame(y_test_pred_arr, columns=pred_cols)\n",
    "\n",
    "# ---- 5) Inverse transform predictions to original scale ----\n",
    "noise_test_pred_true_scale = scaler_y.inverse_transform(df_noise_test_pred)\n",
    "\n",
    "# ---- 6) Create DataFrames for unscaled predictions ----\n",
    "\n",
    "df_noise_test_pred_true_scale = pd.DataFrame(noise_test_pred_true_scale, columns=[f\"{col}_pred\" for col in output_cols])\n",
    "\n",
    "\n",
    "# ---- 7) Inverse transform ground truth Y data ----\n",
    "y_test_true_scale = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# ---- 8) Create DataFrames for ground truth ----\n",
    "df_y_test_true_scale = pd.DataFrame(y_test_true_scale, columns=output_cols)\n",
    "\n",
    "# ---- 9) Compute Metrics ----\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# R² Scores\n",
    "r2_scores = {\n",
    "    col: r2_score(df_y_test_true_scale[col], df_noise_test_pred_true_scale[f\"{col}_pred\"])\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "# RMSE\n",
    "rmse = {\n",
    "    col: np.sqrt(mean_squared_error(df_y_test_true_scale[col], df_noise_test_pred_true_scale[f\"{col}_pred\"]))\n",
    "    for col in output_cols\n",
    "}\n",
    "\n",
    "\n",
    "# ---- 10) Print Results ----\n",
    "print(\"testing R² Scores:\", r2_scores)\n",
    "print(\"Testing RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc315ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "df_y_test_true_scale.to_csv('Results/noisy test/LSTM_true.csv')\n",
    "df_noise_test_pred_true_scale.to_csv('Results/noisy test/LSTM_pred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
