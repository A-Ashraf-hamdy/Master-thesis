{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64884434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chardet\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt \n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import plotly.colors as pc\n",
    "\n",
    "%matplotlib inline  \n",
    "import psutil\n",
    "from pathlib import Path\n",
    "#from Functions import *\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "from scipy.signal import savgol_filter\n",
    "from tqdm import tqdm\n",
    "from sysidentpy.model_structure_selection import FROLS\n",
    "from sysidentpy.basis_function import Polynomial\n",
    "from sysidentpy.metrics import mean_squared_error\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604ddaf",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ebffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_arx_lagged_with_scalers(\n",
    "    df,\n",
    "    input_cols,\n",
    "    output_cols,\n",
    "    scaler_X_func,\n",
    "    scaler_y_func,\n",
    "    na=0,\n",
    "    nb_past=0,\n",
    "    nf_future=0,\n",
    "    test_name_col='test_name',\n",
    "    y_initial_mode='original'  # 'original' ➔ normal; 'zero' ➔ prepend zero rows after scaling\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ARX lagged data from a single DataFrame using custom scaler functions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: original DataFrame (raw, unscaled) with test_name column\n",
    "    - input_cols: list of input column names to scale\n",
    "    - output_cols: list of output column names to scale\n",
    "    - scaler_X_func: function or fitted scaler to scale inputs (df ➔ df)\n",
    "    - scaler_y_func: function or fitted scaler to scale outputs (df ➔ df)\n",
    "    - na: number of output lags (autoregressive)\n",
    "    - nb_past: number of past input lags (exogenous input past)\n",
    "    - nf_future: number of future input lags (exogenous input preview/future)\n",
    "    - test_name_col: column name that identifies test cases\n",
    "    - y_initial_mode: 'original' ➔ normal; 'zero' ➔ prepend zero rows after scaling\n",
    "\n",
    "    Returns:\n",
    "    - X_lagged_df: lagged input DataFrame\n",
    "    - y_target_df: target output DataFrame\n",
    "    - y_initial_df: DataFrame of initial output values used as initial conditions\n",
    "    \"\"\"\n",
    "\n",
    "    X_df_list = []\n",
    "    Y_df_list = []\n",
    "    Y_initial_list = []\n",
    "\n",
    "    # Unique test cases\n",
    "    test_names = df[test_name_col].unique()\n",
    "\n",
    "    for test in test_names:\n",
    "        df_test = df[df[test_name_col] == test].copy()\n",
    "\n",
    "        # Apply scaling functions first\n",
    "        X_scaled = scaler_X_func(df_test[input_cols])\n",
    "        y_scaled = scaler_y_func(df_test[output_cols])\n",
    "\n",
    "        # Convert to DataFrames and keep test_name column for tracking\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=input_cols)\n",
    "        y_scaled_df = pd.DataFrame(y_scaled, columns=output_cols)\n",
    "\n",
    "        if y_initial_mode == 'zero':\n",
    "            # Create zero rows in scaled space\n",
    "            lag_required = max(na, nb_past)\n",
    "\n",
    "            zero_inputs = pd.DataFrame(\n",
    "                np.zeros((lag_required, len(input_cols))),\n",
    "                columns=input_cols\n",
    "            )\n",
    "\n",
    "            zero_outputs = pd.DataFrame(\n",
    "                np.zeros((lag_required, len(output_cols))),\n",
    "                columns=output_cols\n",
    "            )\n",
    "\n",
    "            # Concatenate zero inputs/outputs\n",
    "            zero_rows_inputs = zero_inputs\n",
    "            zero_rows_outputs = zero_outputs\n",
    "\n",
    "            # Concatenate zero rows on top of X_scaled_df and y_scaled_df\n",
    "            X_scaled_df = pd.concat([zero_rows_inputs, X_scaled_df], ignore_index=True)\n",
    "            y_scaled_df = pd.concat([zero_rows_outputs, y_scaled_df], ignore_index=True)\n",
    "\n",
    "        # Now proceed with lag creation\n",
    "        inputs = X_scaled_df.values\n",
    "        outputs = y_scaled_df.values\n",
    "\n",
    "        n_samples = len(X_scaled_df)\n",
    "\n",
    "        lag_required = max(na, nb_past)\n",
    "        min_future_offset = nf_future\n",
    "\n",
    "       \n",
    "        # ➤ Save the first `na` rows as y_initial\n",
    "        if y_initial_mode == 'zero':\n",
    "            start_idx = lag_required\n",
    "        else:  # 'original'\n",
    "            start_idx = 0\n",
    "\n",
    "        end_idx = n_samples - nf_future\n",
    "        \n",
    "        if na > 0:\n",
    "            if y_initial_mode == 'original':\n",
    "               \n",
    "                y_initial = y_scaled_df.iloc[:lag_required].copy()\n",
    "            else:  # 'zero'\n",
    "                y_initial= pd.DataFrame(np.zeros((na, len(output_cols))), columns=output_cols)\n",
    "               \n",
    "                \n",
    "            y_initial[test_name_col] = test\n",
    "            Y_initial_list.append(y_initial)\n",
    "\n",
    "        X_rows = []\n",
    "        Y_rows = []\n",
    "\n",
    "        for t in range(start_idx, end_idx):\n",
    "            row = {}\n",
    "            \n",
    "            #Add current unscaled input values\n",
    "            for i, in_col in enumerate(input_cols):\n",
    "                row[in_col] = inputs[t, i]  # Current time step value\n",
    "\n",
    "            # Add output past lags (autoregressive)\n",
    "            for lag in range(1, na + 1):\n",
    "                for i, out_col in enumerate(output_cols):\n",
    "                    idx = t - lag\n",
    "                    if y_initial_mode == 'zero' and idx < 0:\n",
    "                        row[f'{out_col}_lag_{lag}'] = 0.0\n",
    "                    else:\n",
    "                        row[f'{out_col}_lag_{lag}'] = outputs[idx, i]\n",
    "                            \n",
    "            # Add input past lags (exogenous)\n",
    "            for lag in range(1, nb_past + 1):\n",
    "                for i, in_col in enumerate(input_cols):\n",
    "                    idx = t - lag\n",
    "                    if y_initial_mode == 'zero' and idx < 0:\n",
    "                        row[f'{in_col}_past_{lag}'] = 0.0\n",
    "                    else:\n",
    "                        row[f'{in_col}_past_{lag}'] = inputs[idx, i]\n",
    "\n",
    "            # Add input future lags (preview control)\n",
    "            for lag in range(1, nf_future + 1):\n",
    "                if t + lag < n_samples:\n",
    "                    for i, in_col in enumerate(input_cols):\n",
    "                        row[f'{in_col}_future_{lag}'] = inputs[t + lag, i]\n",
    "                else:\n",
    "                    for in_col in input_cols:\n",
    "                        row[f'{in_col}_future_{lag}'] = 0.0  # optional padding if beyond end\n",
    "\n",
    "            X_rows.append(row)\n",
    "            Y_rows.append(outputs[t])\n",
    "\n",
    "        # Create DataFrames for this test\n",
    "        X_df = pd.DataFrame(X_rows)\n",
    "        Y_df = pd.DataFrame(Y_rows, columns=output_cols)\n",
    "\n",
    "        # Add test_name to track\n",
    "        X_df[test_name_col] = test\n",
    "        Y_df[test_name_col] = test\n",
    "\n",
    "        X_df_list.append(X_df)\n",
    "        Y_df_list.append(Y_df)\n",
    "\n",
    "    # Combine all test cases into final DataFrames\n",
    "    X_lagged_df = pd.concat(X_df_list, ignore_index=True)\n",
    "    y_target_df = pd.concat(Y_df_list, ignore_index=True)\n",
    "\n",
    "    if na > 0:\n",
    "        y_initial_df = pd.concat(Y_initial_list, ignore_index=True)\n",
    "    else:\n",
    "        y_initial_df = pd.DataFrame()\n",
    "\n",
    "    return X_lagged_df, y_target_df, y_initial_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_recursive_series(model, X_df, output_cols, X_feature_names, na):\n",
    "    \"\"\"\n",
    "    Predict recursively one step at a time using lag updates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Trained model\n",
    "    X_df : pd.DataFrame\n",
    "        Input features (with lag columns, same format as training)\n",
    "    output_cols : list of str\n",
    "        List of output columns (e.g., ['heave', 'pitch', ...])\n",
    "    X_feature_names : list\n",
    "        Ordered list of input feature names expected by the model\n",
    "    na : int\n",
    "        Number of output lags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred_df : pd.DataFrame\n",
    "        Recursive prediction results (same shape as y_df)\n",
    "    x_used_df : pd.DataFrame\n",
    "        Input rows actually used at each timestep (after lag updates)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    if na == 0:\n",
    "        y_pred = model.predict(X_df)\n",
    "        return pd.DataFrame(y_pred, columns=output_cols), X_df.copy()\n",
    "\n",
    "    n_steps = len(X_df)\n",
    "    n_outputs = len(output_cols)\n",
    "\n",
    "    y_pred = np.zeros((n_steps, n_outputs))\n",
    "    x_used_rows = []\n",
    "\n",
    "    # Start with the first input row\n",
    "    x_row = X_df.iloc[0].copy()\n",
    "    \n",
    "    value_threshold = 5\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        \n",
    "        # Check if this row contains any absurd value\n",
    "        if np.any(np.abs(x_row.values) > value_threshold):\n",
    "            print(f\"[ABORT] Found large value at step t={t}. Aborting prediction.\")\n",
    "            return pd.DataFrame(np.zeros((n_steps, n_outputs)), columns=output_cols), pd.DataFrame(x_used_rows)\n",
    "        \n",
    "        # Build input feature vector for model\n",
    "        x_input = pd.DataFrame([[x_row[feat] for feat in X_feature_names]], columns=X_feature_names)\n",
    "\n",
    "        # Save the input row used at this step\n",
    "        x_used_rows.append(x_input.iloc[0])\n",
    "\n",
    "        # Predict\n",
    "        pred = model.predict(x_input)\n",
    "        # Ensure it's always a 2D shape: (1, n_outputs)\n",
    "        if pred.ndim == 1:\n",
    "            pred = pred.reshape(1, -1)\n",
    "\n",
    "        pred = pred[0]  # Now this is always indexable\n",
    "\n",
    "        y_pred[t] = pred\n",
    "\n",
    "        if t < n_steps - 1:\n",
    "            # Prepare next input row\n",
    "            x_row_next = X_df.iloc[t + 1].copy()\n",
    "\n",
    "            for col in output_cols:\n",
    "                # Shift lags: lag_n = lag_{n-1}, ..., lag_2 = lag_1\n",
    "                for lag in reversed(range(2, na + 1)):\n",
    "                    x_row_next[f\"{col}_lag_{lag}\"] = x_row[f\"{col}_lag_{lag - 1}\"]\n",
    "                # Set lag_1 to current prediction\n",
    "                x_row_next[f\"{col}_lag_1\"] = pred[output_cols.index(col)]\n",
    "\n",
    "            x_row = x_row_next.copy()\n",
    "\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=output_cols)\n",
    "    x_used_df = pd.DataFrame(x_used_rows)\n",
    "\n",
    "    return y_pred_df, x_used_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_input_features(\n",
    "    X_lagged_df,\n",
    "    input_type='eta',\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Select input columns, preserving:\n",
    "    - Base inputs (e.g., 'eta', 'eta_velocity')\n",
    "    - Their lagged versions ('_past_', '_future_', '_lag_')\n",
    "    - Autoregressive terms ('_lag_')\n",
    "    - 'test_name' (if present)\n",
    "    \"\"\"\n",
    "    cols_to_keep = []\n",
    "\n",
    "    # Base input variable(s) (always keep these)\n",
    "    if input_type == 'eta':\n",
    "        base_inputs = ['eta']\n",
    "    elif input_type == 'eta+vel':\n",
    "        base_inputs = ['eta', 'eta_velocity']\n",
    "    elif input_type == 'eta+vel+acc':\n",
    "        base_inputs = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "    else:\n",
    "        raise ValueError(\"input_type must be 'eta', 'eta+vel', or 'eta+vel+acc'\")\n",
    "\n",
    "    \n",
    "    # Step 1: Add base inputs (if they exist in the DataFrame)\n",
    "    for col in base_inputs:\n",
    "        if col in X_lagged_df.columns and col not in cols_to_keep:\n",
    "            cols_to_keep.append(col)\n",
    "\n",
    "    # Step 2: Add lagged versions (past/future) of base inputs\n",
    "    for col in base_inputs:\n",
    "        if keep_past:\n",
    "            past_cols = [c for c in X_lagged_df.columns if f\"{col}_past_\" in c]\n",
    "            cols_to_keep.extend(past_cols)\n",
    "        if keep_future:\n",
    "            future_cols = [c for c in X_lagged_df.columns if f\"{col}_future_\" in c]\n",
    "            cols_to_keep.extend(future_cols)\n",
    "\n",
    "    # Step 3: Add autoregressive terms (if any)\n",
    "    arx_cols = [c for c in X_lagged_df.columns if '_lag_' in c and c not in cols_to_keep]\n",
    "    cols_to_keep.extend(arx_cols)\n",
    "\n",
    "    # Step 4: Always keep 'test_name' (if present)\n",
    "    test_name_col = next((c for c in X_lagged_df.columns if c.lower() == 'test_name'), None)\n",
    "    if test_name_col and test_name_col not in cols_to_keep:\n",
    "        cols_to_keep.append(test_name_col)\n",
    "\n",
    "    # Step 5: Fallback if no features found (return all columns + warning)\n",
    "    if not cols_to_keep:\n",
    "        print(\"Warning: No matching features found. Returning all columns.\")\n",
    "        return X_lagged_df.copy()\n",
    "\n",
    "    return X_lagged_df[cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdccb0-fee1-4899-a5c4-e8d0ab55c0f6",
   "metadata": {},
   "source": [
    "# loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454d592-6dd4-4418-b45f-c849f9f26a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train_full = pd.read_csv('prepared_data/train_data.csv')\n",
    "df_val_full = pd.read_csv('prepared_data/val_data.csv')\n",
    "df_test_full = pd.read_csv('prepared_data/test_data.csv')\n",
    "\n",
    "print(df_train_full.head())\n",
    "print(df_val_full.head())\n",
    "print(df_test_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1dbef1-c2a4-40a5-9084-c8ad9de4e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrating Only the case we want to work with\n",
    "case='Tp6p8s_Hs2m'\n",
    "df_case_train = df_train_full[df_train_full['test_name'] == case].copy().reset_index(drop=True)\n",
    "df_case_val = df_val_full[df_val_full['test_name'] == case].copy().reset_index(drop=True)\n",
    "df_case_test = df_test_full[df_test_full['test_name'] == case].copy().reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7862802",
   "metadata": {},
   "source": [
    "# Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ed36f-5952-4882-b0ff-ca73947f58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave',  'pitch', 'pendulum']\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "scaler_X.fit(df_train_full[input_cols])\n",
    "scaler_y.fit(df_train_full[output_cols])\n",
    "\n",
    "# Transform train\n",
    "X_train_scaled = scaler_X.transform(df_train_full[input_cols])\n",
    "y_train_scaled = scaler_y.transform(df_train_full[output_cols])\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=input_cols)\n",
    "y_train_scaled_df = pd.DataFrame(y_train_scaled, columns=output_cols)\n",
    "\n",
    "X_train_scaled_df['test_name'] = df_train_full['test_name'].values\n",
    "y_train_scaled_df['test_name'] = df_train_full['test_name'].values\n",
    "\n",
    "# Repeat for validation (no fit!)\n",
    "X_val_scaled = scaler_X.transform(df_val_full[input_cols])\n",
    "y_val_scaled = scaler_y.transform(df_val_full[output_cols])\n",
    "\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=input_cols)\n",
    "y_val_scaled_df = pd.DataFrame(y_val_scaled, columns=output_cols)\n",
    "\n",
    "X_val_scaled_df['test_name'] = df_val_full['test_name'].values\n",
    "y_val_scaled_df['test_name'] = df_val_full['test_name'].values\n",
    "\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "scaler_X_func = lambda df: scaler_X.transform(df)\n",
    "scaler_y_func = lambda df: scaler_y.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave',  'pitch', 'pendulum']\n",
    "# Initialize scalers\n",
    "scaler_X_nov = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y_nov = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "scaler_X_nov.fit(df_train_full[input_cols])\n",
    "scaler_y_nov.fit(df_train_full[output_cols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define scalling function\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "scaler_X_func_nov = lambda df: scaler_X_nov.transform(df)\n",
    "scaler_y_func_nov = lambda df: scaler_y_nov.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['heave']\n",
    "# Initialize scalers\n",
    "scaler_X_nov = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y_heave = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "scaler_X_nov.fit(df_train_full[input_cols])\n",
    "scaler_y_heave.fit(df_train_full[output_cols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define scalling function\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "scaler_y_func_heave = lambda df: scaler_y_heave.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pitch']\n",
    "# Initialize scalers\n",
    "\n",
    "scaler_y_pitch = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_pitch.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "\n",
    "scaler_y_func_pitch = lambda df: scaler_y_pitch.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pendulum']\n",
    "# Initialize scalers\n",
    "\n",
    "scaler_y_pend = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_pend.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "\n",
    "scaler_y_func_pend = lambda df: scaler_y_pend.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c42f",
   "metadata": {},
   "source": [
    "# Lag analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0afae",
   "metadata": {},
   "source": [
    "# ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f970d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "def automated_lag_analysis(\n",
    "    df,\n",
    "    test_case,\n",
    "    output_cols,\n",
    "    max_lag=40\n",
    "):\n",
    "    \"\"\"\n",
    "    Automated lag analysis for ACF, PACF, and MI for multiple output columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: your training DataFrame (with 'test_name' column)\n",
    "    - test_case: specific test_name or 'all' for the entire dataset\n",
    "    - output_cols: list of output columns to process\n",
    "    - max_lag: number of lags to compute\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the data for the specific test case, or use all\n",
    "    if test_case == 'all':\n",
    "        df_case = df.copy()\n",
    "        case_label = 'ALL_CASES'\n",
    "    else:\n",
    "        df_case = df[df['test_name'] == test_case]\n",
    "        case_label = test_case\n",
    "\n",
    "    for col in output_cols:\n",
    "        print(f\"\\n=== Lag Analysis for {col} (Test Case: {case_label}) ===\")\n",
    "\n",
    "        # Extract the signal\n",
    "        y_signal = df_case[col].values\n",
    "\n",
    "        # Prepare MI scores\n",
    "        mi_scores = []\n",
    "\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            y_t = y_signal[lag:]\n",
    "            y_t_minus_lag = y_signal[:-lag]\n",
    "\n",
    "            if len(y_t_minus_lag) > 0:\n",
    "                mi = mutual_info_regression(\n",
    "                    y_t_minus_lag.reshape(-1, 1),\n",
    "                    y_t.ravel(),\n",
    "                    random_state=42\n",
    "                )\n",
    "                mi_scores.append(mi[0])\n",
    "            else:\n",
    "                mi_scores.append(0)\n",
    "\n",
    "        # Plot ACF, PACF, MI side by side\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "        # ACF\n",
    "        plot_acf(y_signal, lags=max_lag, ax=axs[0])\n",
    "        axs[0].set_title(f'ACF - {col}')\n",
    "\n",
    "        # PACF\n",
    "        plot_pacf(y_signal, lags=max_lag, ax=axs[1])\n",
    "        axs[1].set_title(f'PACF - {col}')\n",
    "\n",
    "        # Mutual Information\n",
    "        axs[2].plot(range(1, max_lag + 1), mi_scores, marker='o')\n",
    "        axs[2].set_title(f'Mutual Information - {col}')\n",
    "        axs[2].set_xlabel('Lag')\n",
    "        axs[2].set_ylabel('MI')\n",
    "        axs[2].grid(True)\n",
    "\n",
    "        plt.suptitle(f'Lag Analysis - {col} - {case_label}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ea79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "def automated_lag_analysis_improved(\n",
    "    df,\n",
    "    test_case,\n",
    "    output_cols,\n",
    "    max_lag=40\n",
    "):\n",
    "    \"\"\"\n",
    "    Automated lag analysis for ACF and PACF for multiple output columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: your training DataFrame (with 'test_name' column)\n",
    "    - test_case: specific test_name or 'all' for the entire dataset\n",
    "    - output_cols: list of output columns to process\n",
    "    - max_lag: number of lags to compute\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the data for the specific test case, or use all\n",
    "    if test_case == 'all':\n",
    "        df_case = df.copy()\n",
    "        case_label = 'ALL_CASES'\n",
    "    else:\n",
    "        df_case = df[df['test_name'] == test_case]\n",
    "        case_label = test_case\n",
    "\n",
    "    for col in output_cols:\n",
    "        print(f\"\\n=== Lag Analysis for {col} (Test Case: {case_label}) ===\")\n",
    "\n",
    "        # Extract the signal\n",
    "        y_signal = df_case[col].values\n",
    "\n",
    "        # Create larger figure and improved layout\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "        # ACF\n",
    "        plot_acf(y_signal, lags=max_lag, ax=axs[0])\n",
    "        axs[0].set_title(f'ACF - {col}')\n",
    "        axs[0].set_ylim(-0.5, 1.2)\n",
    "        axs[0].set_xlabel(\"$n_a\")\n",
    "\n",
    "        # PACF\n",
    "        plot_pacf(y_signal, lags=max_lag, ax=axs[1])\n",
    "        axs[1].set_title(f'PACF - {col}')\n",
    "        axs[1].set_ylim(-1.2, 1.2)\n",
    "        axs[1].set_xlabel(\"$_a$\")\n",
    "\n",
    "        plt.suptitle(f'Output Lag Analysis - {col} ', fontsize=18, weight='bold')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e351d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_lag_analysis_improved(\n",
    "    df=df_train_full,\n",
    "    test_case='Tp6p8s_Hs2m',\n",
    "    output_cols=['heave',  'pitch',  'pendulum'],\n",
    "    max_lag=35\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79a3a2",
   "metadata": {},
   "source": [
    "## MI and Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "case='Tp6p8s_Hs2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "def input_output_mi_heatmap_by_lag(\n",
    "    df,\n",
    "    test_case,\n",
    "    input_cols,\n",
    "    output_cols,\n",
    "    max_lag=10  # include nb0 to nb{max_lag}\n",
    "):\n",
    "    df_case = df.copy() if test_case == 'all' else df[df['test_name'] == test_case]\n",
    "    case_label = 'ALL_CASES' if test_case == 'all' else test_case\n",
    "\n",
    "    X_data = df_case[input_cols].values\n",
    "    Y_data = df_case[output_cols].values\n",
    "    n_samples = len(df_case)\n",
    "\n",
    "    for input_idx, input_col in enumerate(input_cols):\n",
    "        mi_matrix = []\n",
    "\n",
    "        for output_idx in range(len(output_cols)):\n",
    "            mi_row = []\n",
    "\n",
    "            for lag in range(0, max_lag + 1):  # now starts from lag = 0 (nb0)\n",
    "                if lag >= n_samples:\n",
    "                    mi_row.append(0)\n",
    "                    continue\n",
    "\n",
    "                if lag == 0:\n",
    "                    x = X_data[:, input_idx]\n",
    "                    y = Y_data[:, output_idx]\n",
    "                else:\n",
    "                    x = X_data[:-lag, input_idx]\n",
    "                    y = Y_data[lag:, output_idx]\n",
    "\n",
    "                mi = mutual_info_regression(x.reshape(-1, 1), y.ravel(), random_state=42)\n",
    "                mi_row.append(mi[0])\n",
    "\n",
    "            mi_matrix.append(mi_row)\n",
    "\n",
    "        # Build DataFrame: outputs as rows, lags as columns\n",
    "        lag_labels = [f'nb{lag}' for lag in range(0, max_lag + 1)]\n",
    "        mi_df = pd.DataFrame(mi_matrix, index=output_cols, columns=lag_labels)\n",
    "\n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(max_lag * 0.8 + 4, len(output_cols) * 0.8 + 2))\n",
    "        ax = sns.heatmap(\n",
    "            mi_df,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            cmap=\"YlOrRd\",\n",
    "            cbar_kws={\"label\": \"Mutual Information \"},\n",
    "            linewidths=0.5\n",
    "        )\n",
    "\n",
    "        ax.set_title(f'Mutual Information Between Output DOFs and Free Surface Elevation (η) ', fontsize=16, pad=20)\n",
    "        ax.set_xlabel(\"Lag ($n_b$)\", fontsize=13)\n",
    "        ax.set_ylabel(\"Outputs\", fontsize=13)\n",
    "        \n",
    "        ax.tick_params(labelsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_mi_heatmap_by_lag(\n",
    "    df=df_train_full,\n",
    "    test_case=case,\n",
    "    input_cols=['eta'],\n",
    "    output_cols=['heave',  'pitch',  'pendulum'],\n",
    "    max_lag=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "def input_output_corr_heatmap_by_lag(\n",
    "    df,\n",
    "    test_case,\n",
    "    input_cols,\n",
    "    output_cols,\n",
    "    max_lag=10  # includes nb0 to nb{max_lag}\n",
    "):\n",
    "    df_case = df.copy() if test_case == 'all' else df[df['test_name'] == test_case]\n",
    "    case_label = 'ALL_CASES' if test_case == 'all' else test_case\n",
    "\n",
    "    X_data = df_case[input_cols].values\n",
    "    Y_data = df_case[output_cols].values\n",
    "    n_samples = len(df_case)\n",
    "\n",
    "    for input_idx, input_col in enumerate(input_cols):\n",
    "        corr_matrix = []\n",
    "\n",
    "        for output_idx in range(len(output_cols)):\n",
    "            corr_row = []\n",
    "\n",
    "            for lag in range(0, max_lag + 1):  # nb0 to nb{max_lag}\n",
    "                if lag >= n_samples:\n",
    "                    corr_row.append(np.nan)\n",
    "                    continue\n",
    "\n",
    "                if lag == 0:\n",
    "                    x = X_data[:, input_idx]\n",
    "                    y = Y_data[:, output_idx]\n",
    "                else:\n",
    "                    x = X_data[:-lag, input_idx]\n",
    "                    y = Y_data[lag:, output_idx]\n",
    "\n",
    "                corr = np.corrcoef(x, y)[0, 1]\n",
    "                corr_row.append(corr)\n",
    "\n",
    "            corr_matrix.append(corr_row)\n",
    "\n",
    "        lag_labels = [f'nb{lag}' for lag in range(0, max_lag + 1)]\n",
    "        corr_df = pd.DataFrame(corr_matrix, index=output_cols, columns=lag_labels)\n",
    "\n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(max_lag * 0.8 + 4, len(output_cols) * 0.8 + 2))\n",
    "        ax = sns.heatmap(\n",
    "            corr_df,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            cbar_kws={\"label\": \"Correlation \"},\n",
    "            linewidths=0.5,\n",
    "            vmin=-1,\n",
    "            vmax=1\n",
    "        )\n",
    "\n",
    "        ax.set_title(f'Correlation Heatmap Between Output DOFs and Free Surface Elevation (η) ', fontsize=16, pad=20)\n",
    "        ax.set_xlabel(\"Lag ($n_b$)\", fontsize=13)\n",
    "        ax.set_ylabel(\"Outputs\", fontsize=13)\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1857d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_corr_heatmap_by_lag(\n",
    "    df=df_train_full,\n",
    "    test_case=case,\n",
    "    input_cols=['eta'],\n",
    "    output_cols=['heave',  'pitch',  'pendulum'],\n",
    "    max_lag=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a438d70-7ed4-4dfd-af9a-2aa280ee4b01",
   "metadata": {},
   "source": [
    "## Checking if the system is causual or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "case='Tp6p8s_Hs2m'\n",
    "input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave',  'pitch', 'pendulum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c97d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_nd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Base save folder\n",
    "save_folder = 'linear_regression/causal_or_not/saved_models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values \n",
    "na  = 0\n",
    "nb  =4 \n",
    "nf_max=6\n",
    "degree = 1\n",
    "xcase = 'eta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724f863-18ed-4c2e-9376-1d127bb4673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for nf in range(0,nf_max+1): \n",
    "    \n",
    "\n",
    "\n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "    # get  df at true scale with only this test case\n",
    "    df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "            df = df_case_train,\n",
    "            input_cols  = input_cols,\n",
    "            output_cols   = output_cols,\n",
    "            scaler_X_func   = scaler_X_func,\n",
    "            scaler_y_func   = scaler_y_func,\n",
    "            na=na,\n",
    "            nb_past=nb,\n",
    "            nf_future=nf,\n",
    "            test_name_col='test_name',\n",
    "            y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "    )\n",
    "        \n",
    "    y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # prepare validation data\n",
    "    df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "    dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "            df = df_case_val,\n",
    "            input_cols  = input_cols,\n",
    "            output_cols   = output_cols,\n",
    "            scaler_X_func   = scaler_X_func,\n",
    "            scaler_y_func   = scaler_y_func,\n",
    "            na=na,\n",
    "            nb_past=nb,\n",
    "            nf_future=nf,\n",
    "            test_name_col='test_name',\n",
    "            y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "    )\n",
    "    print(f\"nf={nf}, nd={nd}, X_train shape: {X_train_selected_df.shape}, y shape: {y_train_target.shape}\")\n",
    "\n",
    "\n",
    "    # select eta only in input features\n",
    "    X_train_selected_df = select_input_features(\n",
    "        dfx_train.drop(columns='test_name'),\n",
    "        input_type=xcase,\n",
    "        keep_future=True,\n",
    "        keep_past=True\n",
    "    )\n",
    "\n",
    "    X_val_selected_df = select_input_features(\n",
    "        dfx_val.drop(columns='test_name'),\n",
    "        input_type=xcase,\n",
    "        keep_future=True,\n",
    "        keep_past=True\n",
    "    )\n",
    "\n",
    "    if nf == 0:\n",
    "        nd_list = [0, 1, 2, 3, 4, 5 , 6, 7, 8]  # Include all lags from 0 to 8\n",
    "    else:\n",
    "        nd_list = [0] \n",
    "        \n",
    "    for nd in nd_list:\n",
    "        if nd > 0:\n",
    "        # Filter out eta and its past lags based on nd\n",
    "            X_train_selected_df = X_train_selected_df.loc[:, [ col for col in X_train_selected_df.columns\n",
    "                if not (col == 'eta' and nd > 0) and not (col.startswith('eta_past_') and int(col.split('_')[-1]) < nd)\n",
    "            ]]\n",
    "            X_val_selected_df = X_val_selected_df.loc[:, [ col for col in X_val_selected_df.columns\n",
    "                if not (col == 'eta' and nd > 0) and not (col.startswith('eta_past_') and int(col.split('_')[-1]) < nd)\n",
    "            ]]           \n",
    "        # Model parameters\n",
    "        model_name = F'LinearRegression_' + xcase + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf) + '_nd' + str(nd) + '_degree' + str(degree) + '.joblib'\n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "        if X_train_selected_df.empty or X_train_selected_df.shape[1] == 0:\n",
    "            print(f\"[SKIP] No input features left for nf={nf}, nd={nd}\")\n",
    "            continue\n",
    "\n",
    "        # Train the model\n",
    "        #start_time = time.perf_counter()\n",
    "        model.fit(X_train_selected_df  ,y_train_target  )\n",
    "        #train_time = time.perf_counter() - start_time\n",
    "        #memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        print(f'-----predicting on training and validation data----')\n",
    "        # Predict on train and validation data\n",
    "        #start_time = time.perf_counter()\n",
    "        y_pred_train_scaled = model.predict(X_train_selected_df)\n",
    "        y_pred_val_scaled = model.predict(X_val_selected_df)\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "        #predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "        \n",
    "        y_true_val = scaler_y.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "        \n",
    "\n",
    "        \n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'nd' : nd,\n",
    "        'xcase': xcase,\n",
    "        'test case': case,\n",
    "    }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'degree': degree,\n",
    "        'xcase': xcase,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "        \n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_nd = pd.concat([metrics_df_nd, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        #perf_df = pd.concat([perf_df, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2391aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_nd.loc[metrics_df_nd['nf'] > 0, 'nd'] = -metrics_df_nd['nf']\n",
    "\n",
    "metrics_df_nd['nd'].unique()  # Ensure nd is treated as a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb657756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "# Prepare the data\n",
    "plot_df = metrics_df_nd[['nd', 'r2_train_heave', 'r2_train_pitch', 'r2_train_pendulum']].copy()\n",
    "plot_df = plot_df.sort_values('nd')\n",
    "plot_df = plot_df[plot_df['nd'] != 8]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot R² lines\n",
    "ax.plot(plot_df['nd'], plot_df['r2_train_heave'], marker='o', label='Heave')\n",
    "ax.plot(plot_df['nd'], plot_df['r2_train_pitch'], marker='s', label='Pitch')\n",
    "ax.plot(plot_df['nd'], plot_df['r2_train_pendulum'], marker='^', label='Pendulum')\n",
    "\n",
    "# Move y-axis to x=0\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.yaxis.tick_left()\n",
    "\n",
    "# Place y-axis label at the top\n",
    "ax.set_ylabel('')  # remove it\n",
    "\n",
    "# Manually add y-axis label at x=0\n",
    "ax.text(\n",
    "    0, 1.02, 'R²', transform=ax.transData,\n",
    "    ha='right', va='bottom', fontsize=13\n",
    ")\n",
    "# Move y-axis to x = 0\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.yaxis.tick_left()\n",
    "# Clean up right and top spines\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "\n",
    "# Set X ticks every 1 unit\n",
    "min_nd = plot_df['nd'].min()\n",
    "max_nd = plot_df['nd'].max()\n",
    "xticks = np.arange(min_nd, max_nd + 1, 1)\n",
    "\n",
    "ax.set_xticks(xticks)\n",
    "\n",
    "\n",
    "# Set Y limits and grid\n",
    "ax.set_ylim(0, 1)\n",
    "yticks = ax.get_yticks()\n",
    "yticks = [y for y in yticks if y != 0]\n",
    "ax.set_yticks(yticks)\n",
    "\n",
    "\n",
    "ax.set_xlabel('$n_d$')\n",
    "ax.legend()\n",
    "\n",
    "# Title with more spacing\n",
    "ax.set_title('Effect of $n_d$ on Training R² — $n_b$ = 4, $n_a$ = 0', pad=50)\n",
    "\n",
    "\n",
    "# Enable both vertical and horizontal gridlines at tick locations\n",
    "ax.grid(False, which='both', axis='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0b5b1",
   "metadata": {},
   "source": [
    "# Checking model in parallel configration For best Input Fearures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d068380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'ARX/parralel_config'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d905a",
   "metadata": {},
   "source": [
    "Adjust output_col , xcase, and scaler functions according to the desired case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83288dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_max=[0,1,2,3,4,5,6,7]\n",
    "nf_max=[0,1,2,3,4,5,6,7]\n",
    "degree=1\n",
    "na=2\n",
    "case='Tp6p8s_Hs2m'\n",
    "output_cols = ['pendulum'] \n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc063bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_ARX_pendulum_only_parralel_eta_only = pd.DataFrame()\n",
    "\n",
    "\n",
    "xcase='eta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4afb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb in nb_max:\n",
    "    for nf in nf_max:\n",
    "        \n",
    "\n",
    "        # Create the pipeline model\n",
    "        model = Pipeline([\n",
    "                ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "        # get  df at true scale witAh only this test case\n",
    "        df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_nov,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # prepare validation data\n",
    "        df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_nov,\n",
    "                scaler_y_func   = scaler_y_func_pend,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "\n",
    "\n",
    "        # select eta only in input features\n",
    "        X_train_selected_df = select_input_features(\n",
    "            dfx_train.drop(columns='test_name'),\n",
    "            input_type=xcase,\n",
    "            keep_future=True,\n",
    "            keep_past=True\n",
    "        )\n",
    "\n",
    "        X_val_selected_df = select_input_features(\n",
    "            dfx_val.drop(columns='test_name'),\n",
    "            input_type=xcase,\n",
    "            keep_future=True,\n",
    "            keep_past=True\n",
    "        )\n",
    "\n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train_selected_df  ,y_train_target  )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled=model.predict(X_train_selected_df)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled=model.predict(X_val_selected_df) \n",
    "\n",
    "\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_pend.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_pend.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'xcase': xcase,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'degree': degree,\n",
    "        'xcase': xcase,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_df_ARX_pendulum_only_parralel_eta_only  = pd.concat([metrics_df_ARX_pendulum_only_parralel_eta_only, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_pendulum_only_parralel_eta_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_pendulum_only_parralel_eta_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_ARX_pendulum_only_parralel_eta_only to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cef485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_values = ['pendulum']\n",
    "df_plot = metrics_df_ARX_pendulum_only_parralel_eta_only.copy()\n",
    "na_values = sorted(df_plot['na'].unique())\n",
    "\n",
    "for target_var in target_values:\n",
    "    for na_val in na_values:\n",
    "        df_na = df_plot[df_plot['na'] == na_val].copy()\n",
    "\n",
    "        unique_nf = sorted(df_na['nf'].unique())\n",
    "        unique_nb = sorted(df_na['nb'].unique())\n",
    "\n",
    "        # Map nf → color using Plotly palette\n",
    "        color_map = {nf: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "                     for i, nf in enumerate(unique_nf)}\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=[f\"R² Train (na={na_val})\", f\"R² Val (na={na_val})\"],\n",
    "            shared_yaxes=False\n",
    "        )\n",
    "\n",
    "        for nf_val in unique_nf:\n",
    "            df_nf = df_na[df_na['nf'] == nf_val].copy().sort_values(by='nb')\n",
    "            color = color_map[nf_val]\n",
    "            nd_label = f\"nd={-nf_val}\"\n",
    "\n",
    "            # Train plot\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nf['nb'],\n",
    "                    y=df_nf[f'r2_train_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    line=dict(color=color, width=2),\n",
    "                    marker=dict(size=8, symbol='circle'),\n",
    "                    name=nd_label,\n",
    "                    legendgroup=nd_label\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            # Val plot\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_nf['nb'],\n",
    "                    y=df_nf[f'r2_val_{target_var}'],\n",
    "                    mode='lines+markers',\n",
    "                    line=dict(color=color, width=2),\n",
    "                    marker=dict(size=8, symbol='circle'),\n",
    "                    name=nd_label,\n",
    "                    legendgroup=nd_label,\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        #η, η̇  and η̈ \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "            text = rf\"R² Performance of Per-DOF Models Using η only as an Input Feature<br>({target_var})\",\n",
    "            x=0.5,\n",
    "            xanchor='center'\n",
    "                        ),\n",
    "            xaxis_title=\"nb (Input Lags)\",\n",
    "            xaxis2_title=\"nb (Input Lags)\",\n",
    "            yaxis_title=\"R² Score\",\n",
    "            template=\"plotly_white\",\n",
    "            height=450,\n",
    "            width=950,\n",
    "            legend_title=\"nd\"\n",
    "        )\n",
    "        #fig.update_yaxes(range=[0.9998, 1], row=1, col=1)\n",
    "        #fig.update_yaxes(range=[-100, 10], row=1, col=2)\n",
    "\n",
    "        output_dir = \"Annex/figures/ARX_pendulum_eta_only\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Create a clean filename from target and na\n",
    "        safe_target = target_var.replace(\" \", \"_\")\n",
    "        filename = f\"R2_eta_only_na{na_val}_target_{safe_target}_ver2.png\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        fig.write_image(save_path, scale=2)  # high-res PNG\n",
    "\n",
    "\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62feacb0",
   "metadata": {},
   "source": [
    "# 1 shared model for all DoFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab63326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'ARX/model_output'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_max=[0,1,2,3,4,5,6,7]\n",
    "nf_max=[0,1,2,3,4,5,6,7]\n",
    "degree=1\n",
    "na=2\n",
    "case='Tp6p8s_Hs2m'\n",
    "output_cols = ['heave','pitch','pendulum'] \n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "xcase='eta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_3dof_arx_series_eta_only = pd.DataFrame()\n",
    "perf_3dof_arx_series_eta_only=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb in nb_max:\n",
    "    for nf in nf_max:\n",
    "        \n",
    "         ##\n",
    "        # Model name\n",
    "        model_name = F'ARX_3dof_' + xcase + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "        # Create the pipeline model\n",
    "        model = Pipeline([\n",
    "                ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "        # get  df at true scale witAh only this test case\n",
    "        df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_nov,\n",
    "                scaler_y_func   = scaler_y_func,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # prepare validation data\n",
    "        df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_nov,\n",
    "                scaler_y_func   = scaler_y_func ,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "\n",
    "\n",
    "        # select eta only in input features\n",
    "        X_train_selected_df = select_input_features(\n",
    "            dfx_train.drop(columns='test_name'),\n",
    "            input_type=xcase,\n",
    "            keep_future=True,\n",
    "            keep_past=True\n",
    "        )\n",
    "\n",
    "        X_val_selected_df = select_input_features(\n",
    "            dfx_val.drop(columns='test_name'),\n",
    "            input_type=xcase,\n",
    "            keep_future=True,\n",
    "            keep_past=True\n",
    "        )\n",
    "\n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train_selected_df  ,y_train_target  )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train_selected_df,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_val_selected_df,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'xcase': xcase,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'degree': degree,\n",
    "        'xcase': xcase,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        metrics_3dof_arx_series_eta_only  = pd.concat([metrics_3dof_arx_series_eta_only, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        perf_3dof_arx_series_eta_only = pd.concat([perf_3dof_arx_series_eta_only, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "       \n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_3dof_arx_series_eta_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_3dof_arx_series_eta_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_3dof_arx_series_eta_only to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_3dof_arx_series_eta_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_3dof_arx_series_eta_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_3dof_arx_series_eta_only to: {file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b97303",
   "metadata": {},
   "source": [
    "## Comparing parrale configration vs series configration predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y_3dof = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit on training data (DataFrames, not NumPy arrays)\n",
    "\n",
    "scaler_y_3dof.fit(df_train_full[output_cols])\n",
    "\n",
    "# define scalling function\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "\n",
    "scaler_y_func_3dpf = lambda df: scaler_y_3dof.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "na  = 2\n",
    "nb=2\n",
    "nf=5\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols=['heave', 'pitch',  'pendulum']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce13cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "case='Tp6p8s_Hs2m'\n",
    "model_name = F'ARX_' + xcase + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "# Create the pipeline model\n",
    "model = Pipeline([\n",
    "        ('polynomial', PolynomialFeatures(degree=1, interaction_only=False)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "# prepare training data \n",
    "print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "# get  df at true scale witAh only this test case\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func,\n",
    "        scaler_y_func   = scaler_y_func_3dpf,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "print(f'-----predicting on training data----')\n",
    "\n",
    "# get feature names\n",
    "X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "print(f'-----predicting on training and validation data----')\n",
    "# Predict on train and validation data\n",
    "y_pred_parralel_scaled = model.predict(X_train_selected_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa46215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_series_scaled,x_used_train = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_train_selected_df.iloc[:10000],\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ce079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_row={}\n",
    "# Inverse transform to original scale\n",
    "y_pred_parralel = scaler_y_3dof.inverse_transform(y_pred_parralel_scaled)\n",
    "y_pred_series = scaler_y_3dof.inverse_transform(y_pred_series_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# Convert predictions to DataFrames for easier handling\n",
    "y_pred_parralel_df = pd.DataFrame(y_pred_parralel, columns=output_cols)\n",
    "y_pred_series_df = pd.DataFrame(y_pred_series, columns=output_cols)\n",
    "\n",
    "\n",
    "# Get true values aligned with dfy_train and dfy_val indexes\n",
    "y_true_train =scaler_y_3dof.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in output_cols:\n",
    "    mse_parralel = mean_squared_error(y_true_train_df[col], y_pred_parralel_df[col])\n",
    "    r2_parralel = r2_score(y_true_train_df[col], y_pred_parralel_df[col])\n",
    "\n",
    "    mse_series = mean_squared_error(y_true_train_df[col].iloc[:10000], y_pred_series_df[col])\n",
    "    r2_series = r2_score(y_true_train_df[col].iloc[:10000], y_pred_series_df[col])\n",
    "\n",
    "    # Add to metrics row\n",
    "    metrics_row[f'r2_parralel_{col}'] = r2_parralel\n",
    "    metrics_row[f'mse_parralel_{col}'] = mse_parralel\n",
    "    metrics_row[f'r2_series_{col}'] = r2_series\n",
    "    metrics_row[f'mse_series_{col}'] = mse_series\n",
    "\n",
    "print(\"metrics_row:\", metrics_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 10000\n",
    "time_steps = np.arange(n_points)  # time steps: 0, 1, ..., 9999\n",
    "\n",
    "for col in output_cols:\n",
    "    if 'heave' in col.lower():\n",
    "        y_label = f\"{col.capitalize()} [m]\"\n",
    "     \n",
    "         \n",
    "    elif 'pitch' in col.lower() or 'pendulum' in col.lower():\n",
    "        y_label = f\"{col.capitalize()} [°]\"\n",
    "    else:\n",
    "        y_label = col.capitalize()\n",
    "        \n",
    "    fig = go.Figure()\n",
    "\n",
    "    # True signal (solid blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=y_true_train_df[col].iloc[:n_points],\n",
    "        mode='lines',\n",
    "        name='True (Train)',\n",
    "        line=dict(color='blue', dash='solid')\n",
    "    ))\n",
    "\n",
    "    # Series prediction (dashed red)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=y_pred_series_df[col].iloc[:n_points],\n",
    "        mode='lines',\n",
    "        name='Predicted (Series)',\n",
    "        line=dict(color='black', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # Parallel prediction (dotted black)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_steps,\n",
    "        y=y_pred_parralel_df[col].iloc[:n_points],\n",
    "        mode='lines',\n",
    "        name='Predicted (Parallel)',\n",
    "        line=dict(color='red', dash='dot')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Train Predictions for {col} - na={na}, nb={nb}, nd={-nf}\",\n",
    "        xaxis_title=\"Time Step\",\n",
    "        yaxis_title=y_label,\n",
    "        template=\"plotly_white\",\n",
    "        width=1000,\n",
    "        height=400,\n",
    "        yaxis=dict(range=[-3, 3]),\n",
    "\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1)\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_parallel.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_parallel.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_df_ARX_parallel.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_df_ARX_parallel.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_df_ARX_parallel to: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a74d6",
   "metadata": {},
   "source": [
    "#  1 Model For Heave Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cf3ba",
   "metadata": {},
   "source": [
    "## Finding Optimal Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_ARX_heave_only_na2 = pd.DataFrame()\n",
    "perf_df_ARX_series_heave_only_na2 = pd.DataFrame()\n",
    "\n",
    "metrics_df_ARX_heave_only  =pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f73334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Base save folder\n",
    "save_folder = 'ARX/saved_models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values \n",
    "na_max  = 2\n",
    "nb_max= [2]\n",
    "nf_max=[7]\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7962415",
   "metadata": {},
   "outputs": [],
   "source": [
    "case='Tp6p8s_Hs2m'\n",
    "output_cols = ['heave']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "\n",
    "\n",
    "for nb in nb_max:\n",
    "    for nf in nf_max:\n",
    "        \n",
    "\n",
    "        ##\n",
    "        # Model parameters\n",
    "        model_name = F'ARX_heave_' + xcase + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "        # Create the pipeline model\n",
    "        model = Pipeline([\n",
    "                ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "            \n",
    "        # prepare training data \n",
    "        print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "        # get  df at true scale witAh only this test case\n",
    "        df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                df = df_case_train,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_nov,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "            \n",
    "        y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # prepare validation data\n",
    "        df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "        dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                df = df_case_val,\n",
    "                input_cols  = input_cols,\n",
    "                output_cols   = output_cols,\n",
    "                scaler_X_func   = scaler_X_func_nov,\n",
    "                scaler_y_func   = scaler_y_func_heave,\n",
    "                na=na,\n",
    "                nb_past=nb,\n",
    "                nf_future=nf,\n",
    "                test_name_col='test_name',\n",
    "                y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "        )\n",
    "\n",
    "\n",
    "        # select eta only in input features\n",
    "        X_train_selected_df = select_input_features(\n",
    "            dfx_train.drop(columns='test_name'),\n",
    "            input_type=xcase,\n",
    "            keep_future=True,\n",
    "            keep_past=True\n",
    "        )\n",
    "\n",
    "        X_val_selected_df = select_input_features(\n",
    "            dfx_val.drop(columns='test_name'),\n",
    "            input_type=xcase,\n",
    "            keep_future=True,\n",
    "            keep_past=True\n",
    "        )\n",
    "\n",
    "        # ============================\n",
    "        # MEASURE CPU & MEMORY USAGE\n",
    "        # ============================\n",
    "        print(f'-----Training model----')\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Train the model\n",
    "        start_time = time.perf_counter()\n",
    "        model.fit(X_train_selected_df  ,y_train_target  )\n",
    "        train_time = time.perf_counter() - start_time\n",
    "        memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "        # get feature names\n",
    "        X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "        print(f'-----predicting on training data----')\n",
    "\n",
    "        # Predict on train and validation data\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_train_selected_df,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "        print(f'-----predicting on validation data----')\n",
    "        \n",
    "        y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_val_selected_df,\n",
    "        output_cols=output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=na\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "        y_pred_val = scaler_y_heave.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "        predict_time = time.perf_counter() - start_time\n",
    "        memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "        # Convert predictions to DataFrames for easier handling\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "        y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "        # Get true values aligned with dfy_train and dfy_val indexes\n",
    "        y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "        y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "        y_true_val = scaler_y_heave.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "        y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "        # save model info to dfs\n",
    "        metrics_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'xcase': xcase,\n",
    "        'test case': case,\n",
    "        }\n",
    "\n",
    "        perf_row = {\n",
    "        'model_name': model_name,\n",
    "        'na': na,\n",
    "        'nb': nb,\n",
    "        'nf': nf,\n",
    "        'degree': degree,\n",
    "        'xcase': xcase,\n",
    "        'train_time': train_time,\n",
    "        'train_memory_MB': memory_usage_train,\n",
    "        'predict_time': predict_time,\n",
    "        'predict_memory_MB': memory_usage_predict\n",
    "            }\n",
    "        print(f'-----Eavluating model----')\n",
    "        # Compute metrics\n",
    "        if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "            print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "        for col in output_cols:\n",
    "            mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "            r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "            mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "            r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "            # Add to metrics row\n",
    "            metrics_row[f'r2_train_{col}'] = r2_train\n",
    "            metrics_row[f'mse_train_{col}'] = mse_train\n",
    "            metrics_row[f'r2_val_{col}'] = r2_val\n",
    "            metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "        print(\"metrics_row:\", metrics_row)\n",
    "        print(\"perf_row:\", perf_row)\n",
    "        # Append the row dictionaries as new rows in the DataFrames\n",
    "        #metrics_df_ARX_heave_only  = pd.concat([metrics_df_ARX_heave_only, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "        #perf_df_ARX_series_heave_only_na2 = pd.concat([perf_df_ARX_series_heave_only_na2, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "        print(f'-----Saving model----')\n",
    "        # Save the trained model\n",
    "        model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "        model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "        # Save with joblib\n",
    "        joblib.dump(model, model_save_path) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_heave_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_heave_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_ARX_heave_only to: {file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved metrics\n",
    "metrics_df_ARX_heave_only = pd.read_csv(\"ARX\\metrics_outpus\\heave\\metrics_df_ARX_heave_only_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e8cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set target variable and work with the heave DataFrame\n",
    "target_var = 'heave'\n",
    "df_plot = metrics_df_ARX_heave_only[metrics_df_ARX_heave_only['na'] == 2 ].copy()\n",
    "\n",
    "# Loop over each unique na value\n",
    "na_values = sorted(df_plot['na'].unique())\n",
    "\n",
    "for na_val in na_values:\n",
    "    df_na = df_plot[df_plot['na'] == na_val].copy()\n",
    "    \n",
    "    # Unique nb values and create a color map for them\n",
    "    unique_nb = sorted(df_na['nb'].unique())\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "                 for i, nb in enumerate(unique_nb)}\n",
    "    \n",
    "    # Create a figure with two subplots (Train and Validation), sharing the y-axis\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[f\"R² Train (na={na_val})\", f\"R² Val (na={na_val})\"],\n",
    "        shared_yaxes=False\n",
    "    )\n",
    "    \n",
    "    # Loop through each unique nb, plotting curves vs nf (keep x values as nf)\n",
    "    for nb_val in unique_nb:\n",
    "        df_nb = df_na[df_na['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "        color = color_map[nb_val]\n",
    "        \n",
    "        # Check if nb is the one to highlight\n",
    "        if nb_val == 2:\n",
    "            line_width = 4    # Thicker line for highlighting\n",
    "            marker_size = 12  # Larger markers for highlighting\n",
    "            marker_symbol = 'star'\n",
    "        else:\n",
    "            line_width = 2\n",
    "            marker_size = 8\n",
    "            marker_symbol = 'circle'\n",
    "            \n",
    "       # Add train R² trace with original nf as x values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_nb['nf'],\n",
    "                y=df_nb[f'r2_train_{target_var}'],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(size=marker_size, symbol=marker_symbol),\n",
    "                line=dict(color=color, width=line_width),\n",
    "                name=f'nb={nb_val}',\n",
    "                legendgroup=f'nb={nb_val}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add validation R² trace with original nf as x values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_nb['nf'],\n",
    "                y=df_nb[f'r2_val_{target_var}'],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(size=marker_size, symbol=marker_symbol),\n",
    "                line=dict(color=color, width=line_width),\n",
    "                name=f'nb={nb_val}',\n",
    "                legendgroup=f'nb={nb_val}',\n",
    "                showlegend=False,  # legend only displayed in the first subplot\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Create custom tick labels: positions are the original nf values,\n",
    "    # but labels show as -nf (e.g. nf=5 -> \"-5\")\n",
    "    unique_nf = sorted(df_na['nf'].unique())\n",
    "    tick_vals = unique_nf  # positions remain as nf values\n",
    "    tick_text = [f\"-{nf}\" for nf in unique_nf]  # labels are negative versions\n",
    "\n",
    "    # Update x-axes for both subplots with custom ticks\n",
    "    fig.update_xaxes(tickmode='array', tickvals=tick_vals, ticktext=tick_text, row=1, col=1)\n",
    "    fig.update_xaxes(tickmode='array', tickvals=tick_vals, ticktext=tick_text, row=1, col=2)\n",
    "    \n",
    "    # Update overall figure layout with titles and axis labels\n",
    "    fig.update_layout(\n",
    "        title_text=f\" R² performance of Heave ARX Model for na = {na_val} \",\n",
    "        xaxis_title=\"nd\",\n",
    "        xaxis2_title=\"nd \",\n",
    "        yaxis_title=\"R² Score\",\n",
    "        template=\"plotly_white\",\n",
    "        height=400,\n",
    "        width=900,\n",
    "        legend_title=\"nb\",\n",
    "        yaxis1=dict(range=[0.7, 1.05]),\n",
    "        yaxis2=dict(range=[0.7, 1.05])\n",
    "    )\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c0240",
   "metadata": {},
   "source": [
    "Best Model: na=2 , nb=2 , nf= 7 (case:'Tp6p8s_Hs2m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bee021",
   "metadata": {},
   "source": [
    "## dt senstivety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83252ccf",
   "metadata": {},
   "source": [
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_heave_dt_senstivery_test=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "case='Tp6p8s_Hs2m'\n",
    "\n",
    "output_cols = ['heave']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=2\n",
    "nf=7\n",
    "degree=1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "\n",
    "dfx_train,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "\n",
    "dfx_test,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df_full = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df_full = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "   \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {dt} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_heave.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    \n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_heave_dt_senstivery_test = pd.concat([metrics_df_ARX_heave_dt_senstivery_test, row_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_heave_dt_senstivery_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/dt_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_heave_dt_senstivery_test-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_heave_dt_senstivery_test.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcf782",
   "metadata": {},
   "source": [
    "Trying parallel Configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_heave_dt_senstivery_test_parralel=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "case='Tp6p8s_Hs2m'\n",
    "\n",
    "output_cols = ['heave']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=2\n",
    "nf=7\n",
    "degree=1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "\n",
    "dfx_train,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "\n",
    "dfx_test,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df_full = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df_full = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "   \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {dt} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled = model.predict(X_train_selected_df)\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_val indexes\n",
    "    y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_heave.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    \n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_heave_dt_senstivery_test_parralel = pd.concat([metrics_df_ARX_heave_dt_senstivery_test_parralel, row_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_heave_dt_senstivery_test_parralel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/dt_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_heave_dt_senstivery_test_parralel-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_heave_dt_senstivery_test_parralel.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441475d",
   "metadata": {},
   "source": [
    "## Data Senstivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297f664",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad84a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght=np.array([0.002,0.004,0.008,0.01,0.02 ,0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9,1])*len(df_case_train)\n",
    "lenght=lenght.astype(int)\n",
    "\n",
    "output_cols = ['heave']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=2\n",
    "nf=7\n",
    "degree=1\n",
    "\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "\n",
    "    \n",
    "\n",
    "# prepare training data\n",
    "\n",
    "\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "\n",
    "\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_heave_data_test_parralel=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2286308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for l in lenght:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled=model.predict(X_train_selected_df[0:l])\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled =model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true testues aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_heave.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    \n",
    "    mse_train = mean_squared_error(y_true_train_df['heave'], y_pred_train_df['heave'])\n",
    "    r2_train = r2_score(y_true_train_df['heave'], y_pred_train_df['heave'])\n",
    "\n",
    "    mse_test = mean_squared_error(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "    r2_test = r2_score(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "\n",
    "    # Add to metrics row\n",
    "    metrics_row[f'r2_train_heave'] = r2_train\n",
    "    metrics_row[f'mse_train_heave'] = mse_train\n",
    "    metrics_row[f'r2_test_heave'] = r2_test\n",
    "    metrics_row[f'mse_test_heave'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_heave_data_test_parralel = pd.concat([metrics_df_ARX_heave_data_test_parralel, row_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/data_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_heave_data_test_parralel-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_heave_data_test_parralel.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e238c18",
   "metadata": {},
   "source": [
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_heave_data_test_series=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b118c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for l in lenght:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df[0:l],\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_heave.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_heave.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_heave.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    \n",
    "    mse_train = mean_squared_error(y_true_train_df['heave'], y_pred_train_df['heave'])\n",
    "    r2_train = r2_score(y_true_train_df['heave'], y_pred_train_df['heave'])\n",
    "\n",
    "    mse_test = mean_squared_error(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "    r2_test = r2_score(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "\n",
    "    # Add to metrics row\n",
    "    metrics_row[f'r2_train_heave'] = r2_train\n",
    "    metrics_row[f'mse_train_heave'] = mse_train\n",
    "    metrics_row[f'r2_test_heave'] = r2_test\n",
    "    metrics_row[f'mse_test_heave'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_heave_data_test_series = pd.concat([metrics_df_ARX_heave_data_test_series, row_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "    print(f'-----Saving model----')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/data_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_heave_data_test_series-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_heave_data_test_series.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ce54d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=2,\n",
    "        nb_past=2,\n",
    "        nf_future=6,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Create the pipeline model\n",
    "model = Pipeline([\n",
    "        ('polynomial', PolynomialFeatures(degree=1, interaction_only=False)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "\n",
    "\n",
    "model.fit(X_train_selected_df  ,y_train_target )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41054efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 2\n",
    "best_nf = 7\n",
    "case = 'Tp6p8s_Hs2m'  # test case\n",
    "best_degree = 1\n",
    "best_xcase = 'eta'\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['heave']\n",
    "# load the best model\n",
    "folder='ARX/saved_models'\n",
    "model_name = f\"ARX_heave_{best_xcase}_na{best_na}_nb{best_nb}_nf{best_nf}.joblib\"\n",
    "# Load the model\n",
    "model_path = os.path.join(folder, model_name)  \n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# print all test cases\n",
    "test_cases = df_test_full['test_name'].unique()\n",
    "print(\"All test cases:\")\n",
    "print(test_cases)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta']  # original inputs\n",
    "output_cols = ['heave']  # outputs\n",
    "\n",
    "# Construct feature names in the exact order used by the data pipeline\n",
    "feature_names = []\n",
    "\n",
    "# 1. Current input signals (no lag)\n",
    "feature_names += input_cols\n",
    "\n",
    "# 2. Output lags (autoregressive part)\n",
    "for lag in range(1, best_na + 1):\n",
    "    for out_col in output_cols:\n",
    "        feature_names.append(f\"{out_col}_lag_{lag}\")\n",
    "\n",
    "# 3. Past input lags\n",
    "for lag in range(1, best_nb + 1):\n",
    "    for in_col in input_cols:\n",
    "        feature_names.append(f\"{in_col}_past_{lag}\")\n",
    "\n",
    "# 4. Future input lags\n",
    "for lag in range(1, best_nf + 1):\n",
    "    for in_col in input_cols:\n",
    "        feature_names.append(f\"{in_col}_future_{lag}\")\n",
    "\n",
    "# View the final ordered feature list\n",
    "print(\"Feature names used in X_lagged_df:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4683de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = model.named_steps['polynomial']\n",
    "linear = model.named_steps['linear']\n",
    "\n",
    "# Use the names remembered from training\n",
    "input_names = poly.get_feature_names_out(poly.feature_names_in_)\n",
    "\n",
    "# Coefficients and intercept\n",
    "coefficients = linear.coef_.flatten()\n",
    "intercept = float(linear.intercept_)\n",
    "\n",
    "\n",
    "# Build equation\n",
    "equation_terms = [f\"{coef:.4f} * {name}\" for coef, name in zip(coefficients, input_names)]\n",
    "equation = f\"{intercept:.4f} + \" + \" + \".join(equation_terms)\n",
    "\n",
    "print(\"\\n📘 Model Equation:\")\n",
    "print(f\"y = {equation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49530a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_test_heave_na2_nb2_nf7_val = pd.DataFrame()  # Will collect all test metrics\n",
    "training_case='Tp6p8s_Hs2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=df_test_full['test_name'].unique()\n",
    "for case in cases:\n",
    "    # Filter the test data\n",
    "    df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "    # Prepare the test data\n",
    "    dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "        df=df_case_test,\n",
    "        input_cols=best_input_cols,\n",
    "        output_cols=best_output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=best_na,\n",
    "        nb_past=best_nb,\n",
    "        nf_future=best_nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "    )\n",
    "\n",
    "    # Select only the best input features\n",
    "    X_test_selected_df = select_input_features(\n",
    "        dfx_test.drop(columns='test_name'),\n",
    "        input_type=best_xcase,\n",
    "        keep_future=True,\n",
    "        keep_past=True\n",
    "    )\n",
    "\n",
    "    # get feature names from model\n",
    "    X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_test_selected_df,\n",
    "        output_cols=best_output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=best_na\n",
    "    )\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "    # Convert to DataFrame\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "    # Get true values aligned with dfy_test indexes\n",
    "    y_true_test = scaler_y_heave.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_test = {}\n",
    "    metrics_test[f'test case'] = case\n",
    "    if case == training_case:\n",
    "        metrics_test[f'Comments'] = 'case used for training'\n",
    "   \n",
    "    mse_test = mean_squared_error(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "    r2_test = r2_score(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "\n",
    "    \n",
    "    metrics_test[f'r2_test_heave'] = r2_test\n",
    "    metrics_test[f'mse_test_heave'] = mse_test\n",
    "    \n",
    "    print(\"metrics:\", metrics_test)\n",
    "    # Append the row dictionaries as new rows in the DataFrame\n",
    "    metrics_df_test_heave_na2_nb2_nf7_val = pd.concat([metrics_df_test_heave_na2_nb2_nf7_val, pd.DataFrame([metrics_test])], ignore_index=True)\n",
    "        # Save true and predicted DataFrames to CSV files\n",
    "    y_true_test_df.to_csv(f'Results/ARX/Testing/heave/y_true_test_heave_{case}.csv', index=False)\n",
    "    y_pred_test_df.to_csv(f'Results/ARX/Testing/heave/y_pred_test_heave_{case}.csv', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/best_model/metrics_outpus\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_test_heave_na2_nb2_nf7_val.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_test_heave_na2_nb2_nf7_val.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_test_heave_na2_nb2_nf7_val to: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fd9fd",
   "metadata": {},
   "source": [
    "## Noisy Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695089d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the best model and predict on the test data\n",
    "import os\n",
    "\n",
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 2\n",
    "best_nf = 7\n",
    "case = 'Tp6p8s_Hs2m'  # test case\n",
    "best_degree = 1\n",
    "best_xcase = 'eta'\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['heave']\n",
    "# load the best model\n",
    "folder='ARX/saved_models'\n",
    "model_name = f\"ARX_heave_{best_xcase}_na{best_na}_nb{best_nb}_nf{best_nf}.joblib\"\n",
    "# Load the model\n",
    "model_path = os.path.join(folder, model_name)  \n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Noisy data\n",
    "df_case_test_noisy=pd.read_csv('Results/df_case_test_noisy.csv')\n",
    "df_case_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the test data\n",
    "dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "    df=df_case_test_noisy,\n",
    "    input_cols=best_input_cols,\n",
    "    output_cols=best_output_cols,\n",
    "    scaler_X_func   = scaler_X_func_nov,\n",
    "    scaler_y_func   = scaler_y_func_heave,\n",
    "    na=best_na,\n",
    "    nb_past=best_nb,\n",
    "    nf_future=best_nf,\n",
    "    test_name_col='test_name',\n",
    "    y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "# Select only the best input features\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=best_xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "# get feature names from model\n",
    "X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=best_output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=best_na\n",
    ")\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_pred_test = scaler_y_heave.inverse_transform(y_pred_test_scaled)\n",
    "# Convert to DataFrame\n",
    "y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "# Get true values aligned with dfy_test indexes\n",
    "y_true_test = scaler_y_heave.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "# Compute metrics\n",
    "metrics_test = {}\n",
    "metrics_test[f'test case'] = case\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "r2_test = r2_score(y_true_test_df['heave'], y_pred_test_df['heave'])\n",
    "\n",
    "\n",
    "metrics_test[f'r2_test_heave'] = r2_test\n",
    "metrics_test[f'mse_test_heave'] = mse_test\n",
    "\n",
    "print(\"metrics:\", metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test_df.to_csv(f'Results/noisy test/ARX_heave_true.csv', index=False)\n",
    "y_pred_test_df.to_csv(f'Results/noisy test/ARX_heave_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb611a",
   "metadata": {},
   "source": [
    "# 1 model for pitch only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d5e74",
   "metadata": {},
   "source": [
    "## Finding Optimal Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_ARX_series_pitch_only = pd.DataFrame()\n",
    "perf_df_ARX_series_pitch_only = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa15a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base save folder\n",
    "save_folder = 'ARX/saved_models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values \n",
    "na_max  = [2]\n",
    "nb_max= [2]\n",
    "nf_max=[2]\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba477c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['pitch']\n",
    "case='Tp6p8s_Hs2m'\n",
    "for na in na_max:\n",
    "    for nb in nb_max:\n",
    "        for nf in nf_max:\n",
    "            \n",
    "    \n",
    "            ##\n",
    "            # Model parameters\n",
    "            model_name = F'ARX_pitch' + xcase + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf)\n",
    "\n",
    "\n",
    "            # Create the pipeline model\n",
    "            model = Pipeline([\n",
    "                    ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "                    ('linear', LinearRegression())\n",
    "                ])\n",
    "                \n",
    "            # prepare training data \n",
    "            print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "            # get  df at true scale witAh only this test case\n",
    "            df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "            dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                    df = df_case_train,\n",
    "                    input_cols  = input_cols,\n",
    "                    output_cols   = output_cols,\n",
    "                    scaler_X_func   = scaler_X_func_nov,\n",
    "                    scaler_y_func   = scaler_y_func_pitch,\n",
    "                    na=na,\n",
    "                    nb_past=nb,\n",
    "                    nf_future=nf,\n",
    "                    test_name_col='test_name',\n",
    "                    y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "            )\n",
    "                \n",
    "            y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "            # prepare validation data\n",
    "            df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "            dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                    df = df_case_val,\n",
    "                    input_cols  = input_cols,\n",
    "                    output_cols   = output_cols,\n",
    "                    scaler_X_func   = scaler_X_func_nov,\n",
    "                    scaler_y_func   = scaler_y_func_pitch,\n",
    "                    na=na,\n",
    "                    nb_past=nb,\n",
    "                    nf_future=nf,\n",
    "                    test_name_col='test_name',\n",
    "                    y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "            )\n",
    "\n",
    "\n",
    "            # select eta only in input features\n",
    "            X_train_selected_df = select_input_features(\n",
    "                dfx_train.drop(columns='test_name'),\n",
    "                input_type=xcase,\n",
    "                keep_future=True,\n",
    "                keep_past=True\n",
    "            )\n",
    "\n",
    "            X_val_selected_df = select_input_features(\n",
    "                dfx_val.drop(columns='test_name'),\n",
    "                input_type=xcase,\n",
    "                keep_future=True,\n",
    "                keep_past=True\n",
    "            )\n",
    "\n",
    "            # ============================\n",
    "            # MEASURE CPU & MEMORY USAGE\n",
    "            # ============================\n",
    "            print(f'-----Training model----')\n",
    "            process = psutil.Process()\n",
    "\n",
    "            # Train the model\n",
    "            start_time = time.perf_counter()\n",
    "            model.fit(X_train_selected_df  ,y_train_target  )\n",
    "            train_time = time.perf_counter() - start_time\n",
    "            memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "            # get feature names\n",
    "            X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "            print(f'-----predicting on training data----')\n",
    "\n",
    "            # Predict on train and validation data\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_train_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "                )\n",
    "\n",
    "            print(f'-----predicting on validation data----')\n",
    "            \n",
    "            y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_val_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            # Inverse transform to original scale\n",
    "            y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "            y_pred_val = scaler_y_pitch.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "            predict_time = time.perf_counter() - start_time\n",
    "            memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "            # Convert predictions to DataFrames for easier handling\n",
    "            y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "            y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "            # Get true values aligned with dfy_train and dfy_val indexes\n",
    "            y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "            y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "            y_true_val = scaler_y_pitch.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "            y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "            # save model info to dfs\n",
    "            metrics_row = {\n",
    "            'model_name': model_name,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'xcase': xcase,\n",
    "            'test case': case,\n",
    "            }\n",
    "\n",
    "            perf_row = {\n",
    "            'model_name': model_name,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'degree': degree,\n",
    "            'xcase': xcase,\n",
    "            'train_time': train_time,\n",
    "            'train_memory_MB': memory_usage_train,\n",
    "            'predict_time': predict_time,\n",
    "            'predict_memory_MB': memory_usage_predict\n",
    "                }\n",
    "            print(f'-----Eavluating model----')\n",
    "            # Compute metrics\n",
    "            if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "                print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "            for col in output_cols:\n",
    "                mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "                r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "                mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "                r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "                # Add to metrics row\n",
    "                metrics_row[f'r2_train_{col}'] = r2_train\n",
    "                metrics_row[f'mse_train_{col}'] = mse_train\n",
    "                metrics_row[f'r2_val_{col}'] = r2_val\n",
    "                metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "            print(\"metrics_row:\", metrics_row)\n",
    "            print(\"perf_row:\", perf_row)\n",
    "            # Append the row dictionaries as new rows in the DataFrames\n",
    "            metrics_df_ARX_series_pitch_only  = pd.concat([metrics_df_ARX_series_pitch_only, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "            #perf_df_ARX_series_pitch_only = pd.concat([perf_df_ARX_series_pitch_only, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "            print(f'-----Saving model----')\n",
    "            # Save the trained model\n",
    "            model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "            model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "            # Save with joblib\n",
    "            joblib.dump(model, model_save_path) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfda603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_series_pitch_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_series_pitch_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_ARX_series_pitch_only to: {file_path}\")\n",
    "\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"perf_df_ARX_series_pitch_only.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_df_ARX_series_pitch_only.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved perf_df_ARX_series_pitch_only to: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac07811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved metrics DataFrame\n",
    "metrics_df_ARX_series_pitch_only= pd.read_csv(r\"D:\\thesis\\Work-space\\ARX\\metrics_outpus\\pitch\\metrics_df_ARX_series_pitch_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set target variable and work with the pitch DataFrame\n",
    "target_var = 'pitch'\n",
    "df_plot = metrics_df_ARX_series_pitch_only[metrics_df_ARX_series_pitch_only['na'] == 2].copy()\n",
    "\n",
    "# Loop over each unique na value (here, only na=2)\n",
    "na_values = sorted(df_plot['na'].unique())\n",
    "\n",
    "for na_val in na_values:\n",
    "    df_na = df_plot[df_plot['na'] == na_val].copy()\n",
    "    \n",
    "    # Unique nb values and create a color map for them\n",
    "    unique_nb = sorted(df_na['nb'].unique())\n",
    "    color_map = {nb: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "                 for i, nb in enumerate(unique_nb)}\n",
    "    \n",
    "    # Create a figure with two subplots (Train and Validation), sharing the y-axis\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[f\"R² Train (na={na_val})\", f\"R² Val (na={na_val})\"],\n",
    "        shared_yaxes=False\n",
    "    )\n",
    "    \n",
    "    # Loop through each unique nb, plotting curves vs nf (x values remain nf)\n",
    "    for nb_val in unique_nb:\n",
    "        df_nb = df_na[df_na['nb'] == nb_val].copy().sort_values(by='nf')\n",
    "        color = color_map[nb_val]\n",
    "        \n",
    "        # Check if nb is the one to highlight\n",
    "        if nb_val == 2:\n",
    "            line_width = 4    # Thicker line for highlighting\n",
    "            marker_size = 12  # Larger markers for highlighting\n",
    "            marker_symbol = 'star'\n",
    "        else:\n",
    "            line_width = 2\n",
    "            marker_size = 8\n",
    "            marker_symbol = 'circle'\n",
    "        \n",
    "        # Add train R² trace with original nf as x values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_nb['nf'],\n",
    "                y=df_nb[f'r2_train_{target_var}'],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(size=marker_size, symbol=marker_symbol),\n",
    "                line=dict(color=color, width=line_width),\n",
    "                name=f'nb={nb_val}',\n",
    "                legendgroup=f'nb={nb_val}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add validation R² trace with original nf as x values\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_nb['nf'],\n",
    "                y=df_nb[f'r2_val_{target_var}'],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(size=marker_size, symbol=marker_symbol),\n",
    "                line=dict(color=color, width=line_width),\n",
    "                name=f'nb={nb_val}',\n",
    "                legendgroup=f'nb={nb_val}',\n",
    "                showlegend=False,  # legend only displayed in the first subplot\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Create custom tick labels: positions are the original nf values,\n",
    "    # but labels show as -nf (e.g. nf=5 -> \"-5\")\n",
    "    unique_nf = sorted(df_na['nf'].unique())\n",
    "    tick_vals = unique_nf  # positions remain as nf values\n",
    "    tick_text = [f\"-{nf}\" for nf in unique_nf]  # labels are negative versions\n",
    "\n",
    "    # Update x-axes for both subplots with custom ticks\n",
    "    fig.update_xaxes(tickmode='array', tickvals=tick_vals, ticktext=tick_text, row=1, col=1)\n",
    "    fig.update_xaxes(tickmode='array', tickvals=tick_vals, ticktext=tick_text, row=1, col=2)\n",
    "    \n",
    "    # Update overall figure layout with titles and axis labels\n",
    "    fig.update_layout(\n",
    "        title_text=f\"R² performance of Pitch ARX Model for na = {na_val}\",\n",
    "        xaxis_title=\"nd \",\n",
    "        xaxis2_title=\"nd \",\n",
    "        yaxis_title=\"R² Score\",\n",
    "        template=\"plotly_white\",\n",
    "        height=400,\n",
    "        width=900,\n",
    "        legend_title=\"nb \",\n",
    "        yaxis1=dict(range=[0.6, 1.05]),\n",
    "        yaxis2=dict(range=[0.6, 1.05]),\n",
    "    )\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5088427",
   "metadata": {},
   "source": [
    "Best Model at na 2 , nb 2 , nf 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1debee",
   "metadata": {},
   "source": [
    "## dt Senstivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_picth_dt_senstivery_test=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "case='Tp6p8s_Hs2m'\n",
    "\n",
    "output_cols = ['pitch']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=2\n",
    "nf=7\n",
    "degree=1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e97715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare training data\n",
    "\n",
    "dfx_train,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "\n",
    "dfx_test,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df_full = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df_full = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "    \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {dt} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pitch.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_picth_dt_senstivery_test = pd.concat([metrics_df_ARX_picth_dt_senstivery_test, row_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking unstable predictions\n",
    "# Assuming y_true_train_df and y_pred_train_df are pandas DataFrames\n",
    "for col in output_cols:\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    plt.plot(y_true_val_df.index, y_true_val_df[col], label='True', linestyle='-', color='b')\n",
    "    plt.plot(y_pred_val_df.index, y_pred_val_df[col], label='Predicted', linestyle='--', color='r')\n",
    "    plt.title(f'True vs Predicted for {col}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_picth_dt_senstivery_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus/dt_test\"\n",
    "\n",
    "# Base save folder\n",
    "save_folder = 'Results/dt_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_picth_dt_senstivery_test-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_picth_dt_senstivery_test.to_csv(file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2538cf8",
   "metadata": {},
   "source": [
    "parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6be4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_picth_dt_senstivery_test_parralel=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "case='Tp6p8s_Hs2m'\n",
    "\n",
    "output_cols = ['pitch']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=2\n",
    "nf=7\n",
    "degree=1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare training data\n",
    "\n",
    "dfx_train,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "\n",
    "dfx_test,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df_full = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df_full = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "    \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {dt} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    \n",
    "\n",
    "    y_pred_train_scaled= model.predict(X_train_selected_df)\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pitch.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_picth_dt_senstivery_test_parralel = pd.concat([metrics_df_ARX_picth_dt_senstivery_test_parralel, row_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_picth_dt_senstivery_test_parralel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus/dt_test\"\n",
    "\n",
    "# Base save folder\n",
    "save_folder = 'Results/dt_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_picth_dt_senstivery_test_parralel-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_picth_dt_senstivery_test_parralel.to_csv(file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c86b9b",
   "metadata": {},
   "source": [
    "## Data Senstivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef2d11",
   "metadata": {},
   "source": [
    "Parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcaf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght=np.array([0.002,0.004,0.008,0.01,0.02 ,0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9,1])*len(df_case_train)\n",
    "lenght=lenght.astype(int)\n",
    "\n",
    "output_cols = ['pitch']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=2\n",
    "nf=7\n",
    "\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eef8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare Testing data\n",
    "\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_picth_data_test_parralel=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96568765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for l in lenght:\n",
    "    \n",
    "        # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled=model.predict(X_train_selected_df[0:l])\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled = model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pitch.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_picth_data_test_parralel = pd.concat([metrics_df_ARX_picth_data_test_parralel, row_df], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/data_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_picth_data_test_parralel-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_picth_data_test_parralel.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f9d6c",
   "metadata": {},
   "source": [
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_picth_data_test_seiries=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lenght:\n",
    "    \n",
    "        # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df[0:l],\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pitch.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pitch.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pitch.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_picth_data_test_seiries = pd.concat([metrics_df_ARX_picth_data_test_seiries, row_df], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f10aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/data_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_picth_data_test_seiries-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_picth_data_test_seiries.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df9bca",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the best model and predict on the test data\n",
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 2\n",
    "best_nf = 7\n",
    "best_degree = 1\n",
    "best_xcase = 'eta'\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['pitch']\n",
    "# load the best model\n",
    "folder='ARX/choosen heros/pitch'\n",
    "model_name = f\"ARX_pitcheta_na2_nb2_nf7.joblib\"\n",
    "# Load the model\n",
    "model_path = os.path.join(folder, model_name)  \n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# print all test cases\n",
    "test_cases = df_test_full['test_name'].unique()\n",
    "print(\"All test cases:\")\n",
    "print(test_cases)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta']  # original inputs\n",
    "output_cols = ['pitch']  # outputs\n",
    "\n",
    "# Construct feature names in the exact order used by the data pipeline\n",
    "feature_names = []\n",
    "\n",
    "# 1. Current input signals (no lag)\n",
    "feature_names += input_cols\n",
    "\n",
    "# 2. Output lags (autoregressive part)\n",
    "for lag in range(1, best_na + 1):\n",
    "    for out_col in output_cols:\n",
    "        feature_names.append(f\"{out_col}_lag_{lag}\")\n",
    "\n",
    "# 3. Past input lags\n",
    "for lag in range(1, best_nb + 1):\n",
    "    for in_col in input_cols:\n",
    "        feature_names.append(f\"{in_col}_past_{lag}\")\n",
    "\n",
    "# 4. Future input lags\n",
    "for lag in range(1, best_nf + 1):\n",
    "    for in_col in input_cols:\n",
    "        feature_names.append(f\"{in_col}_future_{lag}\")\n",
    "\n",
    "# View the final ordered feature list\n",
    "print(\"Feature names used in X_lagged_df:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = model.named_steps['polynomial']\n",
    "linear = model.named_steps['linear']\n",
    "\n",
    "# Use the names remembered from training\n",
    "input_names = poly.get_feature_names_out(poly.feature_names_in_)\n",
    "\n",
    "# Coefficients and intercept\n",
    "coefficients = linear.coef_.flatten()\n",
    "intercept = float(linear.intercept_)\n",
    "\n",
    "\n",
    "# Build equation\n",
    "equation_terms = [f\"{coef:.4f} * {name}\" for coef, name in zip(coefficients, input_names)]\n",
    "equation = f\"{intercept:.4f} + \" + \" + \".join(equation_terms)\n",
    "\n",
    "print(\"\\n📘 Model Equation:\")\n",
    "print(f\"y = {equation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_test_pitch_nb2_nf7 = pd.DataFrame()  # Will collect all test metrics\n",
    "training_case='Tp6p8s_Hs2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=df_test_full['test_name'].unique()\n",
    "for case in cases:\n",
    "    # Filter the test data\n",
    "    df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "    # Prepare the test data\n",
    "    dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "        df=df_case_test,\n",
    "        input_cols=best_input_cols,\n",
    "        output_cols=best_output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=best_na,\n",
    "        nb_past=best_nb,\n",
    "        nf_future=best_nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "    )\n",
    "\n",
    "    # Select only the best input features\n",
    "    X_test_selected_df = select_input_features(\n",
    "        dfx_test.drop(columns='test_name'),\n",
    "        input_type=best_xcase,\n",
    "        keep_future=True,\n",
    "        keep_past=True\n",
    "    )\n",
    "\n",
    "    # get feature names from model\n",
    "    X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_test_selected_df,\n",
    "        output_cols=best_output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=best_na\n",
    "    )\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "    # Convert to DataFrame\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "    # Get true values aligned with dfy_test indexes\n",
    "    y_true_test = scaler_y_pitch.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_test = {}\n",
    "    metrics_test[f'test case'] = case\n",
    "    if case == training_case:\n",
    "        metrics_test[f'Comments'] = 'case used for training'\n",
    "   \n",
    "    for col in best_output_cols:\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        \n",
    "        metrics_test[f'r2_test_{col}'] = r2_test\n",
    "        metrics_test[f'mse_test_{col}'] = mse_test\n",
    "    \n",
    "    print(\"metrics:\", metrics_test)\n",
    "    # Append the row dictionaries as new rows in the DataFrame\n",
    "    metrics_df_test_pitch_nb2_nf7  = pd.concat([metrics_df_test_pitch_nb2_nf7, pd.DataFrame([metrics_test])], ignore_index=True)\n",
    "        # Save true and predicted DataFrames to CSV files\n",
    "    y_true_test_df.to_csv(f'Results/ARX/Testing/heave/y_true_test_pitch_{case}.csv', index=False)\n",
    "    y_pred_test_df.to_csv(f'Results/ARX/Testing/heave/y_pred_test_pitch_{case}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/best_model/metrics_outpus\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_test_pitch_nb2_nf7.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_test_pitch_nb2_nf7.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics_df_test_pitch_nb2_nf7 to: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8cd84",
   "metadata": {},
   "source": [
    "## Noisy data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a24593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the best model and predict on the test data\n",
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 2\n",
    "best_nf = 7\n",
    "best_degree = 1\n",
    "best_xcase = 'eta'\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['pitch']\n",
    "# load the best model\n",
    "folder='ARX/choosen heros/pitch'\n",
    "model_name = f\"ARX_pitcheta_na2_nb2_nf7_new.joblib\"\n",
    "# Load the model\n",
    "model_path = os.path.join(folder, model_name)  \n",
    "model = joblib.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Noisy data\n",
    "df_case_test_noisy=pd.read_csv('Results/df_case_test_noisy.csv')\n",
    "df_case_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c988e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peparing test data\n",
    "\n",
    "dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "    df=df_case_test_noisy,\n",
    "    input_cols=best_input_cols,\n",
    "    output_cols=best_output_cols,\n",
    "    scaler_X_func   = scaler_X_func_nov,\n",
    "    scaler_y_func   = scaler_y_func_pitch,\n",
    "    na=best_na,\n",
    "    nb_past=best_nb,\n",
    "    nf_future=best_nf,\n",
    "    test_name_col='test_name',\n",
    "    y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "# Select only the best input features\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=best_xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "# get feature names from model\n",
    "X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=best_output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=best_na\n",
    ")\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_pred_test = scaler_y_pitch.inverse_transform(y_pred_test_scaled)\n",
    "# Convert to DataFrame\n",
    "y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "# Get true values aligned with dfy_test indexes\n",
    "y_true_test = scaler_y_pitch.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "# Compute metrics\n",
    "metrics_test = {}\n",
    "metrics_test[f'test case'] = case\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_true_test_df['pitch'], y_pred_test_df['pitch'])\n",
    "r2_test = r2_score(y_true_test_df['pitch'], y_pred_test_df['pitch'])\n",
    "\n",
    "\n",
    "metrics_test[f'r2_test_pitch'] = r2_test\n",
    "metrics_test[f'mse_test_pitch'] = mse_test\n",
    "\n",
    "print(\"metrics:\", metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test_df.to_csv(f'Results/noisy test/ARX_pitch_true.csv', index=False)\n",
    "y_pred_test_df.to_csv(f'Results/noisy test/ARX_pitch_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b97f1",
   "metadata": {},
   "source": [
    "# 1 model for Pendulum only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a5e8a",
   "metadata": {},
   "source": [
    "## Finding Oprimal Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497cdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames before the loop\n",
    "metrics_df_ARX_pend_only_new_na2 = pd.DataFrame()\n",
    "perf_df_ARX_series_pend_only_new_na2 = pd.DataFrame()\n",
    "\n",
    "# Base save folder\n",
    "save_folder = 'ARX/saved_models/new'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loop values \n",
    "case='Tp6p8s_Hs2m'\n",
    "na_max  = [2]\n",
    "nb_max= [4]\n",
    "nf_max=[2]\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['pendulum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for na in na_max:\n",
    "    for nb in nb_max:\n",
    "        for nf in nf_max:\n",
    "            \n",
    "            # Model name\n",
    "            model_name = F'ARX_pend' + xcase + '_na' + str(na) + '_nb' + str(nb) + '_nf' + str(nf) + 'new'\n",
    "\n",
    "\n",
    "            # Create the pipeline model\n",
    "            model = Pipeline([\n",
    "                    ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "                    ('linear', LinearRegression())\n",
    "                ])\n",
    "                \n",
    "            # prepare training data \n",
    "            print(f'-----Preprocessing case of na={str(na)} ,nb={str(nb)} and nf={str(nf)} , with xcase: {xcase}----')\n",
    "            # get  df at true scale witAh only this test case\n",
    "            df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "            dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "                    df = df_case_train,\n",
    "                    input_cols  = input_cols,\n",
    "                    output_cols   = output_cols,\n",
    "                    scaler_X_func   = scaler_X_func_nov,\n",
    "                    scaler_y_func   = scaler_y_func_pend,\n",
    "                    na=na,\n",
    "                    nb_past=nb,\n",
    "                    nf_future=nf,\n",
    "                    test_name_col='test_name',\n",
    "                    y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "            )\n",
    "                \n",
    "            y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "            # prepare validation data\n",
    "            df_case_val=df_val_full[df_val_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "            dfx_val,dfy_val,yi_val = build_arx_lagged_with_scalers(\n",
    "                    df = df_case_val,\n",
    "                    input_cols  = input_cols,\n",
    "                    output_cols   = output_cols,\n",
    "                    scaler_X_func   = scaler_X_func_nov,\n",
    "                    scaler_y_func   = scaler_y_func_pend,\n",
    "                    na=na,\n",
    "                    nb_past=nb,\n",
    "                    nf_future=nf,\n",
    "                    test_name_col='test_name',\n",
    "                    y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "            )\n",
    "\n",
    "\n",
    "            # select eta only in input features\n",
    "            X_train_selected_df = select_input_features(\n",
    "                dfx_train.drop(columns='test_name'),\n",
    "                input_type=xcase,\n",
    "                keep_future=True,\n",
    "                keep_past=True\n",
    "            )\n",
    "\n",
    "            X_val_selected_df = select_input_features(\n",
    "                dfx_val.drop(columns='test_name'),\n",
    "                input_type=xcase,\n",
    "                keep_future=True,\n",
    "                keep_past=True\n",
    "            )\n",
    "\n",
    "            # ============================\n",
    "            # MEASURE CPU & MEMORY USAGE\n",
    "            # ============================\n",
    "            print(f'-----Training model----')\n",
    "            process = psutil.Process()\n",
    "\n",
    "            # Train the model\n",
    "            start_time = time.perf_counter()\n",
    "            model.fit(X_train_selected_df  ,y_train_target  )\n",
    "            train_time = time.perf_counter() - start_time\n",
    "            memory_usage_train = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "\n",
    "            # get feature names\n",
    "            X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "            print(f'-----predicting on training data----')\n",
    "\n",
    "            # Predict on train and validation data\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_train_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "                )\n",
    "\n",
    "            print(f'-----predicting on validation data----')\n",
    "            \n",
    "            y_pred_val_scaled , x_used_val=predict_recursive_series(\n",
    "            model=model,\n",
    "            X_df=X_val_selected_df,\n",
    "            output_cols=output_cols,\n",
    "            X_feature_names=X_feature_names,\n",
    "            na=na\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            # Inverse transform to original scale\n",
    "            y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "            y_pred_val = scaler_y_pend.inverse_transform(y_pred_val_scaled)\n",
    "\n",
    "\n",
    "            predict_time = time.perf_counter() - start_time\n",
    "            memory_usage_predict = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "            # Convert predictions to DataFrames for easier handling\n",
    "            y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "            y_pred_val_df = pd.DataFrame(y_pred_val, columns=output_cols)\n",
    "\n",
    "\n",
    "            # Get true values aligned with dfy_train and dfy_val indexes\n",
    "            y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "            y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "            y_true_val = scaler_y_pend.inverse_transform(dfy_val[output_cols].reset_index(drop=True))\n",
    "            y_true_val_df = pd.DataFrame(y_true_val, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "            # save model info to dfs\n",
    "            metrics_row = {\n",
    "            'model_name': model_name,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'xcase': xcase,\n",
    "            'test case': case,\n",
    "            }\n",
    "\n",
    "            perf_row = {\n",
    "            'model_name': model_name,\n",
    "            'na': na,\n",
    "            'nb': nb,\n",
    "            'nf': nf,\n",
    "            'degree': degree,\n",
    "            'xcase': xcase,\n",
    "            'train_time': train_time,\n",
    "            'train_memory_MB': memory_usage_train,\n",
    "            'predict_time': predict_time,\n",
    "            'predict_memory_MB': memory_usage_predict\n",
    "                }\n",
    "            print(f'-----Eavluating model----')\n",
    "            # Compute metrics\n",
    "            if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "                print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "            for col in output_cols:\n",
    "                mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "                r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "                mse_val = mean_squared_error(y_true_val_df[col], y_pred_val_df[col])\n",
    "                r2_val = r2_score(y_true_val_df[col], y_pred_val_df[col])\n",
    "\n",
    "                # Add to metrics row\n",
    "                metrics_row[f'r2_train_{col}'] = r2_train\n",
    "                metrics_row[f'mse_train_{col}'] = mse_train\n",
    "                metrics_row[f'r2_val_{col}'] = r2_val\n",
    "                metrics_row[f'mse_val_{col}'] = mse_val\n",
    "\n",
    "            print(\"metrics_row:\", metrics_row)\n",
    "            print(\"perf_row:\", perf_row)\n",
    "            # Append the row dictionaries as new rows in the DataFrames\n",
    "            metrics_df_ARX_pend_only_final  = pd.concat([metrics_df_ARX_pend_only_final, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "            perf_df_ARX_series_pend_only_new_na2 = pd.concat([perf_df_ARX_series_pend_only_new_na2, pd.DataFrame([perf_row])], ignore_index=True)\n",
    "            print(f'-----Saving model----')\n",
    "            # Save the trained model\n",
    "            model_save_name = f\"{model_name}.joblib\"  # You already have model_name variable!\n",
    "            model_save_path = os.path.join(save_folder, model_save_name)\n",
    "\n",
    "            # Save with joblib\n",
    "            joblib.dump(model, model_save_path) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "# Define your new folder path\n",
    "save_folder = \"ARX/metrics_outpus/pendulum\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_pend_only_final.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "perf_df_ARX_series_pend_only_new_na2.to_csv(file_path, index=False)\n",
    "print(f\"Saved metrics_df_ARX_pend_only_final to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67545eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved metrics\n",
    "metrics_df_ARX_pend_only= pd.read_csv(r\"D:\\thesis\\Work-space\\ARX\\metrics_outpus\\pendulum\\metrics_df_ARX_pend_only_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target output variable\n",
    "target_var = 'pendulum'\n",
    "df_plot = metrics_df_ARX_pend_only.copy()\n",
    "\n",
    "# Unique values\n",
    "na_values = sorted(df_plot['na'].unique())\n",
    "nf_values = sorted(df_plot['nf'].unique())\n",
    "\n",
    "# Color map to keep consistent colors for each nf\n",
    "color_map = {\n",
    "    nf: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "    for i, nf in enumerate(nf_values)\n",
    "}\n",
    "\n",
    "for na_val in na_values:\n",
    "    df_na = df_plot[df_plot['na'] == na_val].copy()\n",
    "\n",
    "    # Create subplot figure\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[f\"R² Train (na={na_val})\", f\"R² Val (na={na_val})\"],\n",
    "        shared_yaxes=False\n",
    "    )\n",
    "\n",
    "    for nf_val in nf_values:\n",
    "        df_nf = df_na[df_na['nf'] == nf_val].copy().sort_values(by='nb')\n",
    "        color = color_map[nf_val]\n",
    "\n",
    "        # Highlight nf=7\n",
    "        if nf_val == 2:\n",
    "            line_width = 4\n",
    "            marker_size = 12\n",
    "            marker_symbol = 'star'\n",
    "        else:\n",
    "            line_width = 2\n",
    "            marker_size = 8\n",
    "            marker_symbol = 'circle'\n",
    "\n",
    "        # --- Train R² ---\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_nf['nb'],\n",
    "                y=df_nf[f'r2_train_{target_var}'],\n",
    "                mode='lines+markers',\n",
    "                name=f'nd={-nf_val}',\n",
    "                legendgroup=f'nd={-nf_val}',\n",
    "                line=dict(color=color, width=line_width),\n",
    "                marker=dict(size=marker_size, symbol=marker_symbol)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # --- Val R² ---\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_nf['nb'],\n",
    "                y=df_nf[f'r2_val_{target_var}'],\n",
    "                mode='lines+markers',\n",
    "                name=f'nd={-nf_val}',\n",
    "                legendgroup=f'nd={-nf_val}',\n",
    "                showlegend=False,\n",
    "                line=dict(color=color, width=line_width),\n",
    "                marker=dict(size=marker_size, symbol=marker_symbol)\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # Layout settings\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"R² Performance for na = {na_val} ({target_var})\"\n",
    "        ),\n",
    "        xaxis_title=\"nb \",\n",
    "        xaxis2_title=\"nb \",\n",
    "        yaxis_title=\"R² Score\",\n",
    "        template=\"plotly_white\",\n",
    "        height=500,\n",
    "        width=1125,\n",
    "        legend_title=\"nd\",\n",
    "        yaxis1=dict(range=[0.5, 1]),\n",
    "        yaxis2=dict(range=[0.5, 1]),\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340bb37",
   "metadata": {},
   "source": [
    "Best Model is at na 2 , nb 4 ,nf 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18a647",
   "metadata": {},
   "source": [
    "## dt senstivety "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41366695",
   "metadata": {},
   "source": [
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf83291",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_pendulum_dt_senstivery_test=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "case='Tp6p8s_Hs2m'\n",
    "\n",
    "output_cols = ['pendulum']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=4\n",
    "nf=2\n",
    "degree=1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c27970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "# Model parameters\n",
    "\n",
    "# get  df at true scale witAh only this test case\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "dfx_train,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare Testing data\n",
    "\n",
    "dfx_test,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df_full = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df_full = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "    \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {dt} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pend.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_pendulum_dt_senstivery_test = pd.concat([metrics_df_ARX_pendulum_dt_senstivery_test, row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/dt_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_pendulum_dt_senstivery_test-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_pendulum_dt_senstivery_test.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622f0f2",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1210fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_pendulum_dt_senstivery_test_parralel=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[1,2,3,4,5,6]\n",
    "case='Tp6p8s_Hs2m'\n",
    "\n",
    "output_cols = ['pendulum']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=4\n",
    "nf=2\n",
    "degree=1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training data\n",
    "\n",
    "dfx_train,dfy_train_full,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target_full = dfy_train_full[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare Testing data\n",
    "\n",
    "dfx_test,dfy_test_full,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df_full = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df_full = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "    \n",
    "for step in steps:\n",
    "    \n",
    "    dt=0.05*step\n",
    "    \n",
    "    \n",
    "    # resample data\n",
    "    \n",
    "    X_train_selected_df= X_train_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    y_train_target= y_train_target_full.iloc[::step].reset_index(drop=True)\n",
    "    X_test_selected_df= X_test_selected_df_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_train= dfy_train_full.iloc[::step].reset_index(drop=True)\n",
    "    dfy_test= dfy_test_full.iloc[::step].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of dt {dt} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df  ,y_train_target )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled=model.predict(X_train_selected_df)\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled =model.predict(X_test_selected_df)        \n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pend.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'dt' : dt\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_pendulum_dt_senstivery_test_parralel = pd.concat([metrics_df_ARX_pendulum_dt_senstivery_test_parralel, row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_pendulum_dt_senstivery_test_parralel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/dt_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_pendulum_dt_senstivery_test_parralel.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_pendulum_dt_senstivery_test_parralel.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c001c21",
   "metadata": {},
   "source": [
    "## data Senstivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7df1f",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f61e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght=np.array([0.002,0.004,0.008,0.01,0.02 ,0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9,1])*len(df_case_train)\n",
    "lenght=lenght.astype(int)\n",
    "\n",
    "output_cols = ['pendulum']\n",
    "#for na in range(4,na_max+1):\n",
    "na=2\n",
    "nb=4\n",
    "nf=2\n",
    "\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training data\n",
    "\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_pendulum_data_test_parallel=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943974fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for l in lenght:\n",
    "    \n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    # prepare training data \n",
    "    print(f'-----Preprocessing case of lenght {str(l)} ----')\n",
    "    # ============================\n",
    "    # MEASURE CPU & MEMORY USAGE\n",
    "    # ============================\n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "\n",
    "    y_pred_train_scaled =model.predict(X_train_selected_df[0:l])\n",
    "    \n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled = model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pend.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_test_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_pendulum_data_test_parallel = pd.concat([metrics_df_ARX_pendulum_data_test_parallel, row_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa91313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/data_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_pendulum_data_test_parallel-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_pendulum_data_test_parallel.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bdb60",
   "metadata": {},
   "source": [
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ARX_pendulum_data_test_pseries=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for l in lenght:\n",
    "    \n",
    "\n",
    "\n",
    "    # Create the pipeline model\n",
    "    model = Pipeline([\n",
    "            ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "  \n",
    "    print(f'-----Training model----')\n",
    "    \n",
    "    model.fit(X_train_selected_df[0:l]  ,y_train_target[0:l] )\n",
    "    \n",
    "\n",
    "\n",
    "    # get feature names\n",
    "    X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "    print(f'-----predicting on training data----')\n",
    "\n",
    "    # Predict on train and Testing data\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_pred_train_scaled,x_used_train = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_train_selected_df[0:l],\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "    print(f'-----predicting on Testing data----')\n",
    "    \n",
    "    y_pred_test_scaled , x_used_test=predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_train = scaler_y_pend.inverse_transform(y_pred_train_scaled)\n",
    "    y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    y_pred_train_df = pd.DataFrame(y_pred_train, columns=output_cols)\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=output_cols)\n",
    "\n",
    "\n",
    "    # Get true values aligned with dfy_train and dfy_test indexes\n",
    "    y_true_train =scaler_y_pend.inverse_transform(dfy_train[output_cols][0:l].reset_index(drop=True))\n",
    "    y_true_train_df = pd.DataFrame(y_true_train, columns=output_cols)\n",
    "\n",
    "    y_true_test = scaler_y_pend.inverse_transform(dfy_test[output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=output_cols)\n",
    "\n",
    "\n",
    "\n",
    "    # save model info to dfs\n",
    "    metrics_row = {\n",
    "    'model_name': model_name,\n",
    "    'na': na,\n",
    "    'nb': nb,\n",
    "    'nf': nf,\n",
    "    'xcase': xcase,\n",
    "    'test case': case,\n",
    "    'lenght' : l\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f'-----Eavluating model----')\n",
    "    # Compute metrics\n",
    "    if np.any(np.isnan(y_pred_train_df)) or np.any(np.isinf(y_pred_train_df)):\n",
    "        print(\"[WARN] Train predictions have NaNs or infs.\")\n",
    "\n",
    "    for col in output_cols:\n",
    "        mse_train = mean_squared_error(y_true_train_df[col], y_pred_train_df[col])\n",
    "        r2_train = r2_score(y_true_train_df[col], y_pred_train_df[col])\n",
    "\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        # Add to metrics row\n",
    "        metrics_row[f'r2_train_{col}'] = r2_train\n",
    "        metrics_row[f'mse_train_{col}'] = mse_train\n",
    "        metrics_row[f'r2_test_{col}'] = r2_test\n",
    "        metrics_row[f'mse_tesr_{col}'] = mse_test\n",
    "\n",
    "    print(\"metrics_row:\", metrics_row)\n",
    "    # Append the row dictionaries as new rows in the DataFrames\n",
    "    # Make sure metrics_row is a dict, not a list of dicts\n",
    "    if isinstance(metrics_row, dict):\n",
    "        row_df = pd.DataFrame([metrics_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame(metrics_row)\n",
    "\n",
    "    metrics_df_ARX_pendulum_data_test_pseries = pd.concat([metrics_df_ARX_pendulum_data_test_pseries, row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b84a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base save folder\n",
    "save_folder = 'Results/data_tests'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "# Define the file name\n",
    "file_path = os.path.join(save_folder, \"metrics_df_ARX_pendulum_data_test_pseries-.csv\")\n",
    "\n",
    "# Save the DataFrame\n",
    "metrics_df_ARX_pendulum_data_test_pseries.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c101437",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the best model and predict on the test data\n",
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 4\n",
    "best_nf = 2\n",
    "best_degree = 1\n",
    "best_xcase = 'eta'\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['pendulum']\n",
    "# load the best model\n",
    "folder='ARX/saved_models/new'\n",
    "#folder = 'ARX/choosen heros/pendulum'\n",
    "model_name = f\"ARX_pend{best_xcase}_na{best_na}_nb{best_nb}_nf{best_nf}.joblib\"\n",
    "# Load the model\n",
    "model_path = os.path.join(folder, model_name)  \n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# print all test cases\n",
    "test_cases = df_test_full['test_name'].unique()\n",
    "print(\"All test cases:\")\n",
    "print(test_cases)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb527b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['eta']  # original inputs\n",
    "output_cols = ['pendulum']  # outputs\n",
    "\n",
    "# Construct feature names in the exact order used by the data pipeline\n",
    "feature_names = []\n",
    "\n",
    "# 1. Current input signals (no lag)\n",
    "feature_names += input_cols\n",
    "\n",
    "# 2. Output lags (autoregressive part)\n",
    "for lag in range(1, best_na + 1):\n",
    "    for out_col in output_cols:\n",
    "        feature_names.append(f\"{out_col}_lag_{lag}\")\n",
    "\n",
    "# 3. Past input lags\n",
    "for lag in range(1, best_nb + 1):\n",
    "    for in_col in input_cols:\n",
    "        feature_names.append(f\"{in_col}_past_{lag}\")\n",
    "\n",
    "# 4. Future input lags\n",
    "for lag in range(1, best_nf + 1):\n",
    "    for in_col in input_cols:\n",
    "        feature_names.append(f\"{in_col}_future_{lag}\")\n",
    "\n",
    "# View the final ordered feature list\n",
    "print(\"Feature names used in X_lagged_df:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = model.named_steps['polynomial']\n",
    "linear = model.named_steps['linear']\n",
    "\n",
    "# Use the names remembered from training\n",
    "input_names = poly.get_feature_names_out(poly.feature_names_in_)\n",
    "\n",
    "# Coefficients and intercept\n",
    "coefficients = linear.coef_.flatten()\n",
    "intercept = float(linear.intercept_)\n",
    "\n",
    "\n",
    "# Build equation\n",
    "equation_terms = [f\"{coef:.4f} * {name}\" for coef, name in zip(coefficients, input_names)]\n",
    "equation = f\"{intercept:.4f} + \" + \" + \".join(equation_terms)\n",
    "\n",
    "print(\"\\n📘 Model Equation:\")\n",
    "print(f\"y = {equation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_test_pendulum_na2_nb4_nf2_val=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=df_test_full['test_name'].unique()\n",
    "for case in cases:\n",
    "    # Filter the test data\n",
    "    df_case_test = df_test_full[df_test_full['test_name'] == case].reset_index(drop=True)\n",
    "    # Prepare the test data\n",
    "    dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "        df=df_case_test,\n",
    "        input_cols=best_input_cols,\n",
    "        output_cols=best_output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=best_na,\n",
    "        nb_past=best_nb,\n",
    "        nf_future=best_nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    "    )\n",
    "\n",
    "    # Select only the best input features\n",
    "    X_test_selected_df = select_input_features(\n",
    "        dfx_test.drop(columns='test_name'),\n",
    "        input_type=best_xcase,\n",
    "        keep_future=True,\n",
    "        keep_past=True\n",
    "    )\n",
    "\n",
    "    # get feature names from model\n",
    "    X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "        model=model,\n",
    "        X_df=X_test_selected_df,\n",
    "        output_cols=best_output_cols,\n",
    "        X_feature_names=X_feature_names,\n",
    "        na=best_na\n",
    "    )\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "    # Convert to DataFrame\n",
    "    y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "    # Get true values aligned with dfy_test indexes\n",
    "    y_true_test = scaler_y_pend.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "    y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_test = {}\n",
    "    metrics_test[f'test case'] = case\n",
    "    if case == training_case:\n",
    "        metrics_test[f'Comments'] = 'case used for training'\n",
    "   \n",
    "    for col in best_output_cols:\n",
    "        mse_test = mean_squared_error(y_true_test_df[col], y_pred_test_df[col])\n",
    "        r2_test = r2_score(y_true_test_df[col], y_pred_test_df[col])\n",
    "\n",
    "        \n",
    "        metrics_test[f'r2_test_{col}'] = r2_test\n",
    "        metrics_test[f'mse_test_{col}'] = mse_test\n",
    "    \n",
    "    print(\"metrics:\", metrics_test)\n",
    "    # Append the row dictionaries as new rows in the DataFrame\n",
    "    metrics_df_test_pendulum_na2_nb4_nf2_val = pd.concat([metrics_df_test_pendulum_na2_nb4_nf2_val, pd.DataFrame([metrics_test])], ignore_index=True)\n",
    "        # Save true and predicted DataFrames to CSV files\n",
    "    y_true_test_df.to_csv(f'Results/ARX/Testing/pendulum/y_true_test_pitch_{case}.csv', index=False)\n",
    "    y_pred_test_df.to_csv(f'Results/ARX/Testing/pendulum/y_pred_test_df{case}.csv', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the metrics DataFrame\n",
    "metrics_df_test_pendulum_na2_nb4_nf2_val.to_csv('ARX/choosen heros/metrics_df_test_pendulum_na2_nb4_nf2_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fe5e9",
   "metadata": {},
   "source": [
    "## Noisy data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ace30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the best model and predict on the test data\n",
    "# Define the best model parameters\n",
    "best_na = 2\n",
    "best_nb = 4\n",
    "best_nf = 2\n",
    "best_degree = 1\n",
    "best_xcase = 'eta'\n",
    "best_input_cols = ['eta', 'eta_velocity', 'eta_acceleration']\n",
    "best_output_cols = ['pendulum']\n",
    "# load the best model\n",
    "folder='ARX/saved_models/new'\n",
    "#folder = 'ARX/choosen heros/pendulum'\n",
    "model_name = f\"ARX_pend{best_xcase}_na{best_na}_nb{best_nb}_nf{best_nf}.joblib\"\n",
    "# Load the model\n",
    "model_path = os.path.join(folder, model_name)  \n",
    "model = joblib.load(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load noisy data\n",
    "df_case_test_noisy=pd.read_csv('Results/df_case_test_noisy.csv')\n",
    "df_case_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test data\n",
    "dfx_test, dfy_test, yi_test = build_arx_lagged_with_scalers(\n",
    "    df=df_case_test_noisy,\n",
    "    input_cols=best_input_cols,\n",
    "    output_cols=best_output_cols,\n",
    "    scaler_X_func   = scaler_X_func_nov,\n",
    "    scaler_y_func   = scaler_y_func_pend,\n",
    "    na=best_na,\n",
    "    nb_past=best_nb,\n",
    "    nf_future=best_nf,\n",
    "    test_name_col='test_name',\n",
    "    y_initial_mode='original'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "\n",
    "# Select only the best input features\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=best_xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "# get feature names from model\n",
    "X_feature_names = X_test_selected_df.columns.tolist()\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_test_scaled, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=best_output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=best_na\n",
    ")\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_pred_test = scaler_y_pend.inverse_transform(y_pred_test_scaled)\n",
    "# Convert to DataFrame\n",
    "y_pred_test_df = pd.DataFrame(y_pred_test, columns=best_output_cols)\n",
    "\n",
    "# Get true values aligned with dfy_test indexes\n",
    "y_true_test = scaler_y_pend.inverse_transform(dfy_test[best_output_cols].reset_index(drop=True))\n",
    "y_true_test_df = pd.DataFrame(y_true_test, columns=best_output_cols)\n",
    "\n",
    "# Compute metrics\n",
    "metrics_test = {}\n",
    "metrics_test[f'test case'] = case\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_true_test_df['pendulum'], y_pred_test_df['pendulum'])\n",
    "r2_test = r2_score(y_true_test_df['pendulum'], y_pred_test_df['pendulum'])\n",
    "\n",
    "\n",
    "metrics_test[f'r2_test_pendulum'] = r2_test\n",
    "metrics_test[f'mse_test_pendulum'] = mse_test\n",
    "\n",
    "print(\"metrics:\", metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb02706",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test_df.to_csv(f'Results/noisy test/ARX_pendulum_true.csv', index=False)\n",
    "y_pred_test_df.to_csv(f'Results/noisy test/ARX_pendulum_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fdeae",
   "metadata": {},
   "source": [
    "# Computation time and emisions caculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecarbon\n",
    "print(codecarbon.__version__)\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573599c0",
   "metadata": {},
   "source": [
    "We adjust the data sizes to be devisible by 64 (like in lstm case) for a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82740268",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "def adjust_for_batch_df(df, y):\n",
    "    # Calculate the number of rows that are divisible by the batch size\n",
    "    n = (df.shape[0] // batch_size) * batch_size\n",
    "    \n",
    "    # Adjust DataFrame and target variable\n",
    "    df_adjusted = df.iloc[:n]  # Select the rows to match the batch size\n",
    "    y_adjusted = y[:n]         # Adjust the target variable in the same way\n",
    "    \n",
    "    return df_adjusted, y_adjusted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840f90e",
   "metadata": {},
   "source": [
    "## Heave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best lag values \n",
    "na  = 2\n",
    "nb= 2\n",
    "nf=7\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['heave']\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "case='Tp6p8s_Hs2m'\n",
    "case_test='Tp6p8s_Hs1m'\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "df_case_test=df_train_full[df_train_full['test_name']==case_test].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "\n",
    "X_train_selected_df, y_train_target = adjust_for_batch_df(X_train_selected_df, y_train_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Create the pipeline model\n",
    "model = Pipeline([\n",
    "        ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "#fit the model\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_heave_train.csv\"    # Custom filename\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "# Define and train model\n",
    "model = Pipeline([\n",
    "    ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Testing data\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_heave,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_test_target = dfy_test[output_cols].reset_index(drop=True)\n",
    "# select eta only in input features\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "\n",
    "X_test_selected_df, y_test_target = adjust_for_batch_df(X_test_selected_df, y_test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# parralel prediction \n",
    "\n",
    "y_pred_heave_test=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_heave_parralel_pred.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "# parralel prediction \n",
    "y_pred_heave_test=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# series prediction \n",
    "\n",
    "# Run prediction process \n",
    "y_pred_test_series_case_compare_heave, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_heave_series.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "\n",
    "# series prediction \n",
    "\n",
    "# Run prediction process \n",
    "y_pred_test_series_case_compare_heave, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b3aab",
   "metadata": {},
   "source": [
    "## Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291384a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best lag values \n",
    "na  = 2\n",
    "nb= 2\n",
    "nf=7\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['pitch']\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "case='Tp6p8s_Hs2m'\n",
    "case_test='Tp6p8s_Hs1m'\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "df_case_test=df_train_full[df_train_full['test_name']==case_test].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8801369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_feature_names = X_train_selected_df.columns.tolist()\n",
    "X_train_selected_df, y_train_target = adjust_for_batch_df(X_train_selected_df, y_train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Create the pipeline model\n",
    "model = Pipeline([\n",
    "        ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "# fit the model\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_pitch_train.csv\"    # Custom filename\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "# Define and train your model\n",
    "model = Pipeline([\n",
    "    ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Testing data\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pitch,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_test_target = dfy_test[output_cols].reset_index(drop=True)\n",
    "# select eta only in input features\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_test_selected_df, y_test_target = adjust_for_batch_df(X_test_selected_df, y_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# parralel prediction \n",
    "\n",
    "y_pred_pitch_test=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_pitch_parralel_pred.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "# parralel prediction \n",
    "y_pred_pitch_test=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67167109",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# series prediction \n",
    "\n",
    "# Run prediction process \n",
    "y_pred_test_series_case_compare_heave, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f173d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_pitch_series.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "\n",
    "# series prediction \n",
    "\n",
    "# Run prediction process \n",
    "y_pred_test_series_case_compare_heave, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc97974",
   "metadata": {},
   "source": [
    "## pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define best lag values \n",
    "na  = 2\n",
    "nb= 4\n",
    "nf=2\n",
    "degree = 1\n",
    "xcase = 'eta'  # xcases\n",
    "input_cols=['eta' , 'eta_velocity', 'eta_acceleration']\n",
    "output_cols = ['pendulum']\n",
    "# Define the scaling functions (they apply the .transform)\n",
    "case='Tp6p8s_Hs2m'\n",
    "case_test='Tp6p8s_Hs1m'\n",
    "df_case_train=df_train_full[df_train_full['test_name']==case].reset_index(drop=True)\n",
    "df_case_test=df_train_full[df_train_full['test_name']==case_test].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "dfx_train,dfy_train,yi_train = build_arx_lagged_with_scalers(\n",
    "        df = df_case_train,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_train_target = dfy_train[output_cols].reset_index(drop=True)\n",
    "# select eta only in input features\n",
    "X_train_selected_df = select_input_features(\n",
    "    dfx_train.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "\n",
    "X_feature_names = X_train_selected_df.columns.tolist()\n",
    "\n",
    "X_train_selected_df, y_train_target = adjust_for_batch_df(X_train_selected_df, y_train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Create the pipeline model\n",
    "model = Pipeline([\n",
    "        ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "# fit the model\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_pendulum_train.csv\"    # Custom filename\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "# Define and train model\n",
    "model = Pipeline([\n",
    "    ('polynomial', PolynomialFeatures(degree=degree, interaction_only=False)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train_selected_df  ,y_train_target  )\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4596e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Testing data\n",
    "dfx_test,dfy_test,yi_test = build_arx_lagged_with_scalers(\n",
    "        df = df_case_test,\n",
    "        input_cols  = input_cols,\n",
    "        output_cols   = output_cols,\n",
    "        scaler_X_func   = scaler_X_func_nov,\n",
    "        scaler_y_func   = scaler_y_func_pend,\n",
    "        na=na,\n",
    "        nb_past=nb,\n",
    "        nf_future=nf,\n",
    "        test_name_col='test_name',\n",
    "        y_initial_mode='zero'  # 'original' ➔ skip initial rows, 'zero' ➔ pad lags with zeros\n",
    ")\n",
    "    \n",
    "y_test_target = dfy_test[output_cols].reset_index(drop=True)\n",
    "# select eta only in input features\n",
    "X_test_selected_df = select_input_features(\n",
    "    dfx_test.drop(columns='test_name'),\n",
    "    input_type=xcase,\n",
    "    keep_future=True,\n",
    "    keep_past=True\n",
    ")\n",
    "X_test_selected_df, y_test_target = adjust_for_batch_df(X_test_selected_df, y_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51192c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# parralel prediction \n",
    "\n",
    "y_pred_pendulum_test=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f30237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_pendulum_parralel_pred.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "\n",
    "# parralel prediction\n",
    "y_pred_pendulum_test=model.predict(X_test_selected_df)\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_time = time.time()  # Start time for measurement\n",
    "initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# series prediction \n",
    "\n",
    "# Run prediction process \n",
    "y_pred_test_series_case_compare_heave, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Measure memory usage and time after prediction\n",
    "end_time = time.time()  # End time for measurement\n",
    "final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "# Print results\n",
    "print(f\"Time taken : {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {final_memory - initial_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(\n",
    "    output_dir=\"Results/co2\",       # Custom folder\n",
    "    output_file=\"arx_pendulum_series.csv\"    # Custom filename\n",
    ")\n",
    "tracker.start()\n",
    "\n",
    "# series prediction \n",
    "\n",
    "# Run prediction process \n",
    "y_pred_test_series_case_compare_heave, x_used_test = predict_recursive_series(\n",
    "    model=model,\n",
    "    X_df=X_test_selected_df,\n",
    "    output_cols=output_cols,\n",
    "    X_feature_names=X_feature_names,\n",
    "    na=na\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Stop the tracker and retrieve the estimated emissions\n",
    "emissions = tracker.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
